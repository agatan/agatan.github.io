<!DOCTYPE html><html><head><meta name="viewport" content="width=device-width"/><meta charSet="utf-8"/><title>【論文読み】Semi-convolutional Operators for Instance Segmentation | 右上↗</title><meta property="og:type" content="website"/><meta property="og:title" content="【論文読み】Semi-convolutional Operators for Instance Segmentation"/><meta property="og:site_name" content="右上↗"/><meta property="twitter:card" content="summary"/><meta property="twitter:creator" content="@agatan_"/><meta property="twitter:title" content="【論文読み】Semi-convolutional Operators for Instance Segmentation"/><meta name="next-head-count" content="9"/><link rel="preload" href="/_next/static/css/fcf459cc03a0aa33d72b.css" as="style"/><link rel="stylesheet" href="/_next/static/css/fcf459cc03a0aa33d72b.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-a54b4f32bdc1ef890ddd.js"></script><script src="/_next/static/chunks/webpack-20d43e08bea62467b090.js" defer=""></script><script src="/_next/static/chunks/framework-92300432a1172ef1338b.js" defer=""></script><script src="/_next/static/chunks/main-588261c74baf7142d208.js" defer=""></script><script src="/_next/static/chunks/pages/_app-a9e8b7fcb86503283a43.js" defer=""></script><script src="/_next/static/chunks/1bfc9850-b731c6aa1c491f14d361.js" defer=""></script><script src="/_next/static/chunks/191-ac794077f326adb56e38.js" defer=""></script><script src="/_next/static/chunks/686-4b319bb0411076e06dbc.js" defer=""></script><script src="/_next/static/chunks/pages/posts/%5Bslug%5D-3b1d79cd751fba9fda5d.js" defer=""></script><script src="/_next/static/LTCqLRPEd5Qd2agqEQJZo/_buildManifest.js" defer=""></script><script src="/_next/static/LTCqLRPEd5Qd2agqEQJZo/_ssgManifest.js" defer=""></script></head><body><div id="__next"><style data-emotion="css-global o7558t">:host,:root{--chakra-ring-inset:var(--chakra-empty,/*!*/ /*!*/);--chakra-ring-offset-width:0px;--chakra-ring-offset-color:#fff;--chakra-ring-color:rgba(66, 153, 225, 0.6);--chakra-ring-offset-shadow:0 0 #0000;--chakra-ring-shadow:0 0 #0000;--chakra-space-x-reverse:0;--chakra-space-y-reverse:0;--chakra-colors-transparent:transparent;--chakra-colors-current:currentColor;--chakra-colors-black:#000000;--chakra-colors-white:#FFFFFF;--chakra-colors-whiteAlpha-50:rgba(255, 255, 255, 0.04);--chakra-colors-whiteAlpha-100:rgba(255, 255, 255, 0.06);--chakra-colors-whiteAlpha-200:rgba(255, 255, 255, 0.08);--chakra-colors-whiteAlpha-300:rgba(255, 255, 255, 0.16);--chakra-colors-whiteAlpha-400:rgba(255, 255, 255, 0.24);--chakra-colors-whiteAlpha-500:rgba(255, 255, 255, 0.36);--chakra-colors-whiteAlpha-600:rgba(255, 255, 255, 0.48);--chakra-colors-whiteAlpha-700:rgba(255, 255, 255, 0.64);--chakra-colors-whiteAlpha-800:rgba(255, 255, 255, 0.80);--chakra-colors-whiteAlpha-900:rgba(255, 255, 255, 0.92);--chakra-colors-blackAlpha-50:rgba(0, 0, 0, 0.04);--chakra-colors-blackAlpha-100:rgba(0, 0, 0, 0.06);--chakra-colors-blackAlpha-200:rgba(0, 0, 0, 0.08);--chakra-colors-blackAlpha-300:rgba(0, 0, 0, 0.16);--chakra-colors-blackAlpha-400:rgba(0, 0, 0, 0.24);--chakra-colors-blackAlpha-500:rgba(0, 0, 0, 0.36);--chakra-colors-blackAlpha-600:rgba(0, 0, 0, 0.48);--chakra-colors-blackAlpha-700:rgba(0, 0, 0, 0.64);--chakra-colors-blackAlpha-800:rgba(0, 0, 0, 0.80);--chakra-colors-blackAlpha-900:rgba(0, 0, 0, 0.92);--chakra-colors-gray-50:#F7FAFC;--chakra-colors-gray-100:#EDF2F7;--chakra-colors-gray-200:#E2E8F0;--chakra-colors-gray-300:#CBD5E0;--chakra-colors-gray-400:#A0AEC0;--chakra-colors-gray-500:#718096;--chakra-colors-gray-600:#4A5568;--chakra-colors-gray-700:#2D3748;--chakra-colors-gray-800:#1A202C;--chakra-colors-gray-900:#171923;--chakra-colors-red-50:#FFF5F5;--chakra-colors-red-100:#FED7D7;--chakra-colors-red-200:#FEB2B2;--chakra-colors-red-300:#FC8181;--chakra-colors-red-400:#F56565;--chakra-colors-red-500:#E53E3E;--chakra-colors-red-600:#C53030;--chakra-colors-red-700:#9B2C2C;--chakra-colors-red-800:#822727;--chakra-colors-red-900:#63171B;--chakra-colors-orange-50:#FFFAF0;--chakra-colors-orange-100:#FEEBC8;--chakra-colors-orange-200:#FBD38D;--chakra-colors-orange-300:#F6AD55;--chakra-colors-orange-400:#ED8936;--chakra-colors-orange-500:#DD6B20;--chakra-colors-orange-600:#C05621;--chakra-colors-orange-700:#9C4221;--chakra-colors-orange-800:#7B341E;--chakra-colors-orange-900:#652B19;--chakra-colors-yellow-50:#FFFFF0;--chakra-colors-yellow-100:#FEFCBF;--chakra-colors-yellow-200:#FAF089;--chakra-colors-yellow-300:#F6E05E;--chakra-colors-yellow-400:#ECC94B;--chakra-colors-yellow-500:#D69E2E;--chakra-colors-yellow-600:#B7791F;--chakra-colors-yellow-700:#975A16;--chakra-colors-yellow-800:#744210;--chakra-colors-yellow-900:#5F370E;--chakra-colors-green-50:#F0FFF4;--chakra-colors-green-100:#C6F6D5;--chakra-colors-green-200:#9AE6B4;--chakra-colors-green-300:#68D391;--chakra-colors-green-400:#48BB78;--chakra-colors-green-500:#38A169;--chakra-colors-green-600:#2F855A;--chakra-colors-green-700:#276749;--chakra-colors-green-800:#22543D;--chakra-colors-green-900:#1C4532;--chakra-colors-teal-50:#E6FFFA;--chakra-colors-teal-100:#B2F5EA;--chakra-colors-teal-200:#81E6D9;--chakra-colors-teal-300:#4FD1C5;--chakra-colors-teal-400:#38B2AC;--chakra-colors-teal-500:#319795;--chakra-colors-teal-600:#2C7A7B;--chakra-colors-teal-700:#285E61;--chakra-colors-teal-800:#234E52;--chakra-colors-teal-900:#1D4044;--chakra-colors-blue-50:#ebf8ff;--chakra-colors-blue-100:#bee3f8;--chakra-colors-blue-200:#90cdf4;--chakra-colors-blue-300:#63b3ed;--chakra-colors-blue-400:#4299e1;--chakra-colors-blue-500:#3182ce;--chakra-colors-blue-600:#2b6cb0;--chakra-colors-blue-700:#2c5282;--chakra-colors-blue-800:#2a4365;--chakra-colors-blue-900:#1A365D;--chakra-colors-cyan-50:#EDFDFD;--chakra-colors-cyan-100:#C4F1F9;--chakra-colors-cyan-200:#9DECF9;--chakra-colors-cyan-300:#76E4F7;--chakra-colors-cyan-400:#0BC5EA;--chakra-colors-cyan-500:#00B5D8;--chakra-colors-cyan-600:#00A3C4;--chakra-colors-cyan-700:#0987A0;--chakra-colors-cyan-800:#086F83;--chakra-colors-cyan-900:#065666;--chakra-colors-purple-50:#FAF5FF;--chakra-colors-purple-100:#E9D8FD;--chakra-colors-purple-200:#D6BCFA;--chakra-colors-purple-300:#B794F4;--chakra-colors-purple-400:#9F7AEA;--chakra-colors-purple-500:#805AD5;--chakra-colors-purple-600:#6B46C1;--chakra-colors-purple-700:#553C9A;--chakra-colors-purple-800:#44337A;--chakra-colors-purple-900:#322659;--chakra-colors-pink-50:#FFF5F7;--chakra-colors-pink-100:#FED7E2;--chakra-colors-pink-200:#FBB6CE;--chakra-colors-pink-300:#F687B3;--chakra-colors-pink-400:#ED64A6;--chakra-colors-pink-500:#D53F8C;--chakra-colors-pink-600:#B83280;--chakra-colors-pink-700:#97266D;--chakra-colors-pink-800:#702459;--chakra-colors-pink-900:#521B41;--chakra-colors-linkedin-50:#E8F4F9;--chakra-colors-linkedin-100:#CFEDFB;--chakra-colors-linkedin-200:#9BDAF3;--chakra-colors-linkedin-300:#68C7EC;--chakra-colors-linkedin-400:#34B3E4;--chakra-colors-linkedin-500:#00A0DC;--chakra-colors-linkedin-600:#008CC9;--chakra-colors-linkedin-700:#0077B5;--chakra-colors-linkedin-800:#005E93;--chakra-colors-linkedin-900:#004471;--chakra-colors-facebook-50:#E8F4F9;--chakra-colors-facebook-100:#D9DEE9;--chakra-colors-facebook-200:#B7C2DA;--chakra-colors-facebook-300:#6482C0;--chakra-colors-facebook-400:#4267B2;--chakra-colors-facebook-500:#385898;--chakra-colors-facebook-600:#314E89;--chakra-colors-facebook-700:#29487D;--chakra-colors-facebook-800:#223B67;--chakra-colors-facebook-900:#1E355B;--chakra-colors-messenger-50:#D0E6FF;--chakra-colors-messenger-100:#B9DAFF;--chakra-colors-messenger-200:#A2CDFF;--chakra-colors-messenger-300:#7AB8FF;--chakra-colors-messenger-400:#2E90FF;--chakra-colors-messenger-500:#0078FF;--chakra-colors-messenger-600:#0063D1;--chakra-colors-messenger-700:#0052AC;--chakra-colors-messenger-800:#003C7E;--chakra-colors-messenger-900:#002C5C;--chakra-colors-whatsapp-50:#dffeec;--chakra-colors-whatsapp-100:#b9f5d0;--chakra-colors-whatsapp-200:#90edb3;--chakra-colors-whatsapp-300:#65e495;--chakra-colors-whatsapp-400:#3cdd78;--chakra-colors-whatsapp-500:#22c35e;--chakra-colors-whatsapp-600:#179848;--chakra-colors-whatsapp-700:#0c6c33;--chakra-colors-whatsapp-800:#01421c;--chakra-colors-whatsapp-900:#001803;--chakra-colors-twitter-50:#E5F4FD;--chakra-colors-twitter-100:#C8E9FB;--chakra-colors-twitter-200:#A8DCFA;--chakra-colors-twitter-300:#83CDF7;--chakra-colors-twitter-400:#57BBF5;--chakra-colors-twitter-500:#1DA1F2;--chakra-colors-twitter-600:#1A94DA;--chakra-colors-twitter-700:#1681BF;--chakra-colors-twitter-800:#136B9E;--chakra-colors-twitter-900:#0D4D71;--chakra-colors-telegram-50:#E3F2F9;--chakra-colors-telegram-100:#C5E4F3;--chakra-colors-telegram-200:#A2D4EC;--chakra-colors-telegram-300:#7AC1E4;--chakra-colors-telegram-400:#47A9DA;--chakra-colors-telegram-500:#0088CC;--chakra-colors-telegram-600:#007AB8;--chakra-colors-telegram-700:#006BA1;--chakra-colors-telegram-800:#005885;--chakra-colors-telegram-900:#003F5E;--chakra-borders-none:0;--chakra-borders-1px:1px solid;--chakra-borders-2px:2px solid;--chakra-borders-4px:4px solid;--chakra-borders-8px:8px solid;--chakra-fonts-heading:-apple-system,BlinkMacSystemFont,"Segoe UI",Helvetica,Arial,sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol";--chakra-fonts-body:-apple-system,BlinkMacSystemFont,"Segoe UI",Helvetica,Arial,sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol";--chakra-fonts-mono:SFMono-Regular,Menlo,Monaco,Consolas,"Liberation Mono","Courier New",monospace;--chakra-fontSizes-xs:0.75rem;--chakra-fontSizes-sm:0.875rem;--chakra-fontSizes-md:1rem;--chakra-fontSizes-lg:1.125rem;--chakra-fontSizes-xl:1.25rem;--chakra-fontSizes-2xl:1.5rem;--chakra-fontSizes-3xl:1.875rem;--chakra-fontSizes-4xl:2.25rem;--chakra-fontSizes-5xl:3rem;--chakra-fontSizes-6xl:3.75rem;--chakra-fontSizes-7xl:4.5rem;--chakra-fontSizes-8xl:6rem;--chakra-fontSizes-9xl:8rem;--chakra-fontWeights-hairline:100;--chakra-fontWeights-thin:200;--chakra-fontWeights-light:300;--chakra-fontWeights-normal:400;--chakra-fontWeights-medium:500;--chakra-fontWeights-semibold:600;--chakra-fontWeights-bold:700;--chakra-fontWeights-extrabold:800;--chakra-fontWeights-black:900;--chakra-letterSpacings-tighter:-0.05em;--chakra-letterSpacings-tight:-0.025em;--chakra-letterSpacings-normal:0;--chakra-letterSpacings-wide:0.025em;--chakra-letterSpacings-wider:0.05em;--chakra-letterSpacings-widest:0.1em;--chakra-lineHeights-3:.75rem;--chakra-lineHeights-4:1rem;--chakra-lineHeights-5:1.25rem;--chakra-lineHeights-6:1.5rem;--chakra-lineHeights-7:1.75rem;--chakra-lineHeights-8:2rem;--chakra-lineHeights-9:2.25rem;--chakra-lineHeights-10:2.5rem;--chakra-lineHeights-normal:normal;--chakra-lineHeights-none:1;--chakra-lineHeights-shorter:1.25;--chakra-lineHeights-short:1.375;--chakra-lineHeights-base:1.5;--chakra-lineHeights-tall:1.625;--chakra-lineHeights-taller:2;--chakra-radii-none:0;--chakra-radii-sm:0.125rem;--chakra-radii-base:0.25rem;--chakra-radii-md:0.375rem;--chakra-radii-lg:0.5rem;--chakra-radii-xl:0.75rem;--chakra-radii-2xl:1rem;--chakra-radii-3xl:1.5rem;--chakra-radii-full:9999px;--chakra-space-1:0.25rem;--chakra-space-2:0.5rem;--chakra-space-3:0.75rem;--chakra-space-4:1rem;--chakra-space-5:1.25rem;--chakra-space-6:1.5rem;--chakra-space-7:1.75rem;--chakra-space-8:2rem;--chakra-space-9:2.25rem;--chakra-space-10:2.5rem;--chakra-space-12:3rem;--chakra-space-14:3.5rem;--chakra-space-16:4rem;--chakra-space-20:5rem;--chakra-space-24:6rem;--chakra-space-28:7rem;--chakra-space-32:8rem;--chakra-space-36:9rem;--chakra-space-40:10rem;--chakra-space-44:11rem;--chakra-space-48:12rem;--chakra-space-52:13rem;--chakra-space-56:14rem;--chakra-space-60:15rem;--chakra-space-64:16rem;--chakra-space-72:18rem;--chakra-space-80:20rem;--chakra-space-96:24rem;--chakra-space-px:1px;--chakra-space-0\.5:0.125rem;--chakra-space-1\.5:0.375rem;--chakra-space-2\.5:0.625rem;--chakra-space-3\.5:0.875rem;--chakra-shadows-xs:0 0 0 1px rgba(0, 0, 0, 0.05);--chakra-shadows-sm:0 1px 2px 0 rgba(0, 0, 0, 0.05);--chakra-shadows-base:0 1px 3px 0 rgba(0, 0, 0, 0.1),0 1px 2px 0 rgba(0, 0, 0, 0.06);--chakra-shadows-md:0 4px 6px -1px rgba(0, 0, 0, 0.1),0 2px 4px -1px rgba(0, 0, 0, 0.06);--chakra-shadows-lg:0 10px 15px -3px rgba(0, 0, 0, 0.1),0 4px 6px -2px rgba(0, 0, 0, 0.05);--chakra-shadows-xl:0 20px 25px -5px rgba(0, 0, 0, 0.1),0 10px 10px -5px rgba(0, 0, 0, 0.04);--chakra-shadows-2xl:0 25px 50px -12px rgba(0, 0, 0, 0.25);--chakra-shadows-outline:0 0 0 3px rgba(66, 153, 225, 0.6);--chakra-shadows-inner:inset 0 2px 4px 0 rgba(0,0,0,0.06);--chakra-shadows-none:none;--chakra-shadows-dark-lg:rgba(0, 0, 0, 0.1) 0px 0px 0px 1px,rgba(0, 0, 0, 0.2) 0px 5px 10px,rgba(0, 0, 0, 0.4) 0px 15px 40px;--chakra-sizes-1:0.25rem;--chakra-sizes-2:0.5rem;--chakra-sizes-3:0.75rem;--chakra-sizes-4:1rem;--chakra-sizes-5:1.25rem;--chakra-sizes-6:1.5rem;--chakra-sizes-7:1.75rem;--chakra-sizes-8:2rem;--chakra-sizes-9:2.25rem;--chakra-sizes-10:2.5rem;--chakra-sizes-12:3rem;--chakra-sizes-14:3.5rem;--chakra-sizes-16:4rem;--chakra-sizes-20:5rem;--chakra-sizes-24:6rem;--chakra-sizes-28:7rem;--chakra-sizes-32:8rem;--chakra-sizes-36:9rem;--chakra-sizes-40:10rem;--chakra-sizes-44:11rem;--chakra-sizes-48:12rem;--chakra-sizes-52:13rem;--chakra-sizes-56:14rem;--chakra-sizes-60:15rem;--chakra-sizes-64:16rem;--chakra-sizes-72:18rem;--chakra-sizes-80:20rem;--chakra-sizes-96:24rem;--chakra-sizes-px:1px;--chakra-sizes-0\.5:0.125rem;--chakra-sizes-1\.5:0.375rem;--chakra-sizes-2\.5:0.625rem;--chakra-sizes-3\.5:0.875rem;--chakra-sizes-max:max-content;--chakra-sizes-min:min-content;--chakra-sizes-full:100%;--chakra-sizes-3xs:14rem;--chakra-sizes-2xs:16rem;--chakra-sizes-xs:20rem;--chakra-sizes-sm:24rem;--chakra-sizes-md:28rem;--chakra-sizes-lg:32rem;--chakra-sizes-xl:36rem;--chakra-sizes-2xl:42rem;--chakra-sizes-3xl:48rem;--chakra-sizes-4xl:56rem;--chakra-sizes-5xl:64rem;--chakra-sizes-6xl:72rem;--chakra-sizes-7xl:80rem;--chakra-sizes-8xl:90rem;--chakra-sizes-container-sm:640px;--chakra-sizes-container-md:768px;--chakra-sizes-container-lg:1024px;--chakra-sizes-container-xl:1280px;--chakra-zIndices-hide:-1;--chakra-zIndices-auto:auto;--chakra-zIndices-base:0;--chakra-zIndices-docked:10;--chakra-zIndices-dropdown:1000;--chakra-zIndices-sticky:1100;--chakra-zIndices-banner:1200;--chakra-zIndices-overlay:1300;--chakra-zIndices-modal:1400;--chakra-zIndices-popover:1500;--chakra-zIndices-skipLink:1600;--chakra-zIndices-toast:1700;--chakra-zIndices-tooltip:1800;--chakra-transition-property-common:background-color,border-color,color,fill,stroke,opacity,box-shadow,transform;--chakra-transition-property-colors:background-color,border-color,color,fill,stroke;--chakra-transition-property-dimensions:width,height;--chakra-transition-property-position:left,right,top,bottom;--chakra-transition-property-background:background-color,background-image,background-position;--chakra-transition-easing-ease-in:cubic-bezier(0.4, 0, 1, 1);--chakra-transition-easing-ease-out:cubic-bezier(0, 0, 0.2, 1);--chakra-transition-easing-ease-in-out:cubic-bezier(0.4, 0, 0.2, 1);--chakra-transition-duration-ultra-fast:50ms;--chakra-transition-duration-faster:100ms;--chakra-transition-duration-fast:150ms;--chakra-transition-duration-normal:200ms;--chakra-transition-duration-slow:300ms;--chakra-transition-duration-slower:400ms;--chakra-transition-duration-ultra-slow:500ms;--chakra-blur-none:0;--chakra-blur-sm:4px;--chakra-blur-base:8px;--chakra-blur-md:12px;--chakra-blur-lg:16px;--chakra-blur-xl:24px;--chakra-blur-2xl:40px;--chakra-blur-3xl:64px;}</style><style data-emotion="css-global 1syi0wy">html{line-height:1.5;-webkit-text-size-adjust:100%;font-family:system-ui,sans-serif;-webkit-font-smoothing:antialiased;text-rendering:optimizeLegibility;-moz-osx-font-smoothing:grayscale;touch-action:manipulation;}body{position:relative;min-height:100%;font-feature-settings:'kern';}*,*::before,*::after{border-width:0;border-style:solid;box-sizing:border-box;}main{display:block;}hr{border-top-width:1px;box-sizing:content-box;height:0;overflow:visible;}pre,code,kbd,samp{font-family:SFMono-Regular,Menlo,Monaco,Consolas,monospace;font-size:1em;}a{background-color:transparent;color:inherit;-webkit-text-decoration:inherit;text-decoration:inherit;}abbr[title]{border-bottom:none;-webkit-text-decoration:underline;text-decoration:underline;-webkit-text-decoration:underline dotted;-webkit-text-decoration:underline dotted;text-decoration:underline dotted;}b,strong{font-weight:bold;}small{font-size:80%;}sub,sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline;}sub{bottom:-0.25em;}sup{top:-0.5em;}img{border-style:none;}button,input,optgroup,select,textarea{font-family:inherit;font-size:100%;line-height:1.15;margin:0;}button,input{overflow:visible;}button,select{text-transform:none;}button::-moz-focus-inner,[type="button"]::-moz-focus-inner,[type="reset"]::-moz-focus-inner,[type="submit"]::-moz-focus-inner{border-style:none;padding:0;}fieldset{padding:0.35em 0.75em 0.625em;}legend{box-sizing:border-box;color:inherit;display:table;max-width:100%;padding:0;white-space:normal;}progress{vertical-align:baseline;}textarea{overflow:auto;}[type="checkbox"],[type="radio"]{box-sizing:border-box;padding:0;}[type="number"]::-webkit-inner-spin-button,[type="number"]::-webkit-outer-spin-button{-webkit-appearance:none!important;}input[type="number"]{-moz-appearance:textfield;}[type="search"]{-webkit-appearance:textfield;outline-offset:-2px;}[type="search"]::-webkit-search-decoration{-webkit-appearance:none!important;}::-webkit-file-upload-button{-webkit-appearance:button;font:inherit;}details{display:block;}summary{display:-webkit-box;display:-webkit-list-item;display:-ms-list-itembox;display:list-item;}template{display:none;}[hidden]{display:none!important;}body,blockquote,dl,dd,h1,h2,h3,h4,h5,h6,hr,figure,p,pre{margin:0;}button{background:transparent;padding:0;}fieldset{margin:0;padding:0;}ol,ul{margin:0;padding:0;}textarea{resize:vertical;}button,[role="button"]{cursor:pointer;}button::-moz-focus-inner{border:0!important;}table{border-collapse:collapse;}h1,h2,h3,h4,h5,h6{font-size:inherit;font-weight:inherit;}button,input,optgroup,select,textarea{padding:0;line-height:inherit;color:inherit;}img,svg,video,canvas,audio,iframe,embed,object{display:block;vertical-align:middle;}img,video{max-width:100%;height:auto;}[data-js-focus-visible] :focus:not([data-focus-visible-added]){outline:none;box-shadow:none;}select::-ms-expand{display:none;}</style><style data-emotion="css-global 1baqkrf">body{font-family:var(--chakra-fonts-body);color:var(--chakra-colors-gray-800);background:var(--chakra-colors-white);transition-property:background-color;transition-duration:var(--chakra-transition-duration-normal);line-height:var(--chakra-lineHeights-base);}*::-webkit-input-placeholder{color:var(--chakra-colors-gray-400);}*::-moz-placeholder{color:var(--chakra-colors-gray-400);}*:-ms-input-placeholder{color:var(--chakra-colors-gray-400);}*::placeholder{color:var(--chakra-colors-gray-400);}*,*::before,::after{border-color:var(--chakra-colors-gray-200);word-wrap:break-word;}</style><style data-emotion="css d4xd0d">.css-d4xd0d{background-color:var(--chakra-colors-green-50);min-height:100vh;}</style><div class="css-d4xd0d"><style data-emotion="css 50jqoj">.css-50jqoj{width:100%;-webkit-margin-start:auto;margin-inline-start:auto;-webkit-margin-end:auto;margin-inline-end:auto;max-width:var(--chakra-sizes-container-lg);-webkit-padding-start:1rem;padding-inline-start:1rem;-webkit-padding-end:1rem;padding-inline-end:1rem;background-color:var(--chakra-colors-green-600);}</style><div class="chakra-container css-50jqoj"><style data-emotion="css jbx4q3">.css-jbx4q3{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:flex-end;-webkit-box-align:flex-end;-ms-flex-align:flex-end;align-items:flex-end;padding:var(--chakra-space-8);}</style><div class="css-jbx4q3"><style data-emotion="css 1dklj6k">.css-1dklj6k{font-family:var(--chakra-fonts-heading);font-weight:var(--chakra-fontWeights-bold);font-size:var(--chakra-fontSizes-3xl);line-height:1.33;}@media screen and (min-width: 48em){.css-1dklj6k{font-size:var(--chakra-fontSizes-4xl);line-height:1.2;}}</style><h2 class="chakra-heading css-1dklj6k"><style data-emotion="css 1naxmyz">.css-1naxmyz{transition-property:var(--chakra-transition-property-common);transition-duration:var(--chakra-transition-duration-fast);transition-timing-function:var(--chakra-transition-easing-ease-out);cursor:pointer;-webkit-text-decoration:none;text-decoration:none;outline:2px solid transparent;outline-offset:2px;color:var(--chakra-colors-white);}.css-1naxmyz:hover,.css-1naxmyz[data-hover]{-webkit-text-decoration:underline;text-decoration:underline;}.css-1naxmyz:focus,.css-1naxmyz[data-focus]{box-shadow:var(--chakra-shadows-outline);}</style><a class="chakra-link css-1naxmyz" href="/"><u class="chakra-text css-0">↗ agatan blog ↗</u></a></h2><style data-emotion="css 17xejub">.css-17xejub{-webkit-flex:1;-ms-flex:1;flex:1;justify-self:stretch;-webkit-align-self:stretch;-ms-flex-item-align:stretch;align-self:stretch;}</style><div class="css-17xejub"></div><div class="css-0"><style data-emotion="css 1awjy5h">.css-1awjy5h{transition-property:var(--chakra-transition-property-common);transition-duration:var(--chakra-transition-duration-fast);transition-timing-function:var(--chakra-transition-easing-ease-out);cursor:pointer;-webkit-text-decoration:none;text-decoration:none;outline:2px solid transparent;outline-offset:2px;color:inherit;padding:var(--chakra-space-2);}.css-1awjy5h:hover,.css-1awjy5h[data-hover]{-webkit-text-decoration:underline;text-decoration:underline;}.css-1awjy5h:focus,.css-1awjy5h[data-focus]{box-shadow:var(--chakra-shadows-outline);}</style><a class="chakra-link css-1awjy5h" href="/feed.xml"><style data-emotion="css sjogs4">.css-sjogs4{width:var(--chakra-sizes-6);height:var(--chakra-sizes-6);display:inline-block;line-height:1em;-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;color:var(--chakra-colors-white);}</style><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" focusable="false" class="chakra-icon css-sjogs4" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M128.081 415.959c0 35.369-28.672 64.041-64.041 64.041S0 451.328 0 415.959s28.672-64.041 64.041-64.041 64.04 28.673 64.04 64.041zm175.66 47.25c-8.354-154.6-132.185-278.587-286.95-286.95C7.656 175.765 0 183.105 0 192.253v48.069c0 8.415 6.49 15.472 14.887 16.018 111.832 7.284 201.473 96.702 208.772 208.772.547 8.397 7.604 14.887 16.018 14.887h48.069c9.149.001 16.489-7.655 15.995-16.79zm144.249.288C439.596 229.677 251.465 40.445 16.503 32.01 7.473 31.686 0 38.981 0 48.016v48.068c0 8.625 6.835 15.645 15.453 15.999 191.179 7.839 344.627 161.316 352.465 352.465.353 8.618 7.373 15.453 15.999 15.453h48.068c9.034-.001 16.329-7.474 16.005-16.504z"></path></svg></a><a class="chakra-link css-1awjy5h" href="/tags"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" focusable="false" class="chakra-icon css-sjogs4" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M497.941 225.941L286.059 14.059A48 48 0 0 0 252.118 0H48C21.49 0 0 21.49 0 48v204.118a48 48 0 0 0 14.059 33.941l211.882 211.882c18.744 18.745 49.136 18.746 67.882 0l204.118-204.118c18.745-18.745 18.745-49.137 0-67.882zM112 160c-26.51 0-48-21.49-48-48s21.49-48 48-48 48 21.49 48 48-21.49 48-48 48zm513.941 133.823L421.823 497.941c-18.745 18.745-49.137 18.745-67.882 0l-.36-.36L527.64 323.522c16.999-16.999 26.36-39.6 26.36-63.64s-9.362-46.641-26.36-63.64L331.397 0h48.721a48 48 0 0 1 33.941 14.059l211.882 211.882c18.745 18.745 18.745 49.137 0 67.882z"></path></svg></a></div></div></div><style data-emotion="css 8shb6n">.css-8shb6n{width:100%;-webkit-margin-start:auto;margin-inline-start:auto;-webkit-margin-end:auto;margin-inline-end:auto;max-width:var(--chakra-sizes-container-lg);-webkit-padding-start:1rem;padding-inline-start:1rem;-webkit-padding-end:1rem;padding-inline-end:1rem;background-color:var(--chakra-colors-white);min-height:100vh;}</style><div class="chakra-container css-8shb6n"><style data-emotion="css 10qlibn">.css-10qlibn{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;padding-top:var(--chakra-space-4);padding-bottom:var(--chakra-space-4);}</style><div class="css-10qlibn"><style data-emotion="css 8jfdow">.css-8jfdow{width:100%;-webkit-margin-start:auto;margin-inline-start:auto;-webkit-margin-end:auto;margin-inline-end:auto;max-width:var(--chakra-sizes-container-lg);-webkit-padding-start:1rem;padding-inline-start:1rem;-webkit-padding-end:1rem;padding-inline-end:1rem;padding:0px;margin:0px;}</style><div class="chakra-container css-8jfdow"><style data-emotion="css nm5t63">.css-nm5t63{width:100%;-webkit-margin-start:auto;margin-inline-start:auto;-webkit-margin-end:auto;margin-inline-end:auto;max-width:var(--chakra-sizes-container-md);-webkit-padding-start:1rem;padding-inline-start:1rem;-webkit-padding-end:1rem;padding-inline-end:1rem;}</style><div class="chakra-container css-nm5t63"><style data-emotion="css 6euoq0">.css-6euoq0{font-family:var(--chakra-fonts-heading);font-weight:var(--chakra-fontWeights-bold);font-size:var(--chakra-fontSizes-3xl);line-height:1.33;padding-bottom:var(--chakra-space-4);}@media screen and (min-width: 48em){.css-6euoq0{line-height:1.2;}}</style><h2 class="chakra-heading css-6euoq0">【論文読み】Semi-convolutional Operators for Instance Segmentation</h2><style data-emotion="css 1h1f62l">.css-1h1f62l{transition-property:var(--chakra-transition-property-common);transition-duration:var(--chakra-transition-duration-fast);transition-timing-function:var(--chakra-transition-easing-ease-out);cursor:pointer;-webkit-text-decoration:none;text-decoration:none;outline:2px solid transparent;outline-offset:2px;color:inherit;padding:var(--chakra-space-1);}.css-1h1f62l:hover,.css-1h1f62l[data-hover]{-webkit-text-decoration:underline;text-decoration:underline;}.css-1h1f62l:focus,.css-1h1f62l[data-focus]{box-shadow:var(--chakra-shadows-outline);}</style><a class="chakra-link css-1h1f62l" href="/tags/DeepLearning"><style data-emotion="css 6rl2qk">.css-6rl2qk{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;vertical-align:top;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;max-width:100%;font-weight:var(--chakra-fontWeights-medium);line-height:1.2;outline:2px solid transparent;outline-offset:2px;min-height:1.5rem;min-width:1.5rem;font-size:var(--chakra-fontSizes-sm);border-radius:var(--chakra-radii-md);-webkit-padding-start:var(--chakra-space-2);padding-inline-start:var(--chakra-space-2);-webkit-padding-end:var(--chakra-space-2);padding-inline-end:var(--chakra-space-2);color:#3182ce;box-shadow:inset 0 0 0px 1px #3182ce;padding:var(--chakra-space-1);}.css-6rl2qk:focus,.css-6rl2qk[data-focus]{box-shadow:var(--chakra-shadows-outline);}</style><span class="css-6rl2qk"><style data-emotion="css 1qpz0yi">.css-1qpz0yi{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;padding-left:var(--chakra-space-1);padding-right:var(--chakra-space-1);}</style><div class="css-1qpz0yi"><style data-emotion="css 1uhrip0">.css-1uhrip0{line-height:1.2;overflow:hidden;text-overflow:ellipsis;white-space:nowrap;font-size:small;}</style><span class="css-1uhrip0">DeepLearning</span></div></span></a><a class="chakra-link css-1h1f62l" href="/tags/論文読み"><span class="css-6rl2qk"><div class="css-1qpz0yi"><span class="css-1uhrip0">論文読み</span></div></span></a><style data-emotion="css 8zn9ea">.css-8zn9ea{opacity:0.6;border:0;border-color:inherit;border-style:solid;border-bottom-width:1px;width:100%;padding:var(--chakra-space-2);}</style><hr aria-orientation="horizontal" class="chakra-divider css-8zn9ea"/><style data-emotion="css 1rwovhe">.css-1rwovhe{padding-top:var(--chakra-space-4);}</style><div class="css-1rwovhe"><div><style data-emotion="css ykzoaw">.css-ykzoaw{font-size:medium;line-height:var(--chakra-lineHeights-7);padding:var(--chakra-space-1);}</style><p class="chakra-text css-ykzoaw">Instance Segmentation のタスクに対する手法を整理・分解し、精度をより向上する <code>Semi-convolutional operators</code> を提案した論文です。</p>
<p class="chakra-text css-ykzoaw"><img src="https://qiita-image-store.s3.amazonaws.com/0/39030/8164ed4c-3f5d-c772-e21d-7d02d5146461.png" alt="image.png"/></p>
<p class="chakra-text css-ykzoaw">この記事は、Wantedly の勉強会で取り上げられた論文・技術をまとめたものです。
<style data-emotion="css f4h6uy">.css-f4h6uy{transition-property:var(--chakra-transition-property-common);transition-duration:var(--chakra-transition-duration-fast);transition-timing-function:var(--chakra-transition-easing-ease-out);cursor:pointer;-webkit-text-decoration:none;text-decoration:none;outline:2px solid transparent;outline-offset:2px;color:inherit;}.css-f4h6uy:hover,.css-f4h6uy[data-hover]{-webkit-text-decoration:underline;text-decoration:underline;}.css-f4h6uy:focus,.css-f4h6uy[data-focus]{box-shadow:var(--chakra-shadows-outline);}</style><a class="chakra-link css-f4h6uy" href="https://qiita.com/advent-calendar/2018/wantedly_ml"><style data-emotion="css hn1ydt">.css-hn1ydt{color:var(--chakra-colors-green-600);}</style><span class="chakra-text css-hn1ydt">2018 年に読んだ機械学習系論文・技術まとめ at Wantedly Advent Calendar 2018 - Qiita</span></a></p>
<style data-emotion="css nppbz4">.css-nppbz4{font-family:var(--chakra-fonts-heading);font-weight:var(--chakra-fontWeights-bold);font-size:x-large;line-height:1.33;padding-top:var(--chakra-space-2);padding-bottom:var(--chakra-space-2);}@media screen and (min-width: 48em){.css-nppbz4{line-height:1.2;}}</style><h2 class="chakra-heading css-nppbz4" id="reference"><style data-emotion="css 70qvj9">.css-70qvj9{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;}</style><div class="css-70qvj9"><a class="chakra-link css-1awjy5h" href="#reference"><style data-emotion="css ifa7ya">.css-ifa7ya{width:var(--chakra-sizes-4);height:var(--chakra-sizes-4);display:inline-block;line-height:1em;-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;color:var(--chakra-colors-gray-500);}</style><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" focusable="false" class="chakra-icon css-ifa7ya" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"></path></svg></a>Reference</div></h2>
<style data-emotion="css 1h3xlp0">.css-1h3xlp0{list-style-type:initial;-webkit-margin-start:1em;margin-inline-start:1em;padding-left:var(--chakra-space-4);}</style><ul role="list" class="css-1h3xlp0"><li class="css-0">Semi-convolutional Operators for Instance Segmentation [David Novotny, Samuel Albanie, Diane Larlus, and Andrea Vedaldi. ECCV 2018]</li><li class="css-0"><a class="chakra-link css-f4h6uy" href="https://arxiv.org/abs/1807.10712"><span class="chakra-text css-hn1ydt">https://arxiv.org/abs/1807.10712</span></a></li></ul>
<p class="chakra-text css-ykzoaw">（文中の図表は論文より引用しています）</p>
<h2 class="chakra-heading css-nppbz4" id="instance-segmentation"><div class="css-70qvj9"><a class="chakra-link css-1awjy5h" href="#instance-segmentation"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" focusable="false" class="chakra-icon css-ifa7ya" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"></path></svg></a>Instance Segmentation</div></h2>
<p class="chakra-text css-ykzoaw">まずはじめに簡単に Instance Segmentation というタスクと、現在主流とされているアプローチについて述べます。</p>
<p class="chakra-text css-ykzoaw">Instance Segmentation とは、画像の各 Pixel について、 <strong>どのクラスに属すか、どのインスタンスに属するか</strong> を予測するタスクです。
入力画像を「この領域は人、この領域は車、...」というように色塗りしていくタスクです。</p>
<p class="chakra-text css-ykzoaw"><img src="https://qiita-image-store.s3.amazonaws.com/0/39030/146f0988-659b-4f15-df32-e065ddae5e70.png" alt="image.png"/>(Fig. 5 より)</p>
<p class="chakra-text css-ykzoaw">Instance Segmentation において重要なのが <strong>どのインスタンスに属するか</strong> も予測しなければならないという点です。
たとえば人が 3 人で肩を組んでいるような画像の場合、どこからどこまでが 1 人目かを予測しなければなりません。
一方、インスタンスを考慮せず色塗りをしていくようなタスクを Semantic Segmentation といいます。</p>
<p class="chakra-text css-ykzoaw">Semantic Segmentation の場合は、入力画像の各 Pixel について多クラス分類を行えば Segmentation の完成になります。
Instance Segmentation ではそれに加えて個々のインスタンスを区別するような仕組みが必要になります。</p>
<style data-emotion="css 1m5epeg">.css-1m5epeg{font-family:var(--chakra-fonts-heading);font-weight:var(--chakra-fontWeights-bold);font-size:large;line-height:1.33;padding-top:var(--chakra-space-2);padding-bottom:var(--chakra-space-2);}@media screen and (min-width: 48em){.css-1m5epeg{line-height:1.2;}}</style><h2 class="chakra-heading css-1m5epeg" id="propose--verify"><div class="css-70qvj9"><a class="chakra-link css-1awjy5h" href="#propose--verify"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" focusable="false" class="chakra-icon css-ifa7ya" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"></path></svg></a>propose &amp; verify</div></h2>
<p class="chakra-text css-ykzoaw">Instance Segmentation タスクへのアプローチとして、現在主流とされているのは Mask R-CNN [^1] に代表される Region based な手法です。
（Mask R-CNN は FAIR から出ている論文で、 OSS として公開されている Detectron に実装が含まれています。 <a class="chakra-link css-f4h6uy" href="https://github.com/facebookresearch/Detectron"><span class="chakra-text css-hn1ydt">https://github.com/facebookresearch/Detectron</span></a> ）</p>
<p class="chakra-text css-ykzoaw">[^1]: K. He, et al., <a class="chakra-link css-f4h6uy" href="https://arxiv.org/abs/1703.06870"><span class="chakra-text css-hn1ydt">https://arxiv.org/abs/1703.06870</span></a></p>
<p class="chakra-text css-ykzoaw">Mask R-CNN は、物体のクラスと bounding box だけを予測する Object Detection タスクへのアプローチを応用しています。
まず Object Detection をすることで「この bounding box に人間が 1 人いる」ということを予測し、その後 bounding box 内を色塗りしていきます。
Object Detection として bounding box を予測している時点で Instance を分離することが出来ています。色塗りのフェーズでは、すでに Instance が分離されているので単なる Pixel 単位の 2 クラス分類をやればよいことになります。</p>
<p class="chakra-text css-ykzoaw">はじめに Region を提案し、その中を精査するこれらの手法を、この論文では <em>propose &amp; verify</em> (P&amp;V) と呼んでいます。</p>
<p class="chakra-text css-ykzoaw">ここで、 <strong>P&amp;V は必ず一度矩形で切り取ってから色塗りをしなければならない</strong> という点が問題になります。
予測したい物体は必ずしも矩形で近似できるような形状をしているとは限りません。
実際の形状と極端にかけ離れた場合、bounding box を予測すること自体が難しく、また Instance の分離も難しくなります。</p>
<p class="chakra-text css-ykzoaw"><img src="https://qiita-image-store.s3.amazonaws.com/0/39030/39b808c2-cc63-a245-73c1-5b6ebc89c9be.png" alt="image.png"/></p>
<h2 class="chakra-heading css-1m5epeg" id="instance-coloring"><div class="css-70qvj9"><a class="chakra-link css-1awjy5h" href="#instance-coloring"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" focusable="false" class="chakra-icon css-ifa7ya" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"></path></svg></a>instance coloring</div></h2>
<p class="chakra-text css-ykzoaw">P&amp;V の問題点を解決する方法として、Pixel ごとに <strong>ラベル + Instance の identifier となる何か</strong> を予測する方法があります。
これらをこの論文では <em>instance coloring</em> (IC) と呼んでいます。</p>
<p class="chakra-text css-ykzoaw">「Instance の identifier となる何か」 は、連番などではうまく学習できません（どの Object が ID 1 なのか ID 2 なのかわからない）。
そこで、 Pixel ごとに低次元の embedding を出力し、<strong>同じ Instance に所属する Pixel の embedding たちが似たものになるように学習します</strong>。
入力画像に対して、Pixel ごとのラベルと embedding を出力し、embedding を基に Pixel たちをクラスタリングすることで Instance を分離します。</p>
<p class="chakra-text css-ykzoaw">IC の良いところは、典型的な image-to-image の問題と同じネットワーク構造を利用できるところです。
Semantic Segmetation, Style Transfer など、画像を入力とし同じサイズの feature map を出力とするタスクは他にも数多くあり、それらと同じ構造をシンプルに流用できるのは大きな利点になります。
（P&amp;V の場合は Region Proposal + Region ごとの Coloring が必要で、ネットワーク構造としてはかなり複雑かつ独特なものになります）</p>
<p class="chakra-text css-ykzoaw">一方、IC であまり精度が出ない大きな理由の一つに <strong>画像的に似た領域が繰り返されると Instance の分離に失敗する</strong> という問題があります。
image-to-image のネットワークは通常 Convolutional operators をベースにしていますが、CNN の出力は、入力である pixel の特徴量にのみ依存し、 <strong>座標は全く結果に影響を及ぼしません</strong> 。
そのため、画像的にそっくりな領域が複数あると、それらの pixel に対する embedding は同じような値になってしまい、クラスタリングがうまくいきません。</p>
<h2 class="chakra-heading css-nppbz4" id="semi-convolutional-operators"><div class="css-70qvj9"><a class="chakra-link css-1awjy5h" href="#semi-convolutional-operators"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" focusable="false" class="chakra-icon css-ifa7ya" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"></path></svg></a>Semi-convolutional operators</div></h2>
<p class="chakra-text css-ykzoaw">一般的な IC では、出力された embedding が次の条件をみたすことを目標とします。</p>
<p class="chakra-text css-ykzoaw"><img src="https://qiita-image-store.s3.amazonaws.com/0/39030/00e031cb-860d-d25c-af24-213ad5fd8565.png" alt="image.png"/></p>
<p class="chakra-text css-ykzoaw">ここで、 $\Omega$ は全 Pixel の集合、 $x$ は入力画像、 $\Phi$ は学習したい関数（NN）、 $S_k$ はクラス k の segmentation mask、 $M$ はマージン （hyperparameter） です。
言葉で説明すると、 <strong>同じクラスに属する Pixel $u$, $v$ の embedding の距離をより近づけ、違うクラスに属する場合はより遠ざける</strong> という感じです。
$M$ は分離境界をよりくっきりさせるためのパラメータです。</p>
<p class="chakra-text css-ykzoaw">さきほど述べたように $\Phi$ は CNN であり、座標情報を加味できません。
Semi-convolutinal 版では、 $\Phi$ の代わりに次のような $\Psi$ を考えます。</p>
<div class="remark-highlight"><pre class="language-math"><code class="language-math">\Psi_u(x) = f(\Phi_u(x), u)</code></pre></div>
<p class="chakra-text css-ykzoaw">ここで、 $u$ は Pixel の座標を表し、 $f$ は $\Phi$ の結果と座標情報を合成するなんらかの関数です。
$f$ の簡単な例としては、単純な足し算が考えられます。
$\Psi$ は、CNN の結果に加えて座標情報も持ち合わせているため、IC の弱点を克服できています。
$f$ を単純な加算とし、うまく学習が成功した場合、各 Instance ごとに centroid $c_k$ が決定され、</p>
<div class="remark-highlight"><pre class="language-math"><code class="language-math">\forall u \in S_k: \Phi_u(x) + u = c_k</code></pre></div>
<p class="chakra-text css-ykzoaw">となるように $\Phi$ が学習されます。
これを可視化すると次の画像のようになります。</p>
<p class="chakra-text css-ykzoaw"><img src="https://qiita-image-store.s3.amazonaws.com/0/39030/74875b7b-be1b-4eb3-0e3d-f3c53fced5a8.png" alt="image.png"/>
(Fig.2 より)</p>
<p class="chakra-text css-ykzoaw">各インスタンス内の Pixel から、なんとなく中心っぽい場所へベクトルが伸びているのがわかります。</p>
<p class="chakra-text css-ykzoaw">実際の学習の際の損失関数は次のようになります。</p>
<img src="https://qiita-image-store.s3.amazonaws.com/0/39030/914e8f93-3c56-8d39-270e-1a5f3703bc79.png" width="60%"/>
<p class="chakra-text css-ykzoaw">同じインスタンスに属する Pixel の embedding たちを平均値になるべく近づける、というのが損失関数になります。
（マージンの考えも含まれていないし、「違うインスタンスとの距離を取る」という損失も含まれていないですが、これで十分に良い学習ができたと述べられています。）</p>
<p class="chakra-text css-ykzoaw">実際にはもうちょっと複雑な $\Psi$ や距離の定義を使っていますが、概要としては上記のようなものを Semi-convolutional operators として提案しています。</p>
<h2 class="chakra-heading css-nppbz4" id="experiments"><div class="css-70qvj9"><a class="chakra-link css-1awjy5h" href="#experiments"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" focusable="false" class="chakra-icon css-ifa7ya" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"></path></svg></a>Experiments</div></h2>
<p class="chakra-text css-ykzoaw">Mask R-CNN との統合もこの論文の重要な topic なのですが、ぶっちゃけ論文を読んだほうがわかりやすいので飛ばして実験結果をざーっと眺めてみます。</p>
<p class="chakra-text css-ykzoaw"><img src="https://qiita-image-store.s3.amazonaws.com/0/39030/e7516b68-9a66-154b-45ae-61199e0de90a.png" alt="image.png"/>(Fig. 3 より)</p>
<p class="chakra-text css-ykzoaw">まずはじめに、 画像的にそっくりな領域が繰り返されてもうまく Instance を分離できることを確認しています。
(c) は通常の Conv. のみを使って IC を行った場合の結果です。クラスタリングに大失敗していることがわかります。
一方 (d) の Semi-conv. 版ではきれいな分離が実現されています。</p>
<p class="chakra-text css-ykzoaw"><img src="https://qiita-image-store.s3.amazonaws.com/0/39030/09bb8f76-5ba7-ad51-af97-2dbfc902cd66.png" alt="image.png"/></p>
<p class="chakra-text css-ykzoaw">つぎに線虫の segmentation です。こちらは P&amp;V のように矩形で認識するタイプの手法がニガテとするようなタスクです。
現在主流である Mask RCNN よりも良い結果が示されています。</p>
<p class="chakra-text css-ykzoaw"><img src="https://qiita-image-store.s3.amazonaws.com/0/39030/77a2f7fe-ee56-13ae-f49b-faf825cbf406.png" alt="image.png"/></p>
<p class="chakra-text css-ykzoaw">より一般的なデータである PASCAL VOC2012 に対しても Mask RCNN より良い結果となっています（Mask RCNN に Semi-conv. の仕組みを組み込んだもので比較しています。）</p>
<h2 class="chakra-heading css-nppbz4" id="まとめと感想"><div class="css-70qvj9"><a class="chakra-link css-1awjy5h" href="#まとめと感想"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" focusable="false" class="chakra-icon css-ifa7ya" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M326.612 185.391c59.747 59.809 58.927 155.698.36 214.59-.11.12-.24.25-.36.37l-67.2 67.2c-59.27 59.27-155.699 59.262-214.96 0-59.27-59.26-59.27-155.7 0-214.96l37.106-37.106c9.84-9.84 26.786-3.3 27.294 10.606.648 17.722 3.826 35.527 9.69 52.721 1.986 5.822.567 12.262-3.783 16.612l-13.087 13.087c-28.026 28.026-28.905 73.66-1.155 101.96 28.024 28.579 74.086 28.749 102.325.51l67.2-67.19c28.191-28.191 28.073-73.757 0-101.83-3.701-3.694-7.429-6.564-10.341-8.569a16.037 16.037 0 0 1-6.947-12.606c-.396-10.567 3.348-21.456 11.698-29.806l21.054-21.055c5.521-5.521 14.182-6.199 20.584-1.731a152.482 152.482 0 0 1 20.522 17.197zM467.547 44.449c-59.261-59.262-155.69-59.27-214.96 0l-67.2 67.2c-.12.12-.25.25-.36.37-58.566 58.892-59.387 154.781.36 214.59a152.454 152.454 0 0 0 20.521 17.196c6.402 4.468 15.064 3.789 20.584-1.731l21.054-21.055c8.35-8.35 12.094-19.239 11.698-29.806a16.037 16.037 0 0 0-6.947-12.606c-2.912-2.005-6.64-4.875-10.341-8.569-28.073-28.073-28.191-73.639 0-101.83l67.2-67.19c28.239-28.239 74.3-28.069 102.325.51 27.75 28.3 26.872 73.934-1.155 101.96l-13.087 13.087c-4.35 4.35-5.769 10.79-3.783 16.612 5.864 17.194 9.042 34.999 9.69 52.721.509 13.906 17.454 20.446 27.294 10.606l37.106-37.106c59.271-59.259 59.271-155.699.001-214.959z"></path></svg></a>まとめと感想</div></h2>
<p class="chakra-text css-ykzoaw">instance coloring の手法をまったく知らなかったのですが、 <a class="chakra-link css-f4h6uy" href="https://arxiv.org/abs/1808.01244"><span class="chakra-text css-hn1ydt">CornerNet: Detecting Objects as Paired Keypoints</span></a> で Pixel ごとの embedding をクラスタリングしてペアを作るという手法を知り、興味を持ったのでその関連で読んでみた論文です。
P&amp;V 形式はかなり複雑な構造になるので、それを避けられるならすごく面白いなと思ったのですが、この論文では Mask R-CNN と組み合わせることで精度向上と言っているので、まだまだ IC 単体で勝てる感じではないのでしょうか？</p>
<p class="chakra-text css-ykzoaw">同じタスクに対して全く違う 2 つのアプローチが（比較対象になるくらいには）同じような成果を出しているのも面白いところです。segmentation は主流ではなかった分、まだまだ改善がありそうで楽しみです。</p>
</div></div><hr aria-orientation="horizontal" class="chakra-divider css-8zn9ea"/><style data-emotion="css 1fgm4wo">.css-1fgm4wo{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-box-pack:center;-ms-flex-pack:center;-webkit-justify-content:center;justify-content:center;padding-top:var(--chakra-space-2);}</style><div class="css-1fgm4wo"><div class="css-17xejub"></div><style data-emotion="css rjrdo">.css-rjrdo{transition-property:var(--chakra-transition-property-common);transition-duration:var(--chakra-transition-duration-fast);transition-timing-function:var(--chakra-transition-easing-ease-out);cursor:pointer;-webkit-text-decoration:none;text-decoration:none;outline:2px solid transparent;outline-offset:2px;color:var(--chakra-colors-green-500);}.css-rjrdo:hover,.css-rjrdo[data-hover]{-webkit-text-decoration:underline;text-decoration:underline;}.css-rjrdo:focus,.css-rjrdo[data-focus]{box-shadow:var(--chakra-shadows-outline);}</style><a class="chakra-link css-rjrdo" href="https://github.com/agatan/agatan.github.io/blob/main/posts/【論文読み】Semi-convolutional_Operators_for_Instance_Segmentation.md"><style data-emotion="css 6ey7w3">.css-6ey7w3{width:var(--chakra-sizes-6);height:var(--chakra-sizes-6);display:inline-block;line-height:1em;-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;color:currentColor;}</style><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" focusable="false" class="chakra-icon css-6ey7w3" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg><style data-emotion="css 1syq16t">.css-1syq16t{padding:var(--chakra-space-2);}</style><span class="chakra-text css-1syq16t">GitHub で編集リクエスト</span></a></div></div></div><style data-emotion="css ul4ode">.css-ul4ode{width:100%;-webkit-margin-start:auto;margin-inline-start:auto;-webkit-margin-end:auto;margin-inline-end:auto;max-width:var(--chakra-sizes-44);-webkit-padding-start:1rem;padding-inline-start:1rem;-webkit-padding-end:1rem;padding-inline-end:1rem;}</style><div class="chakra-container css-ul4ode"><style data-emotion="css xkvmla">.css-xkvmla{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;padding:var(--chakra-space-4);}</style><div class="css-xkvmla"><a class="chakra-link css-f4h6uy" href="https://twitter.com/@agatan_"><style data-emotion="css 1piy145">.css-1piy145{position:relative;width:var(--chakra-sizes-20);height:var(--chakra-sizes-20);}</style><div class="css-1piy145"><style data-emotion="css 1phd9a0">.css-1phd9a0{object-fit:cover;}</style><img alt="agatan" class="chakra-image__placeholder css-1phd9a0" layout="fill"/></div><style data-emotion="css 19dx7hn">.css-19dx7hn{text-align:center;font-size:large;}</style><p class="chakra-text css-19dx7hn">@agatan</p></a><style data-emotion="css 84zodg">.css-84zodg{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;}.css-84zodg>*:not(style)~*:not(style){margin-top:0px;-webkit-margin-end:0px;margin-inline-end:0px;margin-bottom:0px;-webkit-margin-start:0.5rem;margin-inline-start:0.5rem;}</style><div class="chakra-stack css-84zodg"><a class="chakra-link css-1h1f62l" href="https://twitter.com/@agatan_"><style data-emotion="css 15e9ude">.css-15e9ude{width:1.2em;height:1.2em;display:inline-block;line-height:1em;-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;color:currentColor;}</style><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" focusable="true" class="chakra-icon css-15e9ude" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"></path></svg></a><a class="chakra-link css-1h1f62l" href="https://github.com/agatan"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" focusable="true" class="chakra-icon css-15e9ude" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a></div></div></div></div></div></div><span></span></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"post":{"content":"\u003cp\u003eInstance Segmentation のタスクに対する手法を整理・分解し、精度をより向上する \u003ccode\u003eSemi-convolutional operators\u003c/code\u003e を提案した論文です。\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://qiita-image-store.s3.amazonaws.com/0/39030/8164ed4c-3f5d-c772-e21d-7d02d5146461.png\" alt=\"image.png\"\u003e\u003c/p\u003e\n\u003cp\u003eこの記事は、Wantedly の勉強会で取り上げられた論文・技術をまとめたものです。\n\u003ca href=\"https://qiita.com/advent-calendar/2018/wantedly_ml\"\u003e2018 年に読んだ機械学習系論文・技術まとめ at Wantedly Advent Calendar 2018 - Qiita\u003c/a\u003e\u003c/p\u003e\n\u003ch2\u003eReference\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eSemi-convolutional Operators for Instance Segmentation [David Novotny, Samuel Albanie, Diane Larlus, and Andrea Vedaldi. ECCV 2018]\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/abs/1807.10712\"\u003ehttps://arxiv.org/abs/1807.10712\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e（文中の図表は論文より引用しています）\u003c/p\u003e\n\u003ch2\u003eInstance Segmentation\u003c/h2\u003e\n\u003cp\u003eまずはじめに簡単に Instance Segmentation というタスクと、現在主流とされているアプローチについて述べます。\u003c/p\u003e\n\u003cp\u003eInstance Segmentation とは、画像の各 Pixel について、 \u003cstrong\u003eどのクラスに属すか、どのインスタンスに属するか\u003c/strong\u003e を予測するタスクです。\n入力画像を「この領域は人、この領域は車、...」というように色塗りしていくタスクです。\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://qiita-image-store.s3.amazonaws.com/0/39030/146f0988-659b-4f15-df32-e065ddae5e70.png\" alt=\"image.png\"\u003e(Fig. 5 より)\u003c/p\u003e\n\u003cp\u003eInstance Segmentation において重要なのが \u003cstrong\u003eどのインスタンスに属するか\u003c/strong\u003e も予測しなければならないという点です。\nたとえば人が 3 人で肩を組んでいるような画像の場合、どこからどこまでが 1 人目かを予測しなければなりません。\n一方、インスタンスを考慮せず色塗りをしていくようなタスクを Semantic Segmentation といいます。\u003c/p\u003e\n\u003cp\u003eSemantic Segmentation の場合は、入力画像の各 Pixel について多クラス分類を行えば Segmentation の完成になります。\nInstance Segmentation ではそれに加えて個々のインスタンスを区別するような仕組みが必要になります。\u003c/p\u003e\n\u003ch3\u003epropose \u0026#x26; verify\u003c/h3\u003e\n\u003cp\u003eInstance Segmentation タスクへのアプローチとして、現在主流とされているのは Mask R-CNN [^1] に代表される Region based な手法です。\n（Mask R-CNN は FAIR から出ている論文で、 OSS として公開されている Detectron に実装が含まれています。 \u003ca href=\"https://github.com/facebookresearch/Detectron\"\u003ehttps://github.com/facebookresearch/Detectron\u003c/a\u003e ）\u003c/p\u003e\n\u003cp\u003e[^1]: K. He, et al., \u003ca href=\"https://arxiv.org/abs/1703.06870\"\u003ehttps://arxiv.org/abs/1703.06870\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003eMask R-CNN は、物体のクラスと bounding box だけを予測する Object Detection タスクへのアプローチを応用しています。\nまず Object Detection をすることで「この bounding box に人間が 1 人いる」ということを予測し、その後 bounding box 内を色塗りしていきます。\nObject Detection として bounding box を予測している時点で Instance を分離することが出来ています。色塗りのフェーズでは、すでに Instance が分離されているので単なる Pixel 単位の 2 クラス分類をやればよいことになります。\u003c/p\u003e\n\u003cp\u003eはじめに Region を提案し、その中を精査するこれらの手法を、この論文では \u003cem\u003epropose \u0026#x26; verify\u003c/em\u003e (P\u0026#x26;V) と呼んでいます。\u003c/p\u003e\n\u003cp\u003eここで、 \u003cstrong\u003eP\u0026#x26;V は必ず一度矩形で切り取ってから色塗りをしなければならない\u003c/strong\u003e という点が問題になります。\n予測したい物体は必ずしも矩形で近似できるような形状をしているとは限りません。\n実際の形状と極端にかけ離れた場合、bounding box を予測すること自体が難しく、また Instance の分離も難しくなります。\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://qiita-image-store.s3.amazonaws.com/0/39030/39b808c2-cc63-a245-73c1-5b6ebc89c9be.png\" alt=\"image.png\"\u003e\u003c/p\u003e\n\u003ch3\u003einstance coloring\u003c/h3\u003e\n\u003cp\u003eP\u0026#x26;V の問題点を解決する方法として、Pixel ごとに \u003cstrong\u003eラベル + Instance の identifier となる何か\u003c/strong\u003e を予測する方法があります。\nこれらをこの論文では \u003cem\u003einstance coloring\u003c/em\u003e (IC) と呼んでいます。\u003c/p\u003e\n\u003cp\u003e「Instance の identifier となる何か」 は、連番などではうまく学習できません（どの Object が ID 1 なのか ID 2 なのかわからない）。\nそこで、 Pixel ごとに低次元の embedding を出力し、\u003cstrong\u003e同じ Instance に所属する Pixel の embedding たちが似たものになるように学習します\u003c/strong\u003e。\n入力画像に対して、Pixel ごとのラベルと embedding を出力し、embedding を基に Pixel たちをクラスタリングすることで Instance を分離します。\u003c/p\u003e\n\u003cp\u003eIC の良いところは、典型的な image-to-image の問題と同じネットワーク構造を利用できるところです。\nSemantic Segmetation, Style Transfer など、画像を入力とし同じサイズの feature map を出力とするタスクは他にも数多くあり、それらと同じ構造をシンプルに流用できるのは大きな利点になります。\n（P\u0026#x26;V の場合は Region Proposal + Region ごとの Coloring が必要で、ネットワーク構造としてはかなり複雑かつ独特なものになります）\u003c/p\u003e\n\u003cp\u003e一方、IC であまり精度が出ない大きな理由の一つに \u003cstrong\u003e画像的に似た領域が繰り返されると Instance の分離に失敗する\u003c/strong\u003e という問題があります。\nimage-to-image のネットワークは通常 Convolutional operators をベースにしていますが、CNN の出力は、入力である pixel の特徴量にのみ依存し、 \u003cstrong\u003e座標は全く結果に影響を及ぼしません\u003c/strong\u003e 。\nそのため、画像的にそっくりな領域が複数あると、それらの pixel に対する embedding は同じような値になってしまい、クラスタリングがうまくいきません。\u003c/p\u003e\n\u003ch2\u003eSemi-convolutional operators\u003c/h2\u003e\n\u003cp\u003e一般的な IC では、出力された embedding が次の条件をみたすことを目標とします。\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://qiita-image-store.s3.amazonaws.com/0/39030/00e031cb-860d-d25c-af24-213ad5fd8565.png\" alt=\"image.png\"\u003e\u003c/p\u003e\n\u003cp\u003eここで、 $\\Omega$ は全 Pixel の集合、 $x$ は入力画像、 $\\Phi$ は学習したい関数（NN）、 $S_k$ はクラス k の segmentation mask、 $M$ はマージン （hyperparameter） です。\n言葉で説明すると、 \u003cstrong\u003e同じクラスに属する Pixel $u$, $v$ の embedding の距離をより近づけ、違うクラスに属する場合はより遠ざける\u003c/strong\u003e という感じです。\n$M$ は分離境界をよりくっきりさせるためのパラメータです。\u003c/p\u003e\n\u003cp\u003eさきほど述べたように $\\Phi$ は CNN であり、座標情報を加味できません。\nSemi-convolutinal 版では、 $\\Phi$ の代わりに次のような $\\Psi$ を考えます。\u003c/p\u003e\n\u003cdiv class=\"remark-highlight\"\u003e\u003cpre class=\"language-math\"\u003e\u003ccode class=\"language-math\"\u003e\\Psi_u(x) = f(\\Phi_u(x), u)\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eここで、 $u$ は Pixel の座標を表し、 $f$ は $\\Phi$ の結果と座標情報を合成するなんらかの関数です。\n$f$ の簡単な例としては、単純な足し算が考えられます。\n$\\Psi$ は、CNN の結果に加えて座標情報も持ち合わせているため、IC の弱点を克服できています。\n$f$ を単純な加算とし、うまく学習が成功した場合、各 Instance ごとに centroid $c_k$ が決定され、\u003c/p\u003e\n\u003cdiv class=\"remark-highlight\"\u003e\u003cpre class=\"language-math\"\u003e\u003ccode class=\"language-math\"\u003e\\forall u \\in S_k: \\Phi_u(x) + u = c_k\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp\u003eとなるように $\\Phi$ が学習されます。\nこれを可視化すると次の画像のようになります。\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://qiita-image-store.s3.amazonaws.com/0/39030/74875b7b-be1b-4eb3-0e3d-f3c53fced5a8.png\" alt=\"image.png\"\u003e\n(Fig.2 より)\u003c/p\u003e\n\u003cp\u003e各インスタンス内の Pixel から、なんとなく中心っぽい場所へベクトルが伸びているのがわかります。\u003c/p\u003e\n\u003cp\u003e実際の学習の際の損失関数は次のようになります。\u003c/p\u003e\n\u003cimg src=\"https://qiita-image-store.s3.amazonaws.com/0/39030/914e8f93-3c56-8d39-270e-1a5f3703bc79.png\" width=\"60%\"\u003e\n\u003cp\u003e同じインスタンスに属する Pixel の embedding たちを平均値になるべく近づける、というのが損失関数になります。\n（マージンの考えも含まれていないし、「違うインスタンスとの距離を取る」という損失も含まれていないですが、これで十分に良い学習ができたと述べられています。）\u003c/p\u003e\n\u003cp\u003e実際にはもうちょっと複雑な $\\Psi$ や距離の定義を使っていますが、概要としては上記のようなものを Semi-convolutional operators として提案しています。\u003c/p\u003e\n\u003ch2\u003eExperiments\u003c/h2\u003e\n\u003cp\u003eMask R-CNN との統合もこの論文の重要な topic なのですが、ぶっちゃけ論文を読んだほうがわかりやすいので飛ばして実験結果をざーっと眺めてみます。\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://qiita-image-store.s3.amazonaws.com/0/39030/e7516b68-9a66-154b-45ae-61199e0de90a.png\" alt=\"image.png\"\u003e(Fig. 3 より)\u003c/p\u003e\n\u003cp\u003eまずはじめに、 画像的にそっくりな領域が繰り返されてもうまく Instance を分離できることを確認しています。\n(c) は通常の Conv. のみを使って IC を行った場合の結果です。クラスタリングに大失敗していることがわかります。\n一方 (d) の Semi-conv. 版ではきれいな分離が実現されています。\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://qiita-image-store.s3.amazonaws.com/0/39030/09bb8f76-5ba7-ad51-af97-2dbfc902cd66.png\" alt=\"image.png\"\u003e\u003c/p\u003e\n\u003cp\u003eつぎに線虫の segmentation です。こちらは P\u0026#x26;V のように矩形で認識するタイプの手法がニガテとするようなタスクです。\n現在主流である Mask RCNN よりも良い結果が示されています。\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://qiita-image-store.s3.amazonaws.com/0/39030/77a2f7fe-ee56-13ae-f49b-faf825cbf406.png\" alt=\"image.png\"\u003e\u003c/p\u003e\n\u003cp\u003eより一般的なデータである PASCAL VOC2012 に対しても Mask RCNN より良い結果となっています（Mask RCNN に Semi-conv. の仕組みを組み込んだもので比較しています。）\u003c/p\u003e\n\u003ch2\u003eまとめと感想\u003c/h2\u003e\n\u003cp\u003einstance coloring の手法をまったく知らなかったのですが、 \u003ca href=\"https://arxiv.org/abs/1808.01244\"\u003eCornerNet: Detecting Objects as Paired Keypoints\u003c/a\u003e で Pixel ごとの embedding をクラスタリングしてペアを作るという手法を知り、興味を持ったのでその関連で読んでみた論文です。\nP\u0026#x26;V 形式はかなり複雑な構造になるので、それを避けられるならすごく面白いなと思ったのですが、この論文では Mask R-CNN と組み合わせることで精度向上と言っているので、まだまだ IC 単体で勝てる感じではないのでしょうか？\u003c/p\u003e\n\u003cp\u003e同じタスクに対して全く違う 2 つのアプローチが（比較対象になるくらいには）同じような成果を出しているのも面白いところです。segmentation は主流ではなかった分、まだまだ改善がありそうで楽しみです。\u003c/p\u003e\n","meta":{"rawMarkdown":"---\ntitle: \"【論文読み】Semi-convolutional Operators for Instance Segmentation\"\ndate: 2019-02-11T16:59:46+09:00\ntags: [\"DeepLearning\", \"論文読み\"]\nurl: https://qiita.com/agatan/items/2cf1209b7370db45eba5\n---\n\nInstance Segmentation のタスクに対する手法を整理・分解し、精度をより向上する `Semi-convolutional operators` を提案した論文です。\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/8164ed4c-3f5d-c772-e21d-7d02d5146461.png)\n\nこの記事は、Wantedly の勉強会で取り上げられた論文・技術をまとめたものです。\n[2018 年に読んだ機械学習系論文・技術まとめ at Wantedly Advent Calendar 2018 - Qiita](https://qiita.com/advent-calendar/2018/wantedly_ml)\n\n## Reference\n\n- Semi-convolutional Operators for Instance Segmentation [David Novotny, Samuel Albanie, Diane Larlus, and Andrea Vedaldi. ECCV 2018]\n- https://arxiv.org/abs/1807.10712\n\n（文中の図表は論文より引用しています）\n\n## Instance Segmentation\n\nまずはじめに簡単に Instance Segmentation というタスクと、現在主流とされているアプローチについて述べます。\n\nInstance Segmentation とは、画像の各 Pixel について、 **どのクラスに属すか、どのインスタンスに属するか** を予測するタスクです。\n入力画像を「この領域は人、この領域は車、...」というように色塗りしていくタスクです。\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/146f0988-659b-4f15-df32-e065ddae5e70.png)(Fig. 5 より)\n\nInstance Segmentation において重要なのが **どのインスタンスに属するか** も予測しなければならないという点です。\nたとえば人が 3 人で肩を組んでいるような画像の場合、どこからどこまでが 1 人目かを予測しなければなりません。\n一方、インスタンスを考慮せず色塗りをしていくようなタスクを Semantic Segmentation といいます。\n\nSemantic Segmentation の場合は、入力画像の各 Pixel について多クラス分類を行えば Segmentation の完成になります。\nInstance Segmentation ではそれに加えて個々のインスタンスを区別するような仕組みが必要になります。\n\n### propose \u0026 verify\n\nInstance Segmentation タスクへのアプローチとして、現在主流とされているのは Mask R-CNN [^1] に代表される Region based な手法です。\n（Mask R-CNN は FAIR から出ている論文で、 OSS として公開されている Detectron に実装が含まれています。 https://github.com/facebookresearch/Detectron ）\n\n[^1]: K. He, et al., https://arxiv.org/abs/1703.06870\n\nMask R-CNN は、物体のクラスと bounding box だけを予測する Object Detection タスクへのアプローチを応用しています。\nまず Object Detection をすることで「この bounding box に人間が 1 人いる」ということを予測し、その後 bounding box 内を色塗りしていきます。\nObject Detection として bounding box を予測している時点で Instance を分離することが出来ています。色塗りのフェーズでは、すでに Instance が分離されているので単なる Pixel 単位の 2 クラス分類をやればよいことになります。\n\nはじめに Region を提案し、その中を精査するこれらの手法を、この論文では _propose \u0026 verify_ (P\u0026V) と呼んでいます。\n\nここで、 **P\u0026V は必ず一度矩形で切り取ってから色塗りをしなければならない** という点が問題になります。\n予測したい物体は必ずしも矩形で近似できるような形状をしているとは限りません。\n実際の形状と極端にかけ離れた場合、bounding box を予測すること自体が難しく、また Instance の分離も難しくなります。\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/39b808c2-cc63-a245-73c1-5b6ebc89c9be.png)\n\n### instance coloring\n\nP\u0026V の問題点を解決する方法として、Pixel ごとに **ラベル + Instance の identifier となる何か** を予測する方法があります。\nこれらをこの論文では _instance coloring_ (IC) と呼んでいます。\n\n「Instance の identifier となる何か」 は、連番などではうまく学習できません（どの Object が ID 1 なのか ID 2 なのかわからない）。\nそこで、 Pixel ごとに低次元の embedding を出力し、**同じ Instance に所属する Pixel の embedding たちが似たものになるように学習します**。\n入力画像に対して、Pixel ごとのラベルと embedding を出力し、embedding を基に Pixel たちをクラスタリングすることで Instance を分離します。\n\nIC の良いところは、典型的な image-to-image の問題と同じネットワーク構造を利用できるところです。\nSemantic Segmetation, Style Transfer など、画像を入力とし同じサイズの feature map を出力とするタスクは他にも数多くあり、それらと同じ構造をシンプルに流用できるのは大きな利点になります。\n（P\u0026V の場合は Region Proposal + Region ごとの Coloring が必要で、ネットワーク構造としてはかなり複雑かつ独特なものになります）\n\n一方、IC であまり精度が出ない大きな理由の一つに **画像的に似た領域が繰り返されると Instance の分離に失敗する** という問題があります。\nimage-to-image のネットワークは通常 Convolutional operators をベースにしていますが、CNN の出力は、入力である pixel の特徴量にのみ依存し、 **座標は全く結果に影響を及ぼしません** 。\nそのため、画像的にそっくりな領域が複数あると、それらの pixel に対する embedding は同じような値になってしまい、クラスタリングがうまくいきません。\n\n## Semi-convolutional operators\n\n一般的な IC では、出力された embedding が次の条件をみたすことを目標とします。\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/00e031cb-860d-d25c-af24-213ad5fd8565.png)\n\nここで、 $\\Omega$ は全 Pixel の集合、 $x$ は入力画像、 $\\Phi$ は学習したい関数（NN）、 $S_k$ はクラス k の segmentation mask、 $M$ はマージン （hyperparameter） です。\n言葉で説明すると、 **同じクラスに属する Pixel $u$, $v$ の embedding の距離をより近づけ、違うクラスに属する場合はより遠ざける** という感じです。\n$M$ は分離境界をよりくっきりさせるためのパラメータです。\n\nさきほど述べたように $\\Phi$ は CNN であり、座標情報を加味できません。\nSemi-convolutinal 版では、 $\\Phi$ の代わりに次のような $\\Psi$ を考えます。\n\n```math\n\\Psi_u(x) = f(\\Phi_u(x), u)\n```\n\nここで、 $u$ は Pixel の座標を表し、 $f$ は $\\Phi$ の結果と座標情報を合成するなんらかの関数です。\n$f$ の簡単な例としては、単純な足し算が考えられます。\n$\\Psi$ は、CNN の結果に加えて座標情報も持ち合わせているため、IC の弱点を克服できています。\n$f$ を単純な加算とし、うまく学習が成功した場合、各 Instance ごとに centroid $c_k$ が決定され、\n\n```math\n\\forall u \\in S_k: \\Phi_u(x) + u = c_k\n```\n\nとなるように $\\Phi$ が学習されます。\nこれを可視化すると次の画像のようになります。\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/74875b7b-be1b-4eb3-0e3d-f3c53fced5a8.png)\n(Fig.2 より)\n\n各インスタンス内の Pixel から、なんとなく中心っぽい場所へベクトルが伸びているのがわかります。\n\n実際の学習の際の損失関数は次のようになります。\n\n\u003cimg src=\"https://qiita-image-store.s3.amazonaws.com/0/39030/914e8f93-3c56-8d39-270e-1a5f3703bc79.png\" width=\"60%\"\u003e\n\n同じインスタンスに属する Pixel の embedding たちを平均値になるべく近づける、というのが損失関数になります。\n（マージンの考えも含まれていないし、「違うインスタンスとの距離を取る」という損失も含まれていないですが、これで十分に良い学習ができたと述べられています。）\n\n実際にはもうちょっと複雑な $\\Psi$ や距離の定義を使っていますが、概要としては上記のようなものを Semi-convolutional operators として提案しています。\n\n## Experiments\n\nMask R-CNN との統合もこの論文の重要な topic なのですが、ぶっちゃけ論文を読んだほうがわかりやすいので飛ばして実験結果をざーっと眺めてみます。\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/e7516b68-9a66-154b-45ae-61199e0de90a.png)(Fig. 3 より)\n\nまずはじめに、 画像的にそっくりな領域が繰り返されてもうまく Instance を分離できることを確認しています。\n(c) は通常の Conv. のみを使って IC を行った場合の結果です。クラスタリングに大失敗していることがわかります。\n一方 (d) の Semi-conv. 版ではきれいな分離が実現されています。\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/09bb8f76-5ba7-ad51-af97-2dbfc902cd66.png)\n\nつぎに線虫の segmentation です。こちらは P\u0026V のように矩形で認識するタイプの手法がニガテとするようなタスクです。\n現在主流である Mask RCNN よりも良い結果が示されています。\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/77a2f7fe-ee56-13ae-f49b-faf825cbf406.png)\n\nより一般的なデータである PASCAL VOC2012 に対しても Mask RCNN より良い結果となっています（Mask RCNN に Semi-conv. の仕組みを組み込んだもので比較しています。）\n\n## まとめと感想\n\ninstance coloring の手法をまったく知らなかったのですが、 [CornerNet: Detecting Objects as Paired Keypoints](https://arxiv.org/abs/1808.01244) で Pixel ごとの embedding をクラスタリングしてペアを作るという手法を知り、興味を持ったのでその関連で読んでみた論文です。\nP\u0026V 形式はかなり複雑な構造になるので、それを避けられるならすごく面白いなと思ったのですが、この論文では Mask R-CNN と組み合わせることで精度向上と言っているので、まだまだ IC 単体で勝てる感じではないのでしょうか？\n\n同じタスクに対して全く違う 2 つのアプローチが（比較対象になるくらいには）同じような成果を出しているのも面白いところです。segmentation は主流ではなかった分、まだまだ改善がありそうで楽しみです。\n","contentMarkdown":"\nInstance Segmentation のタスクに対する手法を整理・分解し、精度をより向上する `Semi-convolutional operators` を提案した論文です。\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/8164ed4c-3f5d-c772-e21d-7d02d5146461.png)\n\nこの記事は、Wantedly の勉強会で取り上げられた論文・技術をまとめたものです。\n[2018 年に読んだ機械学習系論文・技術まとめ at Wantedly Advent Calendar 2018 - Qiita](https://qiita.com/advent-calendar/2018/wantedly_ml)\n\n## Reference\n\n- Semi-convolutional Operators for Instance Segmentation [David Novotny, Samuel Albanie, Diane Larlus, and Andrea Vedaldi. ECCV 2018]\n- https://arxiv.org/abs/1807.10712\n\n（文中の図表は論文より引用しています）\n\n## Instance Segmentation\n\nまずはじめに簡単に Instance Segmentation というタスクと、現在主流とされているアプローチについて述べます。\n\nInstance Segmentation とは、画像の各 Pixel について、 **どのクラスに属すか、どのインスタンスに属するか** を予測するタスクです。\n入力画像を「この領域は人、この領域は車、...」というように色塗りしていくタスクです。\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/146f0988-659b-4f15-df32-e065ddae5e70.png)(Fig. 5 より)\n\nInstance Segmentation において重要なのが **どのインスタンスに属するか** も予測しなければならないという点です。\nたとえば人が 3 人で肩を組んでいるような画像の場合、どこからどこまでが 1 人目かを予測しなければなりません。\n一方、インスタンスを考慮せず色塗りをしていくようなタスクを Semantic Segmentation といいます。\n\nSemantic Segmentation の場合は、入力画像の各 Pixel について多クラス分類を行えば Segmentation の完成になります。\nInstance Segmentation ではそれに加えて個々のインスタンスを区別するような仕組みが必要になります。\n\n### propose \u0026 verify\n\nInstance Segmentation タスクへのアプローチとして、現在主流とされているのは Mask R-CNN [^1] に代表される Region based な手法です。\n（Mask R-CNN は FAIR から出ている論文で、 OSS として公開されている Detectron に実装が含まれています。 https://github.com/facebookresearch/Detectron ）\n\n[^1]: K. He, et al., https://arxiv.org/abs/1703.06870\n\nMask R-CNN は、物体のクラスと bounding box だけを予測する Object Detection タスクへのアプローチを応用しています。\nまず Object Detection をすることで「この bounding box に人間が 1 人いる」ということを予測し、その後 bounding box 内を色塗りしていきます。\nObject Detection として bounding box を予測している時点で Instance を分離することが出来ています。色塗りのフェーズでは、すでに Instance が分離されているので単なる Pixel 単位の 2 クラス分類をやればよいことになります。\n\nはじめに Region を提案し、その中を精査するこれらの手法を、この論文では _propose \u0026 verify_ (P\u0026V) と呼んでいます。\n\nここで、 **P\u0026V は必ず一度矩形で切り取ってから色塗りをしなければならない** という点が問題になります。\n予測したい物体は必ずしも矩形で近似できるような形状をしているとは限りません。\n実際の形状と極端にかけ離れた場合、bounding box を予測すること自体が難しく、また Instance の分離も難しくなります。\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/39b808c2-cc63-a245-73c1-5b6ebc89c9be.png)\n\n### instance coloring\n\nP\u0026V の問題点を解決する方法として、Pixel ごとに **ラベル + Instance の identifier となる何か** を予測する方法があります。\nこれらをこの論文では _instance coloring_ (IC) と呼んでいます。\n\n「Instance の identifier となる何か」 は、連番などではうまく学習できません（どの Object が ID 1 なのか ID 2 なのかわからない）。\nそこで、 Pixel ごとに低次元の embedding を出力し、**同じ Instance に所属する Pixel の embedding たちが似たものになるように学習します**。\n入力画像に対して、Pixel ごとのラベルと embedding を出力し、embedding を基に Pixel たちをクラスタリングすることで Instance を分離します。\n\nIC の良いところは、典型的な image-to-image の問題と同じネットワーク構造を利用できるところです。\nSemantic Segmetation, Style Transfer など、画像を入力とし同じサイズの feature map を出力とするタスクは他にも数多くあり、それらと同じ構造をシンプルに流用できるのは大きな利点になります。\n（P\u0026V の場合は Region Proposal + Region ごとの Coloring が必要で、ネットワーク構造としてはかなり複雑かつ独特なものになります）\n\n一方、IC であまり精度が出ない大きな理由の一つに **画像的に似た領域が繰り返されると Instance の分離に失敗する** という問題があります。\nimage-to-image のネットワークは通常 Convolutional operators をベースにしていますが、CNN の出力は、入力である pixel の特徴量にのみ依存し、 **座標は全く結果に影響を及ぼしません** 。\nそのため、画像的にそっくりな領域が複数あると、それらの pixel に対する embedding は同じような値になってしまい、クラスタリングがうまくいきません。\n\n## Semi-convolutional operators\n\n一般的な IC では、出力された embedding が次の条件をみたすことを目標とします。\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/00e031cb-860d-d25c-af24-213ad5fd8565.png)\n\nここで、 $\\Omega$ は全 Pixel の集合、 $x$ は入力画像、 $\\Phi$ は学習したい関数（NN）、 $S_k$ はクラス k の segmentation mask、 $M$ はマージン （hyperparameter） です。\n言葉で説明すると、 **同じクラスに属する Pixel $u$, $v$ の embedding の距離をより近づけ、違うクラスに属する場合はより遠ざける** という感じです。\n$M$ は分離境界をよりくっきりさせるためのパラメータです。\n\nさきほど述べたように $\\Phi$ は CNN であり、座標情報を加味できません。\nSemi-convolutinal 版では、 $\\Phi$ の代わりに次のような $\\Psi$ を考えます。\n\n```math\n\\Psi_u(x) = f(\\Phi_u(x), u)\n```\n\nここで、 $u$ は Pixel の座標を表し、 $f$ は $\\Phi$ の結果と座標情報を合成するなんらかの関数です。\n$f$ の簡単な例としては、単純な足し算が考えられます。\n$\\Psi$ は、CNN の結果に加えて座標情報も持ち合わせているため、IC の弱点を克服できています。\n$f$ を単純な加算とし、うまく学習が成功した場合、各 Instance ごとに centroid $c_k$ が決定され、\n\n```math\n\\forall u \\in S_k: \\Phi_u(x) + u = c_k\n```\n\nとなるように $\\Phi$ が学習されます。\nこれを可視化すると次の画像のようになります。\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/74875b7b-be1b-4eb3-0e3d-f3c53fced5a8.png)\n(Fig.2 より)\n\n各インスタンス内の Pixel から、なんとなく中心っぽい場所へベクトルが伸びているのがわかります。\n\n実際の学習の際の損失関数は次のようになります。\n\n\u003cimg src=\"https://qiita-image-store.s3.amazonaws.com/0/39030/914e8f93-3c56-8d39-270e-1a5f3703bc79.png\" width=\"60%\"\u003e\n\n同じインスタンスに属する Pixel の embedding たちを平均値になるべく近づける、というのが損失関数になります。\n（マージンの考えも含まれていないし、「違うインスタンスとの距離を取る」という損失も含まれていないですが、これで十分に良い学習ができたと述べられています。）\n\n実際にはもうちょっと複雑な $\\Psi$ や距離の定義を使っていますが、概要としては上記のようなものを Semi-convolutional operators として提案しています。\n\n## Experiments\n\nMask R-CNN との統合もこの論文の重要な topic なのですが、ぶっちゃけ論文を読んだほうがわかりやすいので飛ばして実験結果をざーっと眺めてみます。\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/e7516b68-9a66-154b-45ae-61199e0de90a.png)(Fig. 3 より)\n\nまずはじめに、 画像的にそっくりな領域が繰り返されてもうまく Instance を分離できることを確認しています。\n(c) は通常の Conv. のみを使って IC を行った場合の結果です。クラスタリングに大失敗していることがわかります。\n一方 (d) の Semi-conv. 版ではきれいな分離が実現されています。\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/09bb8f76-5ba7-ad51-af97-2dbfc902cd66.png)\n\nつぎに線虫の segmentation です。こちらは P\u0026V のように矩形で認識するタイプの手法がニガテとするようなタスクです。\n現在主流である Mask RCNN よりも良い結果が示されています。\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/77a2f7fe-ee56-13ae-f49b-faf825cbf406.png)\n\nより一般的なデータである PASCAL VOC2012 に対しても Mask RCNN より良い結果となっています（Mask RCNN に Semi-conv. の仕組みを組み込んだもので比較しています。）\n\n## まとめと感想\n\ninstance coloring の手法をまったく知らなかったのですが、 [CornerNet: Detecting Objects as Paired Keypoints](https://arxiv.org/abs/1808.01244) で Pixel ごとの embedding をクラスタリングしてペアを作るという手法を知り、興味を持ったのでその関連で読んでみた論文です。\nP\u0026V 形式はかなり複雑な構造になるので、それを避けられるならすごく面白いなと思ったのですが、この論文では Mask R-CNN と組み合わせることで精度向上と言っているので、まだまだ IC 単体で勝てる感じではないのでしょうか？\n\n同じタスクに対して全く違う 2 つのアプローチが（比較対象になるくらいには）同じような成果を出しているのも面白いところです。segmentation は主流ではなかった分、まだまだ改善がありそうで楽しみです。\n","slug":"【論文読み】Semi-convolutional_Operators_for_Instance_Segmentation","title":"【論文読み】Semi-convolutional Operators for Instance Segmentation","timestamp":1549871986000,"tags":["DeepLearning","論文読み"]}}},"__N_SSG":true},"page":"/posts/[slug]","query":{"slug":"【論文読み】Semi-convolutional_Operators_for_Instance_Segmentation"},"buildId":"LTCqLRPEd5Qd2agqEQJZo","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>