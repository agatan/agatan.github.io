<!DOCTYPE html><html><head><meta name="viewport" content="width=device-width"/><meta charSet="utf-8"/><title>#論文読み | 右上↗</title><meta name="description" content="#論文読みに関する投稿"/><meta property="og:type" content="website"/><meta property="og:title" content="#論文読み"/><meta property="og:description" content="#論文読みに関する投稿"/><meta property="og:site_name" content="右上↗"/><meta property="twitter:card" content="summary"/><meta property="twitter:creator" content="@agatan_"/><meta property="twitter:title" content="#論文読み"/><meta property="twitter:description" content="#論文読みに関する投稿"/><meta name="next-head-count" content="12"/><link rel="preload" href="/_next/static/css/fcf459cc03a0aa33d72b.css" as="style"/><link rel="stylesheet" href="/_next/static/css/fcf459cc03a0aa33d72b.css" data-n-g=""/><noscript data-n-css=""></noscript><script defer="" nomodule="" src="/_next/static/chunks/polyfills-a54b4f32bdc1ef890ddd.js"></script><script src="/_next/static/chunks/webpack-20d43e08bea62467b090.js" defer=""></script><script src="/_next/static/chunks/framework-92300432a1172ef1338b.js" defer=""></script><script src="/_next/static/chunks/main-588261c74baf7142d208.js" defer=""></script><script src="/_next/static/chunks/pages/_app-a9e8b7fcb86503283a43.js" defer=""></script><script src="/_next/static/chunks/1bfc9850-b731c6aa1c491f14d361.js" defer=""></script><script src="/_next/static/chunks/191-ac794077f326adb56e38.js" defer=""></script><script src="/_next/static/chunks/pages/tags/%5Bname%5D-5eb20ad24deb5f36a975.js" defer=""></script><script src="/_next/static/LTCqLRPEd5Qd2agqEQJZo/_buildManifest.js" defer=""></script><script src="/_next/static/LTCqLRPEd5Qd2agqEQJZo/_ssgManifest.js" defer=""></script></head><body><div id="__next"><style data-emotion="css-global o7558t">:host,:root{--chakra-ring-inset:var(--chakra-empty,/*!*/ /*!*/);--chakra-ring-offset-width:0px;--chakra-ring-offset-color:#fff;--chakra-ring-color:rgba(66, 153, 225, 0.6);--chakra-ring-offset-shadow:0 0 #0000;--chakra-ring-shadow:0 0 #0000;--chakra-space-x-reverse:0;--chakra-space-y-reverse:0;--chakra-colors-transparent:transparent;--chakra-colors-current:currentColor;--chakra-colors-black:#000000;--chakra-colors-white:#FFFFFF;--chakra-colors-whiteAlpha-50:rgba(255, 255, 255, 0.04);--chakra-colors-whiteAlpha-100:rgba(255, 255, 255, 0.06);--chakra-colors-whiteAlpha-200:rgba(255, 255, 255, 0.08);--chakra-colors-whiteAlpha-300:rgba(255, 255, 255, 0.16);--chakra-colors-whiteAlpha-400:rgba(255, 255, 255, 0.24);--chakra-colors-whiteAlpha-500:rgba(255, 255, 255, 0.36);--chakra-colors-whiteAlpha-600:rgba(255, 255, 255, 0.48);--chakra-colors-whiteAlpha-700:rgba(255, 255, 255, 0.64);--chakra-colors-whiteAlpha-800:rgba(255, 255, 255, 0.80);--chakra-colors-whiteAlpha-900:rgba(255, 255, 255, 0.92);--chakra-colors-blackAlpha-50:rgba(0, 0, 0, 0.04);--chakra-colors-blackAlpha-100:rgba(0, 0, 0, 0.06);--chakra-colors-blackAlpha-200:rgba(0, 0, 0, 0.08);--chakra-colors-blackAlpha-300:rgba(0, 0, 0, 0.16);--chakra-colors-blackAlpha-400:rgba(0, 0, 0, 0.24);--chakra-colors-blackAlpha-500:rgba(0, 0, 0, 0.36);--chakra-colors-blackAlpha-600:rgba(0, 0, 0, 0.48);--chakra-colors-blackAlpha-700:rgba(0, 0, 0, 0.64);--chakra-colors-blackAlpha-800:rgba(0, 0, 0, 0.80);--chakra-colors-blackAlpha-900:rgba(0, 0, 0, 0.92);--chakra-colors-gray-50:#F7FAFC;--chakra-colors-gray-100:#EDF2F7;--chakra-colors-gray-200:#E2E8F0;--chakra-colors-gray-300:#CBD5E0;--chakra-colors-gray-400:#A0AEC0;--chakra-colors-gray-500:#718096;--chakra-colors-gray-600:#4A5568;--chakra-colors-gray-700:#2D3748;--chakra-colors-gray-800:#1A202C;--chakra-colors-gray-900:#171923;--chakra-colors-red-50:#FFF5F5;--chakra-colors-red-100:#FED7D7;--chakra-colors-red-200:#FEB2B2;--chakra-colors-red-300:#FC8181;--chakra-colors-red-400:#F56565;--chakra-colors-red-500:#E53E3E;--chakra-colors-red-600:#C53030;--chakra-colors-red-700:#9B2C2C;--chakra-colors-red-800:#822727;--chakra-colors-red-900:#63171B;--chakra-colors-orange-50:#FFFAF0;--chakra-colors-orange-100:#FEEBC8;--chakra-colors-orange-200:#FBD38D;--chakra-colors-orange-300:#F6AD55;--chakra-colors-orange-400:#ED8936;--chakra-colors-orange-500:#DD6B20;--chakra-colors-orange-600:#C05621;--chakra-colors-orange-700:#9C4221;--chakra-colors-orange-800:#7B341E;--chakra-colors-orange-900:#652B19;--chakra-colors-yellow-50:#FFFFF0;--chakra-colors-yellow-100:#FEFCBF;--chakra-colors-yellow-200:#FAF089;--chakra-colors-yellow-300:#F6E05E;--chakra-colors-yellow-400:#ECC94B;--chakra-colors-yellow-500:#D69E2E;--chakra-colors-yellow-600:#B7791F;--chakra-colors-yellow-700:#975A16;--chakra-colors-yellow-800:#744210;--chakra-colors-yellow-900:#5F370E;--chakra-colors-green-50:#F0FFF4;--chakra-colors-green-100:#C6F6D5;--chakra-colors-green-200:#9AE6B4;--chakra-colors-green-300:#68D391;--chakra-colors-green-400:#48BB78;--chakra-colors-green-500:#38A169;--chakra-colors-green-600:#2F855A;--chakra-colors-green-700:#276749;--chakra-colors-green-800:#22543D;--chakra-colors-green-900:#1C4532;--chakra-colors-teal-50:#E6FFFA;--chakra-colors-teal-100:#B2F5EA;--chakra-colors-teal-200:#81E6D9;--chakra-colors-teal-300:#4FD1C5;--chakra-colors-teal-400:#38B2AC;--chakra-colors-teal-500:#319795;--chakra-colors-teal-600:#2C7A7B;--chakra-colors-teal-700:#285E61;--chakra-colors-teal-800:#234E52;--chakra-colors-teal-900:#1D4044;--chakra-colors-blue-50:#ebf8ff;--chakra-colors-blue-100:#bee3f8;--chakra-colors-blue-200:#90cdf4;--chakra-colors-blue-300:#63b3ed;--chakra-colors-blue-400:#4299e1;--chakra-colors-blue-500:#3182ce;--chakra-colors-blue-600:#2b6cb0;--chakra-colors-blue-700:#2c5282;--chakra-colors-blue-800:#2a4365;--chakra-colors-blue-900:#1A365D;--chakra-colors-cyan-50:#EDFDFD;--chakra-colors-cyan-100:#C4F1F9;--chakra-colors-cyan-200:#9DECF9;--chakra-colors-cyan-300:#76E4F7;--chakra-colors-cyan-400:#0BC5EA;--chakra-colors-cyan-500:#00B5D8;--chakra-colors-cyan-600:#00A3C4;--chakra-colors-cyan-700:#0987A0;--chakra-colors-cyan-800:#086F83;--chakra-colors-cyan-900:#065666;--chakra-colors-purple-50:#FAF5FF;--chakra-colors-purple-100:#E9D8FD;--chakra-colors-purple-200:#D6BCFA;--chakra-colors-purple-300:#B794F4;--chakra-colors-purple-400:#9F7AEA;--chakra-colors-purple-500:#805AD5;--chakra-colors-purple-600:#6B46C1;--chakra-colors-purple-700:#553C9A;--chakra-colors-purple-800:#44337A;--chakra-colors-purple-900:#322659;--chakra-colors-pink-50:#FFF5F7;--chakra-colors-pink-100:#FED7E2;--chakra-colors-pink-200:#FBB6CE;--chakra-colors-pink-300:#F687B3;--chakra-colors-pink-400:#ED64A6;--chakra-colors-pink-500:#D53F8C;--chakra-colors-pink-600:#B83280;--chakra-colors-pink-700:#97266D;--chakra-colors-pink-800:#702459;--chakra-colors-pink-900:#521B41;--chakra-colors-linkedin-50:#E8F4F9;--chakra-colors-linkedin-100:#CFEDFB;--chakra-colors-linkedin-200:#9BDAF3;--chakra-colors-linkedin-300:#68C7EC;--chakra-colors-linkedin-400:#34B3E4;--chakra-colors-linkedin-500:#00A0DC;--chakra-colors-linkedin-600:#008CC9;--chakra-colors-linkedin-700:#0077B5;--chakra-colors-linkedin-800:#005E93;--chakra-colors-linkedin-900:#004471;--chakra-colors-facebook-50:#E8F4F9;--chakra-colors-facebook-100:#D9DEE9;--chakra-colors-facebook-200:#B7C2DA;--chakra-colors-facebook-300:#6482C0;--chakra-colors-facebook-400:#4267B2;--chakra-colors-facebook-500:#385898;--chakra-colors-facebook-600:#314E89;--chakra-colors-facebook-700:#29487D;--chakra-colors-facebook-800:#223B67;--chakra-colors-facebook-900:#1E355B;--chakra-colors-messenger-50:#D0E6FF;--chakra-colors-messenger-100:#B9DAFF;--chakra-colors-messenger-200:#A2CDFF;--chakra-colors-messenger-300:#7AB8FF;--chakra-colors-messenger-400:#2E90FF;--chakra-colors-messenger-500:#0078FF;--chakra-colors-messenger-600:#0063D1;--chakra-colors-messenger-700:#0052AC;--chakra-colors-messenger-800:#003C7E;--chakra-colors-messenger-900:#002C5C;--chakra-colors-whatsapp-50:#dffeec;--chakra-colors-whatsapp-100:#b9f5d0;--chakra-colors-whatsapp-200:#90edb3;--chakra-colors-whatsapp-300:#65e495;--chakra-colors-whatsapp-400:#3cdd78;--chakra-colors-whatsapp-500:#22c35e;--chakra-colors-whatsapp-600:#179848;--chakra-colors-whatsapp-700:#0c6c33;--chakra-colors-whatsapp-800:#01421c;--chakra-colors-whatsapp-900:#001803;--chakra-colors-twitter-50:#E5F4FD;--chakra-colors-twitter-100:#C8E9FB;--chakra-colors-twitter-200:#A8DCFA;--chakra-colors-twitter-300:#83CDF7;--chakra-colors-twitter-400:#57BBF5;--chakra-colors-twitter-500:#1DA1F2;--chakra-colors-twitter-600:#1A94DA;--chakra-colors-twitter-700:#1681BF;--chakra-colors-twitter-800:#136B9E;--chakra-colors-twitter-900:#0D4D71;--chakra-colors-telegram-50:#E3F2F9;--chakra-colors-telegram-100:#C5E4F3;--chakra-colors-telegram-200:#A2D4EC;--chakra-colors-telegram-300:#7AC1E4;--chakra-colors-telegram-400:#47A9DA;--chakra-colors-telegram-500:#0088CC;--chakra-colors-telegram-600:#007AB8;--chakra-colors-telegram-700:#006BA1;--chakra-colors-telegram-800:#005885;--chakra-colors-telegram-900:#003F5E;--chakra-borders-none:0;--chakra-borders-1px:1px solid;--chakra-borders-2px:2px solid;--chakra-borders-4px:4px solid;--chakra-borders-8px:8px solid;--chakra-fonts-heading:-apple-system,BlinkMacSystemFont,"Segoe UI",Helvetica,Arial,sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol";--chakra-fonts-body:-apple-system,BlinkMacSystemFont,"Segoe UI",Helvetica,Arial,sans-serif,"Apple Color Emoji","Segoe UI Emoji","Segoe UI Symbol";--chakra-fonts-mono:SFMono-Regular,Menlo,Monaco,Consolas,"Liberation Mono","Courier New",monospace;--chakra-fontSizes-xs:0.75rem;--chakra-fontSizes-sm:0.875rem;--chakra-fontSizes-md:1rem;--chakra-fontSizes-lg:1.125rem;--chakra-fontSizes-xl:1.25rem;--chakra-fontSizes-2xl:1.5rem;--chakra-fontSizes-3xl:1.875rem;--chakra-fontSizes-4xl:2.25rem;--chakra-fontSizes-5xl:3rem;--chakra-fontSizes-6xl:3.75rem;--chakra-fontSizes-7xl:4.5rem;--chakra-fontSizes-8xl:6rem;--chakra-fontSizes-9xl:8rem;--chakra-fontWeights-hairline:100;--chakra-fontWeights-thin:200;--chakra-fontWeights-light:300;--chakra-fontWeights-normal:400;--chakra-fontWeights-medium:500;--chakra-fontWeights-semibold:600;--chakra-fontWeights-bold:700;--chakra-fontWeights-extrabold:800;--chakra-fontWeights-black:900;--chakra-letterSpacings-tighter:-0.05em;--chakra-letterSpacings-tight:-0.025em;--chakra-letterSpacings-normal:0;--chakra-letterSpacings-wide:0.025em;--chakra-letterSpacings-wider:0.05em;--chakra-letterSpacings-widest:0.1em;--chakra-lineHeights-3:.75rem;--chakra-lineHeights-4:1rem;--chakra-lineHeights-5:1.25rem;--chakra-lineHeights-6:1.5rem;--chakra-lineHeights-7:1.75rem;--chakra-lineHeights-8:2rem;--chakra-lineHeights-9:2.25rem;--chakra-lineHeights-10:2.5rem;--chakra-lineHeights-normal:normal;--chakra-lineHeights-none:1;--chakra-lineHeights-shorter:1.25;--chakra-lineHeights-short:1.375;--chakra-lineHeights-base:1.5;--chakra-lineHeights-tall:1.625;--chakra-lineHeights-taller:2;--chakra-radii-none:0;--chakra-radii-sm:0.125rem;--chakra-radii-base:0.25rem;--chakra-radii-md:0.375rem;--chakra-radii-lg:0.5rem;--chakra-radii-xl:0.75rem;--chakra-radii-2xl:1rem;--chakra-radii-3xl:1.5rem;--chakra-radii-full:9999px;--chakra-space-1:0.25rem;--chakra-space-2:0.5rem;--chakra-space-3:0.75rem;--chakra-space-4:1rem;--chakra-space-5:1.25rem;--chakra-space-6:1.5rem;--chakra-space-7:1.75rem;--chakra-space-8:2rem;--chakra-space-9:2.25rem;--chakra-space-10:2.5rem;--chakra-space-12:3rem;--chakra-space-14:3.5rem;--chakra-space-16:4rem;--chakra-space-20:5rem;--chakra-space-24:6rem;--chakra-space-28:7rem;--chakra-space-32:8rem;--chakra-space-36:9rem;--chakra-space-40:10rem;--chakra-space-44:11rem;--chakra-space-48:12rem;--chakra-space-52:13rem;--chakra-space-56:14rem;--chakra-space-60:15rem;--chakra-space-64:16rem;--chakra-space-72:18rem;--chakra-space-80:20rem;--chakra-space-96:24rem;--chakra-space-px:1px;--chakra-space-0\.5:0.125rem;--chakra-space-1\.5:0.375rem;--chakra-space-2\.5:0.625rem;--chakra-space-3\.5:0.875rem;--chakra-shadows-xs:0 0 0 1px rgba(0, 0, 0, 0.05);--chakra-shadows-sm:0 1px 2px 0 rgba(0, 0, 0, 0.05);--chakra-shadows-base:0 1px 3px 0 rgba(0, 0, 0, 0.1),0 1px 2px 0 rgba(0, 0, 0, 0.06);--chakra-shadows-md:0 4px 6px -1px rgba(0, 0, 0, 0.1),0 2px 4px -1px rgba(0, 0, 0, 0.06);--chakra-shadows-lg:0 10px 15px -3px rgba(0, 0, 0, 0.1),0 4px 6px -2px rgba(0, 0, 0, 0.05);--chakra-shadows-xl:0 20px 25px -5px rgba(0, 0, 0, 0.1),0 10px 10px -5px rgba(0, 0, 0, 0.04);--chakra-shadows-2xl:0 25px 50px -12px rgba(0, 0, 0, 0.25);--chakra-shadows-outline:0 0 0 3px rgba(66, 153, 225, 0.6);--chakra-shadows-inner:inset 0 2px 4px 0 rgba(0,0,0,0.06);--chakra-shadows-none:none;--chakra-shadows-dark-lg:rgba(0, 0, 0, 0.1) 0px 0px 0px 1px,rgba(0, 0, 0, 0.2) 0px 5px 10px,rgba(0, 0, 0, 0.4) 0px 15px 40px;--chakra-sizes-1:0.25rem;--chakra-sizes-2:0.5rem;--chakra-sizes-3:0.75rem;--chakra-sizes-4:1rem;--chakra-sizes-5:1.25rem;--chakra-sizes-6:1.5rem;--chakra-sizes-7:1.75rem;--chakra-sizes-8:2rem;--chakra-sizes-9:2.25rem;--chakra-sizes-10:2.5rem;--chakra-sizes-12:3rem;--chakra-sizes-14:3.5rem;--chakra-sizes-16:4rem;--chakra-sizes-20:5rem;--chakra-sizes-24:6rem;--chakra-sizes-28:7rem;--chakra-sizes-32:8rem;--chakra-sizes-36:9rem;--chakra-sizes-40:10rem;--chakra-sizes-44:11rem;--chakra-sizes-48:12rem;--chakra-sizes-52:13rem;--chakra-sizes-56:14rem;--chakra-sizes-60:15rem;--chakra-sizes-64:16rem;--chakra-sizes-72:18rem;--chakra-sizes-80:20rem;--chakra-sizes-96:24rem;--chakra-sizes-px:1px;--chakra-sizes-0\.5:0.125rem;--chakra-sizes-1\.5:0.375rem;--chakra-sizes-2\.5:0.625rem;--chakra-sizes-3\.5:0.875rem;--chakra-sizes-max:max-content;--chakra-sizes-min:min-content;--chakra-sizes-full:100%;--chakra-sizes-3xs:14rem;--chakra-sizes-2xs:16rem;--chakra-sizes-xs:20rem;--chakra-sizes-sm:24rem;--chakra-sizes-md:28rem;--chakra-sizes-lg:32rem;--chakra-sizes-xl:36rem;--chakra-sizes-2xl:42rem;--chakra-sizes-3xl:48rem;--chakra-sizes-4xl:56rem;--chakra-sizes-5xl:64rem;--chakra-sizes-6xl:72rem;--chakra-sizes-7xl:80rem;--chakra-sizes-8xl:90rem;--chakra-sizes-container-sm:640px;--chakra-sizes-container-md:768px;--chakra-sizes-container-lg:1024px;--chakra-sizes-container-xl:1280px;--chakra-zIndices-hide:-1;--chakra-zIndices-auto:auto;--chakra-zIndices-base:0;--chakra-zIndices-docked:10;--chakra-zIndices-dropdown:1000;--chakra-zIndices-sticky:1100;--chakra-zIndices-banner:1200;--chakra-zIndices-overlay:1300;--chakra-zIndices-modal:1400;--chakra-zIndices-popover:1500;--chakra-zIndices-skipLink:1600;--chakra-zIndices-toast:1700;--chakra-zIndices-tooltip:1800;--chakra-transition-property-common:background-color,border-color,color,fill,stroke,opacity,box-shadow,transform;--chakra-transition-property-colors:background-color,border-color,color,fill,stroke;--chakra-transition-property-dimensions:width,height;--chakra-transition-property-position:left,right,top,bottom;--chakra-transition-property-background:background-color,background-image,background-position;--chakra-transition-easing-ease-in:cubic-bezier(0.4, 0, 1, 1);--chakra-transition-easing-ease-out:cubic-bezier(0, 0, 0.2, 1);--chakra-transition-easing-ease-in-out:cubic-bezier(0.4, 0, 0.2, 1);--chakra-transition-duration-ultra-fast:50ms;--chakra-transition-duration-faster:100ms;--chakra-transition-duration-fast:150ms;--chakra-transition-duration-normal:200ms;--chakra-transition-duration-slow:300ms;--chakra-transition-duration-slower:400ms;--chakra-transition-duration-ultra-slow:500ms;--chakra-blur-none:0;--chakra-blur-sm:4px;--chakra-blur-base:8px;--chakra-blur-md:12px;--chakra-blur-lg:16px;--chakra-blur-xl:24px;--chakra-blur-2xl:40px;--chakra-blur-3xl:64px;}</style><style data-emotion="css-global 1syi0wy">html{line-height:1.5;-webkit-text-size-adjust:100%;font-family:system-ui,sans-serif;-webkit-font-smoothing:antialiased;text-rendering:optimizeLegibility;-moz-osx-font-smoothing:grayscale;touch-action:manipulation;}body{position:relative;min-height:100%;font-feature-settings:'kern';}*,*::before,*::after{border-width:0;border-style:solid;box-sizing:border-box;}main{display:block;}hr{border-top-width:1px;box-sizing:content-box;height:0;overflow:visible;}pre,code,kbd,samp{font-family:SFMono-Regular,Menlo,Monaco,Consolas,monospace;font-size:1em;}a{background-color:transparent;color:inherit;-webkit-text-decoration:inherit;text-decoration:inherit;}abbr[title]{border-bottom:none;-webkit-text-decoration:underline;text-decoration:underline;-webkit-text-decoration:underline dotted;-webkit-text-decoration:underline dotted;text-decoration:underline dotted;}b,strong{font-weight:bold;}small{font-size:80%;}sub,sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline;}sub{bottom:-0.25em;}sup{top:-0.5em;}img{border-style:none;}button,input,optgroup,select,textarea{font-family:inherit;font-size:100%;line-height:1.15;margin:0;}button,input{overflow:visible;}button,select{text-transform:none;}button::-moz-focus-inner,[type="button"]::-moz-focus-inner,[type="reset"]::-moz-focus-inner,[type="submit"]::-moz-focus-inner{border-style:none;padding:0;}fieldset{padding:0.35em 0.75em 0.625em;}legend{box-sizing:border-box;color:inherit;display:table;max-width:100%;padding:0;white-space:normal;}progress{vertical-align:baseline;}textarea{overflow:auto;}[type="checkbox"],[type="radio"]{box-sizing:border-box;padding:0;}[type="number"]::-webkit-inner-spin-button,[type="number"]::-webkit-outer-spin-button{-webkit-appearance:none!important;}input[type="number"]{-moz-appearance:textfield;}[type="search"]{-webkit-appearance:textfield;outline-offset:-2px;}[type="search"]::-webkit-search-decoration{-webkit-appearance:none!important;}::-webkit-file-upload-button{-webkit-appearance:button;font:inherit;}details{display:block;}summary{display:-webkit-box;display:-webkit-list-item;display:-ms-list-itembox;display:list-item;}template{display:none;}[hidden]{display:none!important;}body,blockquote,dl,dd,h1,h2,h3,h4,h5,h6,hr,figure,p,pre{margin:0;}button{background:transparent;padding:0;}fieldset{margin:0;padding:0;}ol,ul{margin:0;padding:0;}textarea{resize:vertical;}button,[role="button"]{cursor:pointer;}button::-moz-focus-inner{border:0!important;}table{border-collapse:collapse;}h1,h2,h3,h4,h5,h6{font-size:inherit;font-weight:inherit;}button,input,optgroup,select,textarea{padding:0;line-height:inherit;color:inherit;}img,svg,video,canvas,audio,iframe,embed,object{display:block;vertical-align:middle;}img,video{max-width:100%;height:auto;}[data-js-focus-visible] :focus:not([data-focus-visible-added]){outline:none;box-shadow:none;}select::-ms-expand{display:none;}</style><style data-emotion="css-global 1baqkrf">body{font-family:var(--chakra-fonts-body);color:var(--chakra-colors-gray-800);background:var(--chakra-colors-white);transition-property:background-color;transition-duration:var(--chakra-transition-duration-normal);line-height:var(--chakra-lineHeights-base);}*::-webkit-input-placeholder{color:var(--chakra-colors-gray-400);}*::-moz-placeholder{color:var(--chakra-colors-gray-400);}*:-ms-input-placeholder{color:var(--chakra-colors-gray-400);}*::placeholder{color:var(--chakra-colors-gray-400);}*,*::before,::after{border-color:var(--chakra-colors-gray-200);word-wrap:break-word;}</style><style data-emotion="css d4xd0d">.css-d4xd0d{background-color:var(--chakra-colors-green-50);min-height:100vh;}</style><div class="css-d4xd0d"><style data-emotion="css 50jqoj">.css-50jqoj{width:100%;-webkit-margin-start:auto;margin-inline-start:auto;-webkit-margin-end:auto;margin-inline-end:auto;max-width:var(--chakra-sizes-container-lg);-webkit-padding-start:1rem;padding-inline-start:1rem;-webkit-padding-end:1rem;padding-inline-end:1rem;background-color:var(--chakra-colors-green-600);}</style><div class="chakra-container css-50jqoj"><style data-emotion="css jbx4q3">.css-jbx4q3{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:flex-end;-webkit-box-align:flex-end;-ms-flex-align:flex-end;align-items:flex-end;padding:var(--chakra-space-8);}</style><div class="css-jbx4q3"><style data-emotion="css 1dklj6k">.css-1dklj6k{font-family:var(--chakra-fonts-heading);font-weight:var(--chakra-fontWeights-bold);font-size:var(--chakra-fontSizes-3xl);line-height:1.33;}@media screen and (min-width: 48em){.css-1dklj6k{font-size:var(--chakra-fontSizes-4xl);line-height:1.2;}}</style><h2 class="chakra-heading css-1dklj6k"><style data-emotion="css 1naxmyz">.css-1naxmyz{transition-property:var(--chakra-transition-property-common);transition-duration:var(--chakra-transition-duration-fast);transition-timing-function:var(--chakra-transition-easing-ease-out);cursor:pointer;-webkit-text-decoration:none;text-decoration:none;outline:2px solid transparent;outline-offset:2px;color:var(--chakra-colors-white);}.css-1naxmyz:hover,.css-1naxmyz[data-hover]{-webkit-text-decoration:underline;text-decoration:underline;}.css-1naxmyz:focus,.css-1naxmyz[data-focus]{box-shadow:var(--chakra-shadows-outline);}</style><a class="chakra-link css-1naxmyz" href="/"><u class="chakra-text css-0">↗ agatan blog ↗</u></a></h2><style data-emotion="css 17xejub">.css-17xejub{-webkit-flex:1;-ms-flex:1;flex:1;justify-self:stretch;-webkit-align-self:stretch;-ms-flex-item-align:stretch;align-self:stretch;}</style><div class="css-17xejub"></div><div class="css-0"><style data-emotion="css 1awjy5h">.css-1awjy5h{transition-property:var(--chakra-transition-property-common);transition-duration:var(--chakra-transition-duration-fast);transition-timing-function:var(--chakra-transition-easing-ease-out);cursor:pointer;-webkit-text-decoration:none;text-decoration:none;outline:2px solid transparent;outline-offset:2px;color:inherit;padding:var(--chakra-space-2);}.css-1awjy5h:hover,.css-1awjy5h[data-hover]{-webkit-text-decoration:underline;text-decoration:underline;}.css-1awjy5h:focus,.css-1awjy5h[data-focus]{box-shadow:var(--chakra-shadows-outline);}</style><a class="chakra-link css-1awjy5h" href="/feed.xml"><style data-emotion="css sjogs4">.css-sjogs4{width:var(--chakra-sizes-6);height:var(--chakra-sizes-6);display:inline-block;line-height:1em;-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;color:var(--chakra-colors-white);}</style><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 448 512" focusable="false" class="chakra-icon css-sjogs4" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M128.081 415.959c0 35.369-28.672 64.041-64.041 64.041S0 451.328 0 415.959s28.672-64.041 64.041-64.041 64.04 28.673 64.04 64.041zm175.66 47.25c-8.354-154.6-132.185-278.587-286.95-286.95C7.656 175.765 0 183.105 0 192.253v48.069c0 8.415 6.49 15.472 14.887 16.018 111.832 7.284 201.473 96.702 208.772 208.772.547 8.397 7.604 14.887 16.018 14.887h48.069c9.149.001 16.489-7.655 15.995-16.79zm144.249.288C439.596 229.677 251.465 40.445 16.503 32.01 7.473 31.686 0 38.981 0 48.016v48.068c0 8.625 6.835 15.645 15.453 15.999 191.179 7.839 344.627 161.316 352.465 352.465.353 8.618 7.373 15.453 15.999 15.453h48.068c9.034-.001 16.329-7.474 16.005-16.504z"></path></svg></a><a class="chakra-link css-1awjy5h" href="/tags"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 640 512" focusable="false" class="chakra-icon css-sjogs4" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M497.941 225.941L286.059 14.059A48 48 0 0 0 252.118 0H48C21.49 0 0 21.49 0 48v204.118a48 48 0 0 0 14.059 33.941l211.882 211.882c18.744 18.745 49.136 18.746 67.882 0l204.118-204.118c18.745-18.745 18.745-49.137 0-67.882zM112 160c-26.51 0-48-21.49-48-48s21.49-48 48-48 48 21.49 48 48-21.49 48-48 48zm513.941 133.823L421.823 497.941c-18.745 18.745-49.137 18.745-67.882 0l-.36-.36L527.64 323.522c16.999-16.999 26.36-39.6 26.36-63.64s-9.362-46.641-26.36-63.64L331.397 0h48.721a48 48 0 0 1 33.941 14.059l211.882 211.882c18.745 18.745 18.745 49.137 0 67.882z"></path></svg></a></div></div></div><style data-emotion="css 8shb6n">.css-8shb6n{width:100%;-webkit-margin-start:auto;margin-inline-start:auto;-webkit-margin-end:auto;margin-inline-end:auto;max-width:var(--chakra-sizes-container-lg);-webkit-padding-start:1rem;padding-inline-start:1rem;-webkit-padding-end:1rem;padding-inline-end:1rem;background-color:var(--chakra-colors-white);min-height:100vh;}</style><div class="chakra-container css-8shb6n"><style data-emotion="css 10qlibn">.css-10qlibn{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;padding-top:var(--chakra-space-4);padding-bottom:var(--chakra-space-4);}</style><div class="css-10qlibn"><style data-emotion="css 8jfdow">.css-8jfdow{width:100%;-webkit-margin-start:auto;margin-inline-start:auto;-webkit-margin-end:auto;margin-inline-end:auto;max-width:var(--chakra-sizes-container-lg);-webkit-padding-start:1rem;padding-inline-start:1rem;-webkit-padding-end:1rem;padding-inline-end:1rem;padding:0px;margin:0px;}</style><div class="chakra-container css-8jfdow"><style data-emotion="css nm5t63">.css-nm5t63{width:100%;-webkit-margin-start:auto;margin-inline-start:auto;-webkit-margin-end:auto;margin-inline-end:auto;max-width:var(--chakra-sizes-container-md);-webkit-padding-start:1rem;padding-inline-start:1rem;-webkit-padding-end:1rem;padding-inline-end:1rem;}</style><div class="chakra-container css-nm5t63"><style data-emotion="css 1d87rfn">.css-1d87rfn{font-family:var(--chakra-fonts-heading);font-weight:var(--chakra-fontWeights-bold);font-size:var(--chakra-fontSizes-3xl);line-height:1.33;padding-bottom:var(--chakra-space-4);color:var(--chakra-colors-blue-500);}@media screen and (min-width: 48em){.css-1d87rfn{line-height:1.2;}}</style><h2 class="chakra-heading css-1d87rfn">#<!-- -->論文読み</h2><style data-emotion="css svjswr">.css-svjswr{opacity:0.6;border:0;border-color:inherit;border-style:solid;border-bottom-width:1px;width:100%;}</style><hr aria-orientation="horizontal" class="chakra-divider css-svjswr"/></div><div class="chakra-container css-nm5t63"><style data-emotion="css u3hp4h">.css-u3hp4h{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:stretch;-webkit-box-align:stretch;-ms-flex-align:stretch;align-items:stretch;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;padding-top:var(--chakra-space-4);padding-bottom:var(--chakra-space-4);}</style><div class="chakra-stack css-u3hp4h"><div class="chakra-container css-nm5t63"><style data-emotion="css 18j379d">.css-18j379d{font-family:var(--chakra-fonts-heading);font-weight:var(--chakra-fontWeights-bold);font-size:var(--chakra-fontSizes-xl);line-height:1.2;}</style><h2 class="chakra-heading css-18j379d"><style data-emotion="css f4h6uy">.css-f4h6uy{transition-property:var(--chakra-transition-property-common);transition-duration:var(--chakra-transition-duration-fast);transition-timing-function:var(--chakra-transition-easing-ease-out);cursor:pointer;-webkit-text-decoration:none;text-decoration:none;outline:2px solid transparent;outline-offset:2px;color:inherit;}.css-f4h6uy:hover,.css-f4h6uy[data-hover]{-webkit-text-decoration:underline;text-decoration:underline;}.css-f4h6uy:focus,.css-f4h6uy[data-focus]{box-shadow:var(--chakra-shadows-outline);}</style><a class="chakra-link css-f4h6uy" href="/posts/【論文読み】_Multi-Oriented_Scene_Text_Detection_via_Corner_Localization_and_Region_Segmentation">【論文読み】 Multi-Oriented Scene Text Detection via Corner Localization and Region Segmentation</a></h2><style data-emotion="css jgpr3k">.css-jgpr3k{color:var(--chakra-colors-gray-800);}</style><p class="chakra-text css-jgpr3k">Tue Feb 12 2019</p><style data-emotion="css 1kqpisq">.css-1kqpisq{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;padding-top:var(--chakra-space-4);padding-bottom:var(--chakra-space-4);}.css-1kqpisq>*:not(style)~*:not(style){margin-top:0px;-webkit-margin-end:0px;margin-inline-end:0px;margin-bottom:0px;-webkit-margin-start:0px;margin-inline-start:0px;}</style><div class="chakra-stack css-1kqpisq"><style data-emotion="css 1h1f62l">.css-1h1f62l{transition-property:var(--chakra-transition-property-common);transition-duration:var(--chakra-transition-duration-fast);transition-timing-function:var(--chakra-transition-easing-ease-out);cursor:pointer;-webkit-text-decoration:none;text-decoration:none;outline:2px solid transparent;outline-offset:2px;color:inherit;padding:var(--chakra-space-1);}.css-1h1f62l:hover,.css-1h1f62l[data-hover]{-webkit-text-decoration:underline;text-decoration:underline;}.css-1h1f62l:focus,.css-1h1f62l[data-focus]{box-shadow:var(--chakra-shadows-outline);}</style><a class="chakra-link css-1h1f62l" href="/tags/画像処理"><style data-emotion="css 6rl2qk">.css-6rl2qk{display:-webkit-inline-box;display:-webkit-inline-flex;display:-ms-inline-flexbox;display:inline-flex;vertical-align:top;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;max-width:100%;font-weight:var(--chakra-fontWeights-medium);line-height:1.2;outline:2px solid transparent;outline-offset:2px;min-height:1.5rem;min-width:1.5rem;font-size:var(--chakra-fontSizes-sm);border-radius:var(--chakra-radii-md);-webkit-padding-start:var(--chakra-space-2);padding-inline-start:var(--chakra-space-2);-webkit-padding-end:var(--chakra-space-2);padding-inline-end:var(--chakra-space-2);color:#3182ce;box-shadow:inset 0 0 0px 1px #3182ce;padding:var(--chakra-space-1);}.css-6rl2qk:focus,.css-6rl2qk[data-focus]{box-shadow:var(--chakra-shadows-outline);}</style><span class="css-6rl2qk"><style data-emotion="css 1qpz0yi">.css-1qpz0yi{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;padding-left:var(--chakra-space-1);padding-right:var(--chakra-space-1);}</style><div class="css-1qpz0yi"><style data-emotion="css 1uhrip0">.css-1uhrip0{line-height:1.2;overflow:hidden;text-overflow:ellipsis;white-space:nowrap;font-size:small;}</style><span class="css-1uhrip0">画像処理</span></div></span></a><a class="chakra-link css-1h1f62l" href="/tags/DeepLearning"><span class="css-6rl2qk"><div class="css-1qpz0yi"><span class="css-1uhrip0">DeepLearning</span></div></span></a><a class="chakra-link css-1h1f62l" href="/tags/OCR"><span class="css-6rl2qk"><div class="css-1qpz0yi"><span class="css-1uhrip0">OCR</span></div></span></a><a class="chakra-link css-1h1f62l" href="/tags/論文読み"><span class="css-6rl2qk"><div class="css-1qpz0yi"><span class="css-1uhrip0">論文読み</span></div></span></a></div></div><style data-emotion="css 1q5cbl">.css-1q5cbl{border-width:0;-webkit-align-self:stretch;-ms-flex-item-align:stretch;align-self:stretch;border-color:inherit;width:auto;height:auto;}.css-1q5cbl{margin-top:var(--chakra-space-4);margin-bottom:var(--chakra-space-4);-webkit-margin-start:0px;margin-inline-start:0px;-webkit-margin-end:0px;margin-inline-end:0px;border-left-width:0;border-bottom-width:1px;}</style><div class="chakra-stack__divider css-1q5cbl"></div><div class="chakra-container css-nm5t63"><h2 class="chakra-heading css-18j379d"><a class="chakra-link css-f4h6uy" href="/posts/【論文読み】Semi-convolutional_Operators_for_Instance_Segmentation">【論文読み】Semi-convolutional Operators for Instance Segmentation</a></h2><p class="chakra-text css-jgpr3k">Mon Feb 11 2019</p><div class="chakra-stack css-1kqpisq"><a class="chakra-link css-1h1f62l" href="/tags/DeepLearning"><span class="css-6rl2qk"><div class="css-1qpz0yi"><span class="css-1uhrip0">DeepLearning</span></div></span></a><a class="chakra-link css-1h1f62l" href="/tags/論文読み"><span class="css-6rl2qk"><div class="css-1qpz0yi"><span class="css-1uhrip0">論文読み</span></div></span></a></div></div><div class="chakra-stack__divider css-1q5cbl"></div><div class="chakra-container css-nm5t63"><h2 class="chakra-heading css-18j379d"><a class="chakra-link css-f4h6uy" href="/posts/【論文紹介】Concurrent_Spatial_and_Channel_Squeeze_&amp;_Excitation_in_Fully_Convolutional_Networks">【論文紹介】Concurrent Spatial and Channel Squeeze &amp; Excitation in Fully Convolutional Networks</a></h2><p class="chakra-text css-jgpr3k">Fri Jan 04 2019</p><div class="chakra-stack css-1kqpisq"><a class="chakra-link css-1h1f62l" href="/tags/Python"><span class="css-6rl2qk"><div class="css-1qpz0yi"><span class="css-1uhrip0">Python</span></div></span></a><a class="chakra-link css-1h1f62l" href="/tags/DeepLearning"><span class="css-6rl2qk"><div class="css-1qpz0yi"><span class="css-1uhrip0">DeepLearning</span></div></span></a><a class="chakra-link css-1h1f62l" href="/tags/Keras"><span class="css-6rl2qk"><div class="css-1qpz0yi"><span class="css-1uhrip0">Keras</span></div></span></a><a class="chakra-link css-1h1f62l" href="/tags/TensorFlow"><span class="css-6rl2qk"><div class="css-1qpz0yi"><span class="css-1uhrip0">TensorFlow</span></div></span></a><a class="chakra-link css-1h1f62l" href="/tags/論文読み"><span class="css-6rl2qk"><div class="css-1qpz0yi"><span class="css-1uhrip0">論文読み</span></div></span></a></div></div><div class="chakra-stack__divider css-1q5cbl"></div><div class="chakra-container css-nm5t63"><h2 class="chakra-heading css-18j379d"><a class="chakra-link css-f4h6uy" href="/posts/DKN:_Deep_Knowledge-Aware_Network_for_News_Recommendation">DKN: Deep Knowledge-Aware Network for News Recommendation</a></h2><p class="chakra-text css-jgpr3k">Wed Dec 19 2018</p><div class="chakra-stack css-1kqpisq"><a class="chakra-link css-1h1f62l" href="/tags/DeepLearning"><span class="css-6rl2qk"><div class="css-1qpz0yi"><span class="css-1uhrip0">DeepLearning</span></div></span></a><a class="chakra-link css-1h1f62l" href="/tags/WWW"><span class="css-6rl2qk"><div class="css-1qpz0yi"><span class="css-1uhrip0">WWW</span></div></span></a><a class="chakra-link css-1h1f62l" href="/tags/Recommendation"><span class="css-6rl2qk"><div class="css-1qpz0yi"><span class="css-1uhrip0">Recommendation</span></div></span></a><a class="chakra-link css-1h1f62l" href="/tags/論文読み"><span class="css-6rl2qk"><div class="css-1qpz0yi"><span class="css-1uhrip0">論文読み</span></div></span></a></div></div><div class="chakra-stack__divider css-1q5cbl"></div><div class="chakra-container css-nm5t63"><h2 class="chakra-heading css-18j379d"><a class="chakra-link css-f4h6uy" href="/posts/Object_間の関係を使って後処理_0_の物体検出を実現する:_Relation_Networks_for_Object_Detection">Object 間の関係を使って後処理 0 の物体検出を実現する: Relation Networks for Object Detection</a></h2><p class="chakra-text css-jgpr3k">Mon Dec 10 2018</p><div class="chakra-stack css-1kqpisq"><a class="chakra-link css-1h1f62l" href="/tags/画像処理"><span class="css-6rl2qk"><div class="css-1qpz0yi"><span class="css-1uhrip0">画像処理</span></div></span></a><a class="chakra-link css-1h1f62l" href="/tags/DeepLearning"><span class="css-6rl2qk"><div class="css-1qpz0yi"><span class="css-1uhrip0">DeepLearning</span></div></span></a><a class="chakra-link css-1h1f62l" href="/tags/論文読み"><span class="css-6rl2qk"><div class="css-1qpz0yi"><span class="css-1uhrip0">論文読み</span></div></span></a><a class="chakra-link css-1h1f62l" href="/tags/物体検出"><span class="css-6rl2qk"><div class="css-1qpz0yi"><span class="css-1uhrip0">物体検出</span></div></span></a></div></div><div class="chakra-stack__divider css-1q5cbl"></div><div class="chakra-container css-nm5t63"><h2 class="chakra-heading css-18j379d"><a class="chakra-link css-f4h6uy" href="/posts/簡単な問題は省エネで解き、難しい問題には全力を出すネットワーク:_Multi-Scale_Dense_Networks">簡単な問題は省エネで解き、難しい問題には全力を出すネットワーク: Multi-Scale Dense Networks</a></h2><p class="chakra-text css-jgpr3k">Fri Dec 07 2018</p><div class="chakra-stack css-1kqpisq"><a class="chakra-link css-1h1f62l" href="/tags/DeepLearning"><span class="css-6rl2qk"><div class="css-1qpz0yi"><span class="css-1uhrip0">DeepLearning</span></div></span></a><a class="chakra-link css-1h1f62l" href="/tags/論文読み"><span class="css-6rl2qk"><div class="css-1qpz0yi"><span class="css-1uhrip0">論文読み</span></div></span></a></div></div><div class="chakra-stack__divider css-1q5cbl"></div><div class="chakra-container css-nm5t63"><h2 class="chakra-heading css-18j379d"><a class="chakra-link css-f4h6uy" href="/posts/[論文紹介]_Focal_Loss_for_Dense_Object_Detection">[論文紹介] Focal Loss for Dense Object Detection</a></h2><p class="chakra-text css-jgpr3k">Sun Dec 02 2018</p><div class="chakra-stack css-1kqpisq"><a class="chakra-link css-1h1f62l" href="/tags/DeepLearning"><span class="css-6rl2qk"><div class="css-1qpz0yi"><span class="css-1uhrip0">DeepLearning</span></div></span></a><a class="chakra-link css-1h1f62l" href="/tags/論文読み"><span class="css-6rl2qk"><div class="css-1qpz0yi"><span class="css-1uhrip0">論文読み</span></div></span></a><a class="chakra-link css-1h1f62l" href="/tags/ICCV"><span class="css-6rl2qk"><div class="css-1qpz0yi"><span class="css-1uhrip0">ICCV</span></div></span></a></div></div></div></div></div><style data-emotion="css ul4ode">.css-ul4ode{width:100%;-webkit-margin-start:auto;margin-inline-start:auto;-webkit-margin-end:auto;margin-inline-end:auto;max-width:var(--chakra-sizes-44);-webkit-padding-start:1rem;padding-inline-start:1rem;-webkit-padding-end:1rem;padding-inline-end:1rem;}</style><div class="chakra-container css-ul4ode"><style data-emotion="css xkvmla">.css-xkvmla{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-flex-direction:column;-ms-flex-direction:column;flex-direction:column;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;padding:var(--chakra-space-4);}</style><div class="css-xkvmla"><a class="chakra-link css-f4h6uy" href="https://twitter.com/@agatan_"><style data-emotion="css 1piy145">.css-1piy145{position:relative;width:var(--chakra-sizes-20);height:var(--chakra-sizes-20);}</style><div class="css-1piy145"><style data-emotion="css 1phd9a0">.css-1phd9a0{object-fit:cover;}</style><img alt="agatan" class="chakra-image__placeholder css-1phd9a0" layout="fill"/></div><style data-emotion="css 19dx7hn">.css-19dx7hn{text-align:center;font-size:large;}</style><p class="chakra-text css-19dx7hn">@agatan</p></a><style data-emotion="css 84zodg">.css-84zodg{display:-webkit-box;display:-webkit-flex;display:-ms-flexbox;display:flex;-webkit-align-items:center;-webkit-box-align:center;-ms-flex-align:center;align-items:center;-webkit-flex-direction:row;-ms-flex-direction:row;flex-direction:row;}.css-84zodg>*:not(style)~*:not(style){margin-top:0px;-webkit-margin-end:0px;margin-inline-end:0px;margin-bottom:0px;-webkit-margin-start:0.5rem;margin-inline-start:0.5rem;}</style><div class="chakra-stack css-84zodg"><a class="chakra-link css-1h1f62l" href="https://twitter.com/@agatan_"><style data-emotion="css 15e9ude">.css-15e9ude{width:1.2em;height:1.2em;display:inline-block;line-height:1em;-webkit-flex-shrink:0;-ms-flex-negative:0;flex-shrink:0;color:currentColor;}</style><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 512 512" focusable="true" class="chakra-icon css-15e9ude" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"></path></svg></a><a class="chakra-link css-1h1f62l" href="https://github.com/agatan"><svg stroke="currentColor" fill="currentColor" stroke-width="0" viewBox="0 0 496 512" focusable="true" class="chakra-icon css-15e9ude" height="1em" width="1em" xmlns="http://www.w3.org/2000/svg"><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg></a></div></div></div></div></div></div><span></span></div><script id="__NEXT_DATA__" type="application/json">{"props":{"pageProps":{"tag":"論文読み","postMetas":[{"rawMarkdown":"---\ntitle: \"【論文読み】 Multi-Oriented Scene Text Detection via Corner Localization and Region Segmentation\"\ndate: 2019-02-13T00:12:22+09:00\ntags: [\"画像処理\", \"DeepLearning\", \"OCR\", \"論文読み\"]\nurl: https://qiita.com/agatan/items/afbdb922688984283775\n---\n\n画像中の文字領域検出における 2 つの主流な手法のいいとこ取りを目指した論文、 Multi-Oriented Scene Text Detection via Corner Localization and Region Segmentation を読んでみました。\n\nこの記事は、Wantedly の勉強会で取り上げられた論文・技術をまとめたものです。\n[2018 年に読んだ機械学習系論文・技術まとめ at Wantedly Advent Calendar 2018 - Qiita](https://qiita.com/advent-calendar/2018/wantedly_ml)\n\n## Reference\n\n- Multi-Oriented Scene Text Detection via Corner Localization and Region Segmentation [Pengyuan Lyu, Cong Yao, Wenhao Wu, Shuicheng Yan, Xiang Bai. CVPR 2018]\n- https://arxiv.org/abs/1802.08948\n\n（文中の図表は論文より引用しています）\n\n## Scene Text Detection\n\nScene Text Detection は、風景写真のなかにある文字領域（かんばん、ポスターなど）を検出するタスクです。\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/a78b72da-8550-641f-819b-ed024d9be7fa.png)\n(Figure 6.)\n\nCNN を活用した研究が進んでおり、現在では 2 つのアプローチが主流となっています。（このあたりは [【論文読み】Semi-convolutional Operators for Instance Segmentation](https://qiita.com/agatan/items/2cf1209b7370db45eba5) や [[論文紹介] Focal Loss for Dense Object Detection](https://qiita.com/agatan/items/53fe8d21f2147b0ac982) でもすこし触れています）\n\n1 つ目は、文字領域検出を、一般的な物体検知(Object Detection)の特殊系とみなして解く手法です。\n物体検知に対するアプローチとして主流なのは bounding box の座標を regression として解くというものです。\nこの場合、bounding box 形式で当てに行くので、歪んだ形状への対応が難しく、縦横比が大きく偏った文字領域に弱いといった問題があります。\n[EAST: An Efficient and Accurate Scene Text Detector](https://arxiv.org/abs/1704.03155) [X. Zhou et al., CVPR 2018] はこちらのアプローチを採用しています。\n\n2 つ目は、Instance Segmentation として解くアプローチです。\nピクセル単位で文字領域かどうかの 2 クラス分類 + なんらかの方法でインスタンスの分離を行うという方法ですが、インスタンスの分離には複雑な後処理を要するケースが多く、複雑さや実行時間に問題があります。\n[PixelLink: Detecting Scene Text via Instance Segmentation](https://arxiv.org/abs/1801.01315) [D. Deng et al., AAAI 2018] が代表例です。（論文中で refer されているのは Multi-oriented Text Detection with Fully Convolutional Networks [Z. Zhang et al., CVPR 2016])\n\nこの論文ではこれらの 2 つの手法をいいとこ取りした Scene Text Detector を提案しています。\n\n## Network Architecture\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/281d3451-0e23-c20b-bb18-88b46495da15.png)\n\n全体像は ↑ の図のようになっています。\n**Corner Detection** と **Position Sensitive Segmentation** の 2 つからなる architecture です。\n\nCorner Detection はその名の通り、文字領域の角の位置を予測します。ただし、「角である」ことだけを考慮し、「どの 4 つの組み合わせが 1 領域を表しているのか」は考えません。\nCorner Detection が予測した大量の「角」たちを sampling \u0026 grouping し、大量の「文字領域候補」をつくります。\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/d62ec4c7-2e8b-56e9-045c-af825dfac279.png)\n（Corner Detection は概念的にはわかりやすいですが、実際には default box を用意して offset 計算して...と、SSD や YOLO と同程度には複雑なことをしています。詳細は論文をご参照ください...）\n\nCorner Detection と並行して、Position Sensitive Segmentation 側では、各ピクセルを「文字領域の右上」「右下」「左上」「左下」の 4 クラス（+ 背景）に分類します。\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/f8a4b98a-a428-48be-2d97-f57246f53a5a.png)\nそれぞれ、白が「左上」、赤が「右上」、青が「左下」、緑が「右下」に分類された領域です。\n\nさいごに、Corner Detection によって生成された大量の「文字領域候補」を、Position Sensitive Segmentation の結果との整合性に応じてスコアづけします。\n「文字領域候補」の左上にあるピクセルが Segmentation によって「左上」に分類されていればいるほど高いスコアになります。\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/aad5fe04-8bad-c758-fa7a-fd037c62f877.png)\n\n## 感想\n\nInstance Segmentation 方式は後処理が複雑すぎる、という問題提起のわりには、提案手法の後処理も相当大変そうという印象があります。\nまた、Corner Detection 部分はやけに複雑で、なぜこんなに複雑なことをしているのかあんまり理解できませんでした。\n一方、Object Detection 系のやり方と Segmentation 系のやり方を組み合わせる手法としては概念的にもわかりやすい構成で面白かったです。\nPosition Sensitive Segmentation というアプローチもこの論文を読むまで知らなかったので勉強になりました。\n\nこの論文の少しあとに ECCV 2018 に通った論文で関連していそうなものとして、CornerNet と PixelLink という論文があります。\n[CornerNet: Detecting Objects as Paired Keypoints](https://arxiv.org/abs/1808.01244) は、 Corner を予測 + ピクセル単位の Embedding を計算 → Embedding の距離に応じて Corner のペアを作っていくという手法で、よりシンプルに Corner のグルーピングを実現しています。\nまた、[PixelLink: Detecting Scene Text via Instance Segmentation](https://arxiv.org/abs/1801.01315) は text/non-text の segmentation + 隣接ピクセルと連結するかしないかの 2 クラス分類を組み合わせて Instance Segmentation をし、文字領域検出を行っています。\nどちらもとてもおもしろい論文なのでおすすめです。\n","contentMarkdown":"\n画像中の文字領域検出における 2 つの主流な手法のいいとこ取りを目指した論文、 Multi-Oriented Scene Text Detection via Corner Localization and Region Segmentation を読んでみました。\n\nこの記事は、Wantedly の勉強会で取り上げられた論文・技術をまとめたものです。\n[2018 年に読んだ機械学習系論文・技術まとめ at Wantedly Advent Calendar 2018 - Qiita](https://qiita.com/advent-calendar/2018/wantedly_ml)\n\n## Reference\n\n- Multi-Oriented Scene Text Detection via Corner Localization and Region Segmentation [Pengyuan Lyu, Cong Yao, Wenhao Wu, Shuicheng Yan, Xiang Bai. CVPR 2018]\n- https://arxiv.org/abs/1802.08948\n\n（文中の図表は論文より引用しています）\n\n## Scene Text Detection\n\nScene Text Detection は、風景写真のなかにある文字領域（かんばん、ポスターなど）を検出するタスクです。\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/a78b72da-8550-641f-819b-ed024d9be7fa.png)\n(Figure 6.)\n\nCNN を活用した研究が進んでおり、現在では 2 つのアプローチが主流となっています。（このあたりは [【論文読み】Semi-convolutional Operators for Instance Segmentation](https://qiita.com/agatan/items/2cf1209b7370db45eba5) や [[論文紹介] Focal Loss for Dense Object Detection](https://qiita.com/agatan/items/53fe8d21f2147b0ac982) でもすこし触れています）\n\n1 つ目は、文字領域検出を、一般的な物体検知(Object Detection)の特殊系とみなして解く手法です。\n物体検知に対するアプローチとして主流なのは bounding box の座標を regression として解くというものです。\nこの場合、bounding box 形式で当てに行くので、歪んだ形状への対応が難しく、縦横比が大きく偏った文字領域に弱いといった問題があります。\n[EAST: An Efficient and Accurate Scene Text Detector](https://arxiv.org/abs/1704.03155) [X. Zhou et al., CVPR 2018] はこちらのアプローチを採用しています。\n\n2 つ目は、Instance Segmentation として解くアプローチです。\nピクセル単位で文字領域かどうかの 2 クラス分類 + なんらかの方法でインスタンスの分離を行うという方法ですが、インスタンスの分離には複雑な後処理を要するケースが多く、複雑さや実行時間に問題があります。\n[PixelLink: Detecting Scene Text via Instance Segmentation](https://arxiv.org/abs/1801.01315) [D. Deng et al., AAAI 2018] が代表例です。（論文中で refer されているのは Multi-oriented Text Detection with Fully Convolutional Networks [Z. Zhang et al., CVPR 2016])\n\nこの論文ではこれらの 2 つの手法をいいとこ取りした Scene Text Detector を提案しています。\n\n## Network Architecture\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/281d3451-0e23-c20b-bb18-88b46495da15.png)\n\n全体像は ↑ の図のようになっています。\n**Corner Detection** と **Position Sensitive Segmentation** の 2 つからなる architecture です。\n\nCorner Detection はその名の通り、文字領域の角の位置を予測します。ただし、「角である」ことだけを考慮し、「どの 4 つの組み合わせが 1 領域を表しているのか」は考えません。\nCorner Detection が予測した大量の「角」たちを sampling \u0026 grouping し、大量の「文字領域候補」をつくります。\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/d62ec4c7-2e8b-56e9-045c-af825dfac279.png)\n（Corner Detection は概念的にはわかりやすいですが、実際には default box を用意して offset 計算して...と、SSD や YOLO と同程度には複雑なことをしています。詳細は論文をご参照ください...）\n\nCorner Detection と並行して、Position Sensitive Segmentation 側では、各ピクセルを「文字領域の右上」「右下」「左上」「左下」の 4 クラス（+ 背景）に分類します。\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/f8a4b98a-a428-48be-2d97-f57246f53a5a.png)\nそれぞれ、白が「左上」、赤が「右上」、青が「左下」、緑が「右下」に分類された領域です。\n\nさいごに、Corner Detection によって生成された大量の「文字領域候補」を、Position Sensitive Segmentation の結果との整合性に応じてスコアづけします。\n「文字領域候補」の左上にあるピクセルが Segmentation によって「左上」に分類されていればいるほど高いスコアになります。\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/aad5fe04-8bad-c758-fa7a-fd037c62f877.png)\n\n## 感想\n\nInstance Segmentation 方式は後処理が複雑すぎる、という問題提起のわりには、提案手法の後処理も相当大変そうという印象があります。\nまた、Corner Detection 部分はやけに複雑で、なぜこんなに複雑なことをしているのかあんまり理解できませんでした。\n一方、Object Detection 系のやり方と Segmentation 系のやり方を組み合わせる手法としては概念的にもわかりやすい構成で面白かったです。\nPosition Sensitive Segmentation というアプローチもこの論文を読むまで知らなかったので勉強になりました。\n\nこの論文の少しあとに ECCV 2018 に通った論文で関連していそうなものとして、CornerNet と PixelLink という論文があります。\n[CornerNet: Detecting Objects as Paired Keypoints](https://arxiv.org/abs/1808.01244) は、 Corner を予測 + ピクセル単位の Embedding を計算 → Embedding の距離に応じて Corner のペアを作っていくという手法で、よりシンプルに Corner のグルーピングを実現しています。\nまた、[PixelLink: Detecting Scene Text via Instance Segmentation](https://arxiv.org/abs/1801.01315) は text/non-text の segmentation + 隣接ピクセルと連結するかしないかの 2 クラス分類を組み合わせて Instance Segmentation をし、文字領域検出を行っています。\nどちらもとてもおもしろい論文なのでおすすめです。\n","slug":"【論文読み】_Multi-Oriented_Scene_Text_Detection_via_Corner_Localization_and_Region_Segmentation","title":"【論文読み】 Multi-Oriented Scene Text Detection via Corner Localization and Region Segmentation","timestamp":1549984342000,"tags":["画像処理","DeepLearning","OCR","論文読み"]},{"rawMarkdown":"---\ntitle: \"【論文読み】Semi-convolutional Operators for Instance Segmentation\"\ndate: 2019-02-11T16:59:46+09:00\ntags: [\"DeepLearning\", \"論文読み\"]\nurl: https://qiita.com/agatan/items/2cf1209b7370db45eba5\n---\n\nInstance Segmentation のタスクに対する手法を整理・分解し、精度をより向上する `Semi-convolutional operators` を提案した論文です。\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/8164ed4c-3f5d-c772-e21d-7d02d5146461.png)\n\nこの記事は、Wantedly の勉強会で取り上げられた論文・技術をまとめたものです。\n[2018 年に読んだ機械学習系論文・技術まとめ at Wantedly Advent Calendar 2018 - Qiita](https://qiita.com/advent-calendar/2018/wantedly_ml)\n\n## Reference\n\n- Semi-convolutional Operators for Instance Segmentation [David Novotny, Samuel Albanie, Diane Larlus, and Andrea Vedaldi. ECCV 2018]\n- https://arxiv.org/abs/1807.10712\n\n（文中の図表は論文より引用しています）\n\n## Instance Segmentation\n\nまずはじめに簡単に Instance Segmentation というタスクと、現在主流とされているアプローチについて述べます。\n\nInstance Segmentation とは、画像の各 Pixel について、 **どのクラスに属すか、どのインスタンスに属するか** を予測するタスクです。\n入力画像を「この領域は人、この領域は車、...」というように色塗りしていくタスクです。\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/146f0988-659b-4f15-df32-e065ddae5e70.png)(Fig. 5 より)\n\nInstance Segmentation において重要なのが **どのインスタンスに属するか** も予測しなければならないという点です。\nたとえば人が 3 人で肩を組んでいるような画像の場合、どこからどこまでが 1 人目かを予測しなければなりません。\n一方、インスタンスを考慮せず色塗りをしていくようなタスクを Semantic Segmentation といいます。\n\nSemantic Segmentation の場合は、入力画像の各 Pixel について多クラス分類を行えば Segmentation の完成になります。\nInstance Segmentation ではそれに加えて個々のインスタンスを区別するような仕組みが必要になります。\n\n### propose \u0026 verify\n\nInstance Segmentation タスクへのアプローチとして、現在主流とされているのは Mask R-CNN [^1] に代表される Region based な手法です。\n（Mask R-CNN は FAIR から出ている論文で、 OSS として公開されている Detectron に実装が含まれています。 https://github.com/facebookresearch/Detectron ）\n\n[^1]: K. He, et al., https://arxiv.org/abs/1703.06870\n\nMask R-CNN は、物体のクラスと bounding box だけを予測する Object Detection タスクへのアプローチを応用しています。\nまず Object Detection をすることで「この bounding box に人間が 1 人いる」ということを予測し、その後 bounding box 内を色塗りしていきます。\nObject Detection として bounding box を予測している時点で Instance を分離することが出来ています。色塗りのフェーズでは、すでに Instance が分離されているので単なる Pixel 単位の 2 クラス分類をやればよいことになります。\n\nはじめに Region を提案し、その中を精査するこれらの手法を、この論文では _propose \u0026 verify_ (P\u0026V) と呼んでいます。\n\nここで、 **P\u0026V は必ず一度矩形で切り取ってから色塗りをしなければならない** という点が問題になります。\n予測したい物体は必ずしも矩形で近似できるような形状をしているとは限りません。\n実際の形状と極端にかけ離れた場合、bounding box を予測すること自体が難しく、また Instance の分離も難しくなります。\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/39b808c2-cc63-a245-73c1-5b6ebc89c9be.png)\n\n### instance coloring\n\nP\u0026V の問題点を解決する方法として、Pixel ごとに **ラベル + Instance の identifier となる何か** を予測する方法があります。\nこれらをこの論文では _instance coloring_ (IC) と呼んでいます。\n\n「Instance の identifier となる何か」 は、連番などではうまく学習できません（どの Object が ID 1 なのか ID 2 なのかわからない）。\nそこで、 Pixel ごとに低次元の embedding を出力し、**同じ Instance に所属する Pixel の embedding たちが似たものになるように学習します**。\n入力画像に対して、Pixel ごとのラベルと embedding を出力し、embedding を基に Pixel たちをクラスタリングすることで Instance を分離します。\n\nIC の良いところは、典型的な image-to-image の問題と同じネットワーク構造を利用できるところです。\nSemantic Segmetation, Style Transfer など、画像を入力とし同じサイズの feature map を出力とするタスクは他にも数多くあり、それらと同じ構造をシンプルに流用できるのは大きな利点になります。\n（P\u0026V の場合は Region Proposal + Region ごとの Coloring が必要で、ネットワーク構造としてはかなり複雑かつ独特なものになります）\n\n一方、IC であまり精度が出ない大きな理由の一つに **画像的に似た領域が繰り返されると Instance の分離に失敗する** という問題があります。\nimage-to-image のネットワークは通常 Convolutional operators をベースにしていますが、CNN の出力は、入力である pixel の特徴量にのみ依存し、 **座標は全く結果に影響を及ぼしません** 。\nそのため、画像的にそっくりな領域が複数あると、それらの pixel に対する embedding は同じような値になってしまい、クラスタリングがうまくいきません。\n\n## Semi-convolutional operators\n\n一般的な IC では、出力された embedding が次の条件をみたすことを目標とします。\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/00e031cb-860d-d25c-af24-213ad5fd8565.png)\n\nここで、 $\\Omega$ は全 Pixel の集合、 $x$ は入力画像、 $\\Phi$ は学習したい関数（NN）、 $S_k$ はクラス k の segmentation mask、 $M$ はマージン （hyperparameter） です。\n言葉で説明すると、 **同じクラスに属する Pixel $u$, $v$ の embedding の距離をより近づけ、違うクラスに属する場合はより遠ざける** という感じです。\n$M$ は分離境界をよりくっきりさせるためのパラメータです。\n\nさきほど述べたように $\\Phi$ は CNN であり、座標情報を加味できません。\nSemi-convolutinal 版では、 $\\Phi$ の代わりに次のような $\\Psi$ を考えます。\n\n```math\n\\Psi_u(x) = f(\\Phi_u(x), u)\n```\n\nここで、 $u$ は Pixel の座標を表し、 $f$ は $\\Phi$ の結果と座標情報を合成するなんらかの関数です。\n$f$ の簡単な例としては、単純な足し算が考えられます。\n$\\Psi$ は、CNN の結果に加えて座標情報も持ち合わせているため、IC の弱点を克服できています。\n$f$ を単純な加算とし、うまく学習が成功した場合、各 Instance ごとに centroid $c_k$ が決定され、\n\n```math\n\\forall u \\in S_k: \\Phi_u(x) + u = c_k\n```\n\nとなるように $\\Phi$ が学習されます。\nこれを可視化すると次の画像のようになります。\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/74875b7b-be1b-4eb3-0e3d-f3c53fced5a8.png)\n(Fig.2 より)\n\n各インスタンス内の Pixel から、なんとなく中心っぽい場所へベクトルが伸びているのがわかります。\n\n実際の学習の際の損失関数は次のようになります。\n\n\u003cimg src=\"https://qiita-image-store.s3.amazonaws.com/0/39030/914e8f93-3c56-8d39-270e-1a5f3703bc79.png\" width=\"60%\"\u003e\n\n同じインスタンスに属する Pixel の embedding たちを平均値になるべく近づける、というのが損失関数になります。\n（マージンの考えも含まれていないし、「違うインスタンスとの距離を取る」という損失も含まれていないですが、これで十分に良い学習ができたと述べられています。）\n\n実際にはもうちょっと複雑な $\\Psi$ や距離の定義を使っていますが、概要としては上記のようなものを Semi-convolutional operators として提案しています。\n\n## Experiments\n\nMask R-CNN との統合もこの論文の重要な topic なのですが、ぶっちゃけ論文を読んだほうがわかりやすいので飛ばして実験結果をざーっと眺めてみます。\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/e7516b68-9a66-154b-45ae-61199e0de90a.png)(Fig. 3 より)\n\nまずはじめに、 画像的にそっくりな領域が繰り返されてもうまく Instance を分離できることを確認しています。\n(c) は通常の Conv. のみを使って IC を行った場合の結果です。クラスタリングに大失敗していることがわかります。\n一方 (d) の Semi-conv. 版ではきれいな分離が実現されています。\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/09bb8f76-5ba7-ad51-af97-2dbfc902cd66.png)\n\nつぎに線虫の segmentation です。こちらは P\u0026V のように矩形で認識するタイプの手法がニガテとするようなタスクです。\n現在主流である Mask RCNN よりも良い結果が示されています。\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/77a2f7fe-ee56-13ae-f49b-faf825cbf406.png)\n\nより一般的なデータである PASCAL VOC2012 に対しても Mask RCNN より良い結果となっています（Mask RCNN に Semi-conv. の仕組みを組み込んだもので比較しています。）\n\n## まとめと感想\n\ninstance coloring の手法をまったく知らなかったのですが、 [CornerNet: Detecting Objects as Paired Keypoints](https://arxiv.org/abs/1808.01244) で Pixel ごとの embedding をクラスタリングしてペアを作るという手法を知り、興味を持ったのでその関連で読んでみた論文です。\nP\u0026V 形式はかなり複雑な構造になるので、それを避けられるならすごく面白いなと思ったのですが、この論文では Mask R-CNN と組み合わせることで精度向上と言っているので、まだまだ IC 単体で勝てる感じではないのでしょうか？\n\n同じタスクに対して全く違う 2 つのアプローチが（比較対象になるくらいには）同じような成果を出しているのも面白いところです。segmentation は主流ではなかった分、まだまだ改善がありそうで楽しみです。\n","contentMarkdown":"\nInstance Segmentation のタスクに対する手法を整理・分解し、精度をより向上する `Semi-convolutional operators` を提案した論文です。\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/8164ed4c-3f5d-c772-e21d-7d02d5146461.png)\n\nこの記事は、Wantedly の勉強会で取り上げられた論文・技術をまとめたものです。\n[2018 年に読んだ機械学習系論文・技術まとめ at Wantedly Advent Calendar 2018 - Qiita](https://qiita.com/advent-calendar/2018/wantedly_ml)\n\n## Reference\n\n- Semi-convolutional Operators for Instance Segmentation [David Novotny, Samuel Albanie, Diane Larlus, and Andrea Vedaldi. ECCV 2018]\n- https://arxiv.org/abs/1807.10712\n\n（文中の図表は論文より引用しています）\n\n## Instance Segmentation\n\nまずはじめに簡単に Instance Segmentation というタスクと、現在主流とされているアプローチについて述べます。\n\nInstance Segmentation とは、画像の各 Pixel について、 **どのクラスに属すか、どのインスタンスに属するか** を予測するタスクです。\n入力画像を「この領域は人、この領域は車、...」というように色塗りしていくタスクです。\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/146f0988-659b-4f15-df32-e065ddae5e70.png)(Fig. 5 より)\n\nInstance Segmentation において重要なのが **どのインスタンスに属するか** も予測しなければならないという点です。\nたとえば人が 3 人で肩を組んでいるような画像の場合、どこからどこまでが 1 人目かを予測しなければなりません。\n一方、インスタンスを考慮せず色塗りをしていくようなタスクを Semantic Segmentation といいます。\n\nSemantic Segmentation の場合は、入力画像の各 Pixel について多クラス分類を行えば Segmentation の完成になります。\nInstance Segmentation ではそれに加えて個々のインスタンスを区別するような仕組みが必要になります。\n\n### propose \u0026 verify\n\nInstance Segmentation タスクへのアプローチとして、現在主流とされているのは Mask R-CNN [^1] に代表される Region based な手法です。\n（Mask R-CNN は FAIR から出ている論文で、 OSS として公開されている Detectron に実装が含まれています。 https://github.com/facebookresearch/Detectron ）\n\n[^1]: K. He, et al., https://arxiv.org/abs/1703.06870\n\nMask R-CNN は、物体のクラスと bounding box だけを予測する Object Detection タスクへのアプローチを応用しています。\nまず Object Detection をすることで「この bounding box に人間が 1 人いる」ということを予測し、その後 bounding box 内を色塗りしていきます。\nObject Detection として bounding box を予測している時点で Instance を分離することが出来ています。色塗りのフェーズでは、すでに Instance が分離されているので単なる Pixel 単位の 2 クラス分類をやればよいことになります。\n\nはじめに Region を提案し、その中を精査するこれらの手法を、この論文では _propose \u0026 verify_ (P\u0026V) と呼んでいます。\n\nここで、 **P\u0026V は必ず一度矩形で切り取ってから色塗りをしなければならない** という点が問題になります。\n予測したい物体は必ずしも矩形で近似できるような形状をしているとは限りません。\n実際の形状と極端にかけ離れた場合、bounding box を予測すること自体が難しく、また Instance の分離も難しくなります。\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/39b808c2-cc63-a245-73c1-5b6ebc89c9be.png)\n\n### instance coloring\n\nP\u0026V の問題点を解決する方法として、Pixel ごとに **ラベル + Instance の identifier となる何か** を予測する方法があります。\nこれらをこの論文では _instance coloring_ (IC) と呼んでいます。\n\n「Instance の identifier となる何か」 は、連番などではうまく学習できません（どの Object が ID 1 なのか ID 2 なのかわからない）。\nそこで、 Pixel ごとに低次元の embedding を出力し、**同じ Instance に所属する Pixel の embedding たちが似たものになるように学習します**。\n入力画像に対して、Pixel ごとのラベルと embedding を出力し、embedding を基に Pixel たちをクラスタリングすることで Instance を分離します。\n\nIC の良いところは、典型的な image-to-image の問題と同じネットワーク構造を利用できるところです。\nSemantic Segmetation, Style Transfer など、画像を入力とし同じサイズの feature map を出力とするタスクは他にも数多くあり、それらと同じ構造をシンプルに流用できるのは大きな利点になります。\n（P\u0026V の場合は Region Proposal + Region ごとの Coloring が必要で、ネットワーク構造としてはかなり複雑かつ独特なものになります）\n\n一方、IC であまり精度が出ない大きな理由の一つに **画像的に似た領域が繰り返されると Instance の分離に失敗する** という問題があります。\nimage-to-image のネットワークは通常 Convolutional operators をベースにしていますが、CNN の出力は、入力である pixel の特徴量にのみ依存し、 **座標は全く結果に影響を及ぼしません** 。\nそのため、画像的にそっくりな領域が複数あると、それらの pixel に対する embedding は同じような値になってしまい、クラスタリングがうまくいきません。\n\n## Semi-convolutional operators\n\n一般的な IC では、出力された embedding が次の条件をみたすことを目標とします。\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/00e031cb-860d-d25c-af24-213ad5fd8565.png)\n\nここで、 $\\Omega$ は全 Pixel の集合、 $x$ は入力画像、 $\\Phi$ は学習したい関数（NN）、 $S_k$ はクラス k の segmentation mask、 $M$ はマージン （hyperparameter） です。\n言葉で説明すると、 **同じクラスに属する Pixel $u$, $v$ の embedding の距離をより近づけ、違うクラスに属する場合はより遠ざける** という感じです。\n$M$ は分離境界をよりくっきりさせるためのパラメータです。\n\nさきほど述べたように $\\Phi$ は CNN であり、座標情報を加味できません。\nSemi-convolutinal 版では、 $\\Phi$ の代わりに次のような $\\Psi$ を考えます。\n\n```math\n\\Psi_u(x) = f(\\Phi_u(x), u)\n```\n\nここで、 $u$ は Pixel の座標を表し、 $f$ は $\\Phi$ の結果と座標情報を合成するなんらかの関数です。\n$f$ の簡単な例としては、単純な足し算が考えられます。\n$\\Psi$ は、CNN の結果に加えて座標情報も持ち合わせているため、IC の弱点を克服できています。\n$f$ を単純な加算とし、うまく学習が成功した場合、各 Instance ごとに centroid $c_k$ が決定され、\n\n```math\n\\forall u \\in S_k: \\Phi_u(x) + u = c_k\n```\n\nとなるように $\\Phi$ が学習されます。\nこれを可視化すると次の画像のようになります。\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/74875b7b-be1b-4eb3-0e3d-f3c53fced5a8.png)\n(Fig.2 より)\n\n各インスタンス内の Pixel から、なんとなく中心っぽい場所へベクトルが伸びているのがわかります。\n\n実際の学習の際の損失関数は次のようになります。\n\n\u003cimg src=\"https://qiita-image-store.s3.amazonaws.com/0/39030/914e8f93-3c56-8d39-270e-1a5f3703bc79.png\" width=\"60%\"\u003e\n\n同じインスタンスに属する Pixel の embedding たちを平均値になるべく近づける、というのが損失関数になります。\n（マージンの考えも含まれていないし、「違うインスタンスとの距離を取る」という損失も含まれていないですが、これで十分に良い学習ができたと述べられています。）\n\n実際にはもうちょっと複雑な $\\Psi$ や距離の定義を使っていますが、概要としては上記のようなものを Semi-convolutional operators として提案しています。\n\n## Experiments\n\nMask R-CNN との統合もこの論文の重要な topic なのですが、ぶっちゃけ論文を読んだほうがわかりやすいので飛ばして実験結果をざーっと眺めてみます。\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/e7516b68-9a66-154b-45ae-61199e0de90a.png)(Fig. 3 より)\n\nまずはじめに、 画像的にそっくりな領域が繰り返されてもうまく Instance を分離できることを確認しています。\n(c) は通常の Conv. のみを使って IC を行った場合の結果です。クラスタリングに大失敗していることがわかります。\n一方 (d) の Semi-conv. 版ではきれいな分離が実現されています。\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/09bb8f76-5ba7-ad51-af97-2dbfc902cd66.png)\n\nつぎに線虫の segmentation です。こちらは P\u0026V のように矩形で認識するタイプの手法がニガテとするようなタスクです。\n現在主流である Mask RCNN よりも良い結果が示されています。\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/77a2f7fe-ee56-13ae-f49b-faf825cbf406.png)\n\nより一般的なデータである PASCAL VOC2012 に対しても Mask RCNN より良い結果となっています（Mask RCNN に Semi-conv. の仕組みを組み込んだもので比較しています。）\n\n## まとめと感想\n\ninstance coloring の手法をまったく知らなかったのですが、 [CornerNet: Detecting Objects as Paired Keypoints](https://arxiv.org/abs/1808.01244) で Pixel ごとの embedding をクラスタリングしてペアを作るという手法を知り、興味を持ったのでその関連で読んでみた論文です。\nP\u0026V 形式はかなり複雑な構造になるので、それを避けられるならすごく面白いなと思ったのですが、この論文では Mask R-CNN と組み合わせることで精度向上と言っているので、まだまだ IC 単体で勝てる感じではないのでしょうか？\n\n同じタスクに対して全く違う 2 つのアプローチが（比較対象になるくらいには）同じような成果を出しているのも面白いところです。segmentation は主流ではなかった分、まだまだ改善がありそうで楽しみです。\n","slug":"【論文読み】Semi-convolutional_Operators_for_Instance_Segmentation","title":"【論文読み】Semi-convolutional Operators for Instance Segmentation","timestamp":1549871986000,"tags":["DeepLearning","論文読み"]},{"rawMarkdown":"---\ntitle: \"【論文紹介】Concurrent Spatial and Channel Squeeze \u0026 Excitation in Fully Convolutional Networks\"\ndate: 2019-01-04T14:51:07+09:00\ntags: [\"Python\", \"DeepLearning\", \"Keras\", \"TensorFlow\", \"論文読み\"]\nurl: https://qiita.com/agatan/items/61546d71e7ea7ad14b11\n---\n\nFully Convolutional Network (FCN) の性能を enhance する Concurrent Spatial and Channel Squeeze \u0026 Excitation (scSE) というモジュールを提案した論文です。\n既存の良いとされてきたモデルたちに計算量をそこまで増やさずに \u0026 簡単に組み込むことができ、 Image Segmentation などのタスクで性能を向上させることができます。\n\n[ILSVRC 2017 画像分類 Top の手法 Squeeze-and-Excitation Networks - Qiita](https://qiita.com/agatan/items/8cf2566908228eaa5450) で紹介した SE モジュールの後継にあたります。\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/f606f8d0-9510-f251-31d8-e9091ed031b9.png)\n\n## Reference\n\n- Abhijit Guha Roy, et al., MICCAI 2018\n- https://arxiv.org/abs/1803.02579\n\n文中の図表は論文から引用しています。\n\nこの記事は、Wantedly の勉強会で取り上げられた論文・技術をまとめたものです。\n[2018 年に読んだ機械学習系論文・技術まとめ at Wantedly Advent Calendar 2018 - Qiita](https://qiita.com/advent-calendar/2018/wantedly_ml)\n\n## Squeeze and Excitation を Image Segmentation に応用する\n\nSqueeze and Excitation (SE) モジュールは、[Squeeze-and-Excitation Networks](https://arxiv.org/abs/1709.01507) で提案されたもので、 ILSVRC 2017 でトップのスコアを記録しています。\nSE は Channel 間の関係性を考慮できるようにしたい、というモチベーションで、 チャンネルごとに画像全体の activation の平均を取り（Squeeze)、それをもとにチャンネル間の Attention をとる（Excitation）というものでした。（[ILSVRC 2017 画像分類 Top の手法 Squeeze-and-Excitation Networks - Qiita](https://qiita.com/agatan/items/8cf2566908228eaa5450)）\n本論文ではこのオリジナルの SE モジュールのことを channel SE (Spatial Squeeze and Channel Excitation, cSE) と呼んでいます。\n\n```python\nfrom tensorflow.python.keras.layers import GlovelAveragePooling2D, Dense, multiply\n\ndef spatial_squeeze_and_channel_excitation(x, ch, ratio=16):\n    squeeze = GlobalAveragePooling2D()(x)\n    z = Dense(ch // ratio, activation='relu')(squeeze)\n    excitation = Dense(ch, activation='sigmoid')(x)\n    return multiply([x, excitation])\n```\n\n本論文では Image Classification の性能を大きく向上した SE モジュールを、 Image Segmentation に応用することを考えます。\nImage Segmentation のタスクでは、Fully Convolutional な Architecture がよく採用されます。\nこの論文では、U-Net[^1] やそこから派生した SkipDeconv-Net[^2]， Fully Convolutional DenseNet[^3] などに対して SE モジュール的な考え方で性能を向上できないか実験しています。\n\n[^1]: Ronneberger O, Fischer P, Brox T. U-net: Convolutional networks for biomedical image segmentation. In Proc. MICCAI, Springer 2015, pp. 234-241.\n[^2]: Roy, A.G., Conjeti, S., Sheet, D., Katouzian, A., Navab, N. and Wachinger, C., 2017, September. Error Corrective Boosting for Learning Fully Convolutional Networks with Limited Data. In MICCAI, pp. 231-239, Springer.\n[^3]: J ́egou, S., Drozdzal, M., Vazquez, D., Romero, A. and Bengio, Y., 2017, July. The one hundred layers tiramisu: Fully convolutional densenets for semantic segmentation. In CVPR Workshop, pp. 1175-1183, IEEE.\n\nが、実際に SE モジュールをこれらの FCN に組み込んでみると、 Image Classification のときよりも性能が上がりづらいという結果が得られています。\nこの論文では、 「Image Segmentation は pixel-wise の情報が重要であり、チャンネルごとに画像全体から平均を取る cSE ではピクセル単位の情報をうまく enhance できていないのでは」という仮説を立てています。\n\n## Channel Squeeze and Spatial Excitation Block (sSE)\n\nそこでこの論文で提案されているので、 sSE です。\n名前のとおりですが、 Channel 方向に Squeeze し、Pixel ごとに Excitation を計算します。\ncSE は画像全体（Spatial）で Squeeze し、Channel ごとの Excitation を計算しているので、その逆をやっているというイメージです。\n\n\u003cimg src=\"https://qiita-image-store.s3.amazonaws.com/0/39030/7c6f2822-ff51-c4f6-2d3f-6c0422dca3fa.png\" width=\"60%\"\u003e\n\n実装はものすごく単純です。以下に `tf.keras` をつかった場合の実装例を載せます。\n\n```python\nimport tensorflow as tf\n\ndef channel_squeeze_and_spatial_excitation(x):\n    excitation = tf.keras.layers.Conv2D(filters=1, kernel_size=1, activation='sigmoid')(x)\n    return tf.keras.layers.multiply([x, excitation])\n```\n\n`Conv2D(filters=1, kernel_size=1, activation='sigmoid')` で、pixel ごとに 1 チャンネルの値を 0~1 で出力させます。\nこれが「ある pixel における excitation」になります。出力は、入力である feature map と excitation の element-wise な積です。\n\n## Spatial and Channel Squeeze \u0026 Excitation (scSE)\n\nまた、提案手法である sSE とオリジナルの cSE は conflict しないので、両方採用してしまおう、というのが scSE です。\n\n\u003cimg src=\"https://qiita-image-store.s3.amazonaws.com/0/39030/fcbc2c76-3947-f587-eb8a-4422b4a28b7e.png\" width=\"70%\"\u003e\n\nこの図の上部が sSE、下部が cSE です。同じ入力からそれぞれを計算し、最後に単純に足し算したものを scSE と呼んでいます。\n\n```python\ndef _concurrent_spartial_and_channel_se(input_feature, ch, ratio=16):\n    cse = _spatial_squeeze_and_channel_excitation(input_feature, ch, ratio=ratio)\n    sse = _channel_squeeze_and_spatial_excitation(input_feature)\n    return tf.keras.layers.Add()([cse, sse])\n```\n\nこの論文で実験に使われている U-Net の場合、 scSE を使った場合でも計算量は 1.5% 程度の増加で済んでいます。\n\n## Experiments\n\nいくつかのネットワークについて、「素の状態」「cSE」「sSE」「scSE」の 4 パターンで実験しています。\nここでは DenseNet のケースについてまとめた図を論文中の Fig.2 から引用します。\n詳細や全体像は論文を参照してください。\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/31a9cee5-5d22-b682-1400-b3f98bfe5ae3.png)\n\n横軸はタスク名です。\nタスクにもよりますが、概ね `DenseNets` \u003c `DenseNets + cSE` \u003c `DenseNets + sSE` \u003c `DenseNets + scSE` になっているように見えます。\ncSE だけをいれると素の状態より性能が悪くなっているケースも見られるのが面白いところです。\n\n## まとめと感想\n\nタスクの特性を見て仮説を立て、実際にそれがうまくハマっているという論文で、よみやすいし納得感のある論文でした。\n実装の容易さと試しやすさ（既存モデルへ着脱できる）がうれしい手法で、実際に活用しているモデルに組み込まれています。\n","contentMarkdown":"\nFully Convolutional Network (FCN) の性能を enhance する Concurrent Spatial and Channel Squeeze \u0026 Excitation (scSE) というモジュールを提案した論文です。\n既存の良いとされてきたモデルたちに計算量をそこまで増やさずに \u0026 簡単に組み込むことができ、 Image Segmentation などのタスクで性能を向上させることができます。\n\n[ILSVRC 2017 画像分類 Top の手法 Squeeze-and-Excitation Networks - Qiita](https://qiita.com/agatan/items/8cf2566908228eaa5450) で紹介した SE モジュールの後継にあたります。\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/f606f8d0-9510-f251-31d8-e9091ed031b9.png)\n\n## Reference\n\n- Abhijit Guha Roy, et al., MICCAI 2018\n- https://arxiv.org/abs/1803.02579\n\n文中の図表は論文から引用しています。\n\nこの記事は、Wantedly の勉強会で取り上げられた論文・技術をまとめたものです。\n[2018 年に読んだ機械学習系論文・技術まとめ at Wantedly Advent Calendar 2018 - Qiita](https://qiita.com/advent-calendar/2018/wantedly_ml)\n\n## Squeeze and Excitation を Image Segmentation に応用する\n\nSqueeze and Excitation (SE) モジュールは、[Squeeze-and-Excitation Networks](https://arxiv.org/abs/1709.01507) で提案されたもので、 ILSVRC 2017 でトップのスコアを記録しています。\nSE は Channel 間の関係性を考慮できるようにしたい、というモチベーションで、 チャンネルごとに画像全体の activation の平均を取り（Squeeze)、それをもとにチャンネル間の Attention をとる（Excitation）というものでした。（[ILSVRC 2017 画像分類 Top の手法 Squeeze-and-Excitation Networks - Qiita](https://qiita.com/agatan/items/8cf2566908228eaa5450)）\n本論文ではこのオリジナルの SE モジュールのことを channel SE (Spatial Squeeze and Channel Excitation, cSE) と呼んでいます。\n\n```python\nfrom tensorflow.python.keras.layers import GlovelAveragePooling2D, Dense, multiply\n\ndef spatial_squeeze_and_channel_excitation(x, ch, ratio=16):\n    squeeze = GlobalAveragePooling2D()(x)\n    z = Dense(ch // ratio, activation='relu')(squeeze)\n    excitation = Dense(ch, activation='sigmoid')(x)\n    return multiply([x, excitation])\n```\n\n本論文では Image Classification の性能を大きく向上した SE モジュールを、 Image Segmentation に応用することを考えます。\nImage Segmentation のタスクでは、Fully Convolutional な Architecture がよく採用されます。\nこの論文では、U-Net[^1] やそこから派生した SkipDeconv-Net[^2]， Fully Convolutional DenseNet[^3] などに対して SE モジュール的な考え方で性能を向上できないか実験しています。\n\n[^1]: Ronneberger O, Fischer P, Brox T. U-net: Convolutional networks for biomedical image segmentation. In Proc. MICCAI, Springer 2015, pp. 234-241.\n[^2]: Roy, A.G., Conjeti, S., Sheet, D., Katouzian, A., Navab, N. and Wachinger, C., 2017, September. Error Corrective Boosting for Learning Fully Convolutional Networks with Limited Data. In MICCAI, pp. 231-239, Springer.\n[^3]: J ́egou, S., Drozdzal, M., Vazquez, D., Romero, A. and Bengio, Y., 2017, July. The one hundred layers tiramisu: Fully convolutional densenets for semantic segmentation. In CVPR Workshop, pp. 1175-1183, IEEE.\n\nが、実際に SE モジュールをこれらの FCN に組み込んでみると、 Image Classification のときよりも性能が上がりづらいという結果が得られています。\nこの論文では、 「Image Segmentation は pixel-wise の情報が重要であり、チャンネルごとに画像全体から平均を取る cSE ではピクセル単位の情報をうまく enhance できていないのでは」という仮説を立てています。\n\n## Channel Squeeze and Spatial Excitation Block (sSE)\n\nそこでこの論文で提案されているので、 sSE です。\n名前のとおりですが、 Channel 方向に Squeeze し、Pixel ごとに Excitation を計算します。\ncSE は画像全体（Spatial）で Squeeze し、Channel ごとの Excitation を計算しているので、その逆をやっているというイメージです。\n\n\u003cimg src=\"https://qiita-image-store.s3.amazonaws.com/0/39030/7c6f2822-ff51-c4f6-2d3f-6c0422dca3fa.png\" width=\"60%\"\u003e\n\n実装はものすごく単純です。以下に `tf.keras` をつかった場合の実装例を載せます。\n\n```python\nimport tensorflow as tf\n\ndef channel_squeeze_and_spatial_excitation(x):\n    excitation = tf.keras.layers.Conv2D(filters=1, kernel_size=1, activation='sigmoid')(x)\n    return tf.keras.layers.multiply([x, excitation])\n```\n\n`Conv2D(filters=1, kernel_size=1, activation='sigmoid')` で、pixel ごとに 1 チャンネルの値を 0~1 で出力させます。\nこれが「ある pixel における excitation」になります。出力は、入力である feature map と excitation の element-wise な積です。\n\n## Spatial and Channel Squeeze \u0026 Excitation (scSE)\n\nまた、提案手法である sSE とオリジナルの cSE は conflict しないので、両方採用してしまおう、というのが scSE です。\n\n\u003cimg src=\"https://qiita-image-store.s3.amazonaws.com/0/39030/fcbc2c76-3947-f587-eb8a-4422b4a28b7e.png\" width=\"70%\"\u003e\n\nこの図の上部が sSE、下部が cSE です。同じ入力からそれぞれを計算し、最後に単純に足し算したものを scSE と呼んでいます。\n\n```python\ndef _concurrent_spartial_and_channel_se(input_feature, ch, ratio=16):\n    cse = _spatial_squeeze_and_channel_excitation(input_feature, ch, ratio=ratio)\n    sse = _channel_squeeze_and_spatial_excitation(input_feature)\n    return tf.keras.layers.Add()([cse, sse])\n```\n\nこの論文で実験に使われている U-Net の場合、 scSE を使った場合でも計算量は 1.5% 程度の増加で済んでいます。\n\n## Experiments\n\nいくつかのネットワークについて、「素の状態」「cSE」「sSE」「scSE」の 4 パターンで実験しています。\nここでは DenseNet のケースについてまとめた図を論文中の Fig.2 から引用します。\n詳細や全体像は論文を参照してください。\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/31a9cee5-5d22-b682-1400-b3f98bfe5ae3.png)\n\n横軸はタスク名です。\nタスクにもよりますが、概ね `DenseNets` \u003c `DenseNets + cSE` \u003c `DenseNets + sSE` \u003c `DenseNets + scSE` になっているように見えます。\ncSE だけをいれると素の状態より性能が悪くなっているケースも見られるのが面白いところです。\n\n## まとめと感想\n\nタスクの特性を見て仮説を立て、実際にそれがうまくハマっているという論文で、よみやすいし納得感のある論文でした。\n実装の容易さと試しやすさ（既存モデルへ着脱できる）がうれしい手法で、実際に活用しているモデルに組み込まれています。\n","slug":"【論文紹介】Concurrent_Spatial_and_Channel_Squeeze_\u0026_Excitation_in_Fully_Convolutional_Networks","title":"【論文紹介】Concurrent Spatial and Channel Squeeze \u0026 Excitation in Fully Convolutional Networks","timestamp":1546581067000,"tags":["Python","DeepLearning","Keras","TensorFlow","論文読み"]},{"rawMarkdown":"---\ntitle: \"DKN: Deep Knowledge-Aware Network for News Recommendation\"\ndate: 2018-12-19T21:02:20+09:00\ntags: [\"DeepLearning\", \"WWW\", \"Recommendation\", \"論文読み\"]\nurl: https://qiita.com/agatan/items/24c6d8e00f2fc861bb04\n---\n\nニュースの推薦に \"Knowledge Graph\" を活用する論文です。\nMicrosoft Research Asia のチームが WWW 2018 に投稿しています。\n\nこの記事は、Wantedly の勉強会で取り上げられた論文・技術をまとめたものです。\n[2018 年に読んだ機械学習系論文・技術まとめ at Wantedly Advent Calendar 2018 - Qiita](https://qiita.com/advent-calendar/2018/wantedly_ml)\n\n## Reference\n\n- DKN: Deep Knowledge-Aware Network for News Recommendation\n  - Hongwei Wang, Fuzheng Zhang, Xing Xie and Minyi Guo, et al., WWW 2018\n  - https://www2018.thewebconf.org/proceedings/#indus-922\n\n文中の図表は論文より引用しています。\n\n### 概要\n\n一般にニュース中の言葉は、常識的知識を仮定していて凝縮された文章になっています。\n一方で推薦系の既存手法は ニュース中に現れない知識を取り扱えておらず、潜在的なニュース間の関係を活かした探索が出来ていないという問題がありました。\n\nこの論文は、knowledge graph を活用した content-based な recommendation framework である _deep knowledge-aware network_ (DKN) を提案しています。\n\nknowledge graph とは、様々なエンティティを様々なエッジでつないだ heterogeneous なグラフで、たとえば \"モナリザ -[の作者」-\u003e ダ・ヴィンチ\" のような情報を溜め込んだ巨大なグラフです。\nGoogle Search の裏でも活躍しているらしく、一般的な「知識」を構造化された形で表現する方法としてよく使われています。\nknowledge graph を使うことで、 \"Donald Trump\" という単語そのものだけからはわからない、 \"United States\" という単語との関連、\"Politician\" という単語との関連などを導くことができます。\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/6967d5d1-0708-7e59-51c8-db107866ea63.png)\n\n### 何が難しいか\n\nこの論文では、ニュース推薦の難しさとして以下のようなものを挙げています。\n\n- news の推薦は movie などと違って、リアルタイム性が高い問題（いつ publish されたニュースかが重要）であり、news 間の関係性もすぐにダメになる。\n  - なので ID ベースの既存手法（協調フィルタリングなど）は効果が弱い\n- news はユーザによって興味範囲が違うし、ユーザは複数の興味範囲を持っていることがほとんど。\n- news の文言は凝縮されている。\n  - 常識、大量の既知の entity を仮定している。\n  - “Boris Johnson Has Warned Donald Trump To Stick To The Iran Nuclear Deal\" というニュースは “Boris Johnson”, “Donald Trump”, “Iran\", “Nuclear” を知っている前提で書かれている\n  - ↑ に興味のあるユーザはきっと “North Korean EMP Attack Would Cause Mass U.S. Starvation, Says Congressional Report” にも興味があるが、単語レベルでの関係性はほぼ無い\n\nこれらを Knowledge Graph を活用しつつ解決していきます。\n\n### Knowledge Graph Embedding\n\nKnowledge Graph Embedding 自体はこの研究の contribution ではありませんが重要なので簡単に紹介します。\nKnowledge Graph Embedding は、通常の network embedding に近い問題設定で、 knowledge graph における各エンティティとエッジの低次元な embedding を求めるという問題です。\n`(head, relation, trail)` という triplet 構造をなるべく維持したまま、`h, r, t` それぞれを低次元空間で表現することが目標です。\nDKN は translation-based knowledge graph embedding というのを使っています。\n一番簡単な手法は `h + r = t` になるように embedding を定める手法です。\n（他にもいくつか紹介、実験されているけど大体発想は同じなので省略。）\nまずランダムな値で各エンティティ、エッジの embedding を初期化し、 `h + r = t` を満たすように gradient descent で embedding を微調整していきます。\nこれによって、エンティティやエッジ（＝関係性）の低次元なベクトルを得ることができました。\n\n### Deep Knowledge-aware Network (DKN)\n\nDKN 自体は\n\n入力: 候補ニュースと、あるユーザの過去に見たニュースたち\n出力: クリック率\n\nとなるような CTR 予測モデルです。\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/f64784da-f8b9-4620-c0b5-5a3db5640417.png)\n\n候補となるニュースの特徴量をベクトルとして得るために、通常であれば単語の embedding を RNN や CNN でまとめ上げて固定長のベクトルに変換します。\nDKN ではここに Knowledge Graph Embedding によって得た特徴量を加えます。\n出現する単語ごとに Knowledge Graph 上のエンティティを探し、もし見つかったならそのエンティティ自体の embedding + 周辺のエンティティ embedding の平均を context vector として単語レベルの embedding に concat します。\nもしエンティティが見つからなければ 0 埋めでサイズをあわせます。\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/8e4f7cc4-89ee-5b3a-7343-245f33fa0b51.png)\n\n#### Attention-based User Interest Extraction\n\nつぎに、「ユーザが過去に見た記事」という情報をどのように活用するかを考えます。（＝ユーザの特徴をどのようにベクトル化するか）\n記事ごとのベクトルは ↑ で求められたので、単純に過去クリックした記事のベクトルの平均値を使うという方法が考えられますが、ユーザの興味は複数にまたがりうるので単純に平均を取るのは適しているとは言えません。\n（たとえば「プログラミング」「テニス」「ラーメン」の記事をクリックしたユーザに対して、「プログラミング」系の記事を推薦するのは多分良いはずだけど、平均をとってしまっているとその寄与が薄まる。）\n「今推薦の候補に考えている記事」について過去見た記事それぞれがどう関わっているかを表現できる方法でなければならないと言っています。\n\nそこで、 Attention Module をつかっています。\n候補の記事と過去に見た記事たちとの間の attention を計算し、過去に見た記事たちのベクトルの重み付き和をとることで、興味分野が複数にまたがっていても候補記事との関連をうまく見出したベクトルが作り出せます。\nAttention Module の入力は「候補記事のベクトル」と「過去に見た記事のベクトル」で、出力はその記事の寄与度になります。過去に見たすべての記事に対してそれぞれ network に入れて寄与度を計算し、softmax にかけたうで記事ベクトルの重み付き和をとっています。\n\n## 感想\n\nニュースタイトルは単語数が少なく固有名詞も多いので、単純な単語の embedding ではなかなか扱いづらいという問題を抱えていたので、 knowledge graph を使うというのはすごく納得の行く選択だなと思いました。\nただ、結果を見てみると Gain に対して複雑さや Knowledge Graph 自体を用意するコストが見合うかというとやはり厳しいかなという印象があります。（Microsoft はすでに自前の knowledge graph を持っているので...）\n\nKnowledge Graph Embedding については全く知らなかったのですが、面白い問題設定ですね。\n色々工夫されているようですが、 `h + r = t` というわかりやすく単純な方法でもそれなりに上手く行っていて面白かったです。\n\nまた、ユーザごとのベクトル表現の作り方の部分は Knowledge Graph の活用部分よりも簡単かつ一般的なので、この部分だけでも応用できそうだなと思いました。\n","contentMarkdown":"\nニュースの推薦に \"Knowledge Graph\" を活用する論文です。\nMicrosoft Research Asia のチームが WWW 2018 に投稿しています。\n\nこの記事は、Wantedly の勉強会で取り上げられた論文・技術をまとめたものです。\n[2018 年に読んだ機械学習系論文・技術まとめ at Wantedly Advent Calendar 2018 - Qiita](https://qiita.com/advent-calendar/2018/wantedly_ml)\n\n## Reference\n\n- DKN: Deep Knowledge-Aware Network for News Recommendation\n  - Hongwei Wang, Fuzheng Zhang, Xing Xie and Minyi Guo, et al., WWW 2018\n  - https://www2018.thewebconf.org/proceedings/#indus-922\n\n文中の図表は論文より引用しています。\n\n### 概要\n\n一般にニュース中の言葉は、常識的知識を仮定していて凝縮された文章になっています。\n一方で推薦系の既存手法は ニュース中に現れない知識を取り扱えておらず、潜在的なニュース間の関係を活かした探索が出来ていないという問題がありました。\n\nこの論文は、knowledge graph を活用した content-based な recommendation framework である _deep knowledge-aware network_ (DKN) を提案しています。\n\nknowledge graph とは、様々なエンティティを様々なエッジでつないだ heterogeneous なグラフで、たとえば \"モナリザ -[の作者」-\u003e ダ・ヴィンチ\" のような情報を溜め込んだ巨大なグラフです。\nGoogle Search の裏でも活躍しているらしく、一般的な「知識」を構造化された形で表現する方法としてよく使われています。\nknowledge graph を使うことで、 \"Donald Trump\" という単語そのものだけからはわからない、 \"United States\" という単語との関連、\"Politician\" という単語との関連などを導くことができます。\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/6967d5d1-0708-7e59-51c8-db107866ea63.png)\n\n### 何が難しいか\n\nこの論文では、ニュース推薦の難しさとして以下のようなものを挙げています。\n\n- news の推薦は movie などと違って、リアルタイム性が高い問題（いつ publish されたニュースかが重要）であり、news 間の関係性もすぐにダメになる。\n  - なので ID ベースの既存手法（協調フィルタリングなど）は効果が弱い\n- news はユーザによって興味範囲が違うし、ユーザは複数の興味範囲を持っていることがほとんど。\n- news の文言は凝縮されている。\n  - 常識、大量の既知の entity を仮定している。\n  - “Boris Johnson Has Warned Donald Trump To Stick To The Iran Nuclear Deal\" というニュースは “Boris Johnson”, “Donald Trump”, “Iran\", “Nuclear” を知っている前提で書かれている\n  - ↑ に興味のあるユーザはきっと “North Korean EMP Attack Would Cause Mass U.S. Starvation, Says Congressional Report” にも興味があるが、単語レベルでの関係性はほぼ無い\n\nこれらを Knowledge Graph を活用しつつ解決していきます。\n\n### Knowledge Graph Embedding\n\nKnowledge Graph Embedding 自体はこの研究の contribution ではありませんが重要なので簡単に紹介します。\nKnowledge Graph Embedding は、通常の network embedding に近い問題設定で、 knowledge graph における各エンティティとエッジの低次元な embedding を求めるという問題です。\n`(head, relation, trail)` という triplet 構造をなるべく維持したまま、`h, r, t` それぞれを低次元空間で表現することが目標です。\nDKN は translation-based knowledge graph embedding というのを使っています。\n一番簡単な手法は `h + r = t` になるように embedding を定める手法です。\n（他にもいくつか紹介、実験されているけど大体発想は同じなので省略。）\nまずランダムな値で各エンティティ、エッジの embedding を初期化し、 `h + r = t` を満たすように gradient descent で embedding を微調整していきます。\nこれによって、エンティティやエッジ（＝関係性）の低次元なベクトルを得ることができました。\n\n### Deep Knowledge-aware Network (DKN)\n\nDKN 自体は\n\n入力: 候補ニュースと、あるユーザの過去に見たニュースたち\n出力: クリック率\n\nとなるような CTR 予測モデルです。\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/f64784da-f8b9-4620-c0b5-5a3db5640417.png)\n\n候補となるニュースの特徴量をベクトルとして得るために、通常であれば単語の embedding を RNN や CNN でまとめ上げて固定長のベクトルに変換します。\nDKN ではここに Knowledge Graph Embedding によって得た特徴量を加えます。\n出現する単語ごとに Knowledge Graph 上のエンティティを探し、もし見つかったならそのエンティティ自体の embedding + 周辺のエンティティ embedding の平均を context vector として単語レベルの embedding に concat します。\nもしエンティティが見つからなければ 0 埋めでサイズをあわせます。\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/8e4f7cc4-89ee-5b3a-7343-245f33fa0b51.png)\n\n#### Attention-based User Interest Extraction\n\nつぎに、「ユーザが過去に見た記事」という情報をどのように活用するかを考えます。（＝ユーザの特徴をどのようにベクトル化するか）\n記事ごとのベクトルは ↑ で求められたので、単純に過去クリックした記事のベクトルの平均値を使うという方法が考えられますが、ユーザの興味は複数にまたがりうるので単純に平均を取るのは適しているとは言えません。\n（たとえば「プログラミング」「テニス」「ラーメン」の記事をクリックしたユーザに対して、「プログラミング」系の記事を推薦するのは多分良いはずだけど、平均をとってしまっているとその寄与が薄まる。）\n「今推薦の候補に考えている記事」について過去見た記事それぞれがどう関わっているかを表現できる方法でなければならないと言っています。\n\nそこで、 Attention Module をつかっています。\n候補の記事と過去に見た記事たちとの間の attention を計算し、過去に見た記事たちのベクトルの重み付き和をとることで、興味分野が複数にまたがっていても候補記事との関連をうまく見出したベクトルが作り出せます。\nAttention Module の入力は「候補記事のベクトル」と「過去に見た記事のベクトル」で、出力はその記事の寄与度になります。過去に見たすべての記事に対してそれぞれ network に入れて寄与度を計算し、softmax にかけたうで記事ベクトルの重み付き和をとっています。\n\n## 感想\n\nニュースタイトルは単語数が少なく固有名詞も多いので、単純な単語の embedding ではなかなか扱いづらいという問題を抱えていたので、 knowledge graph を使うというのはすごく納得の行く選択だなと思いました。\nただ、結果を見てみると Gain に対して複雑さや Knowledge Graph 自体を用意するコストが見合うかというとやはり厳しいかなという印象があります。（Microsoft はすでに自前の knowledge graph を持っているので...）\n\nKnowledge Graph Embedding については全く知らなかったのですが、面白い問題設定ですね。\n色々工夫されているようですが、 `h + r = t` というわかりやすく単純な方法でもそれなりに上手く行っていて面白かったです。\n\nまた、ユーザごとのベクトル表現の作り方の部分は Knowledge Graph の活用部分よりも簡単かつ一般的なので、この部分だけでも応用できそうだなと思いました。\n","slug":"DKN:_Deep_Knowledge-Aware_Network_for_News_Recommendation","title":"DKN: Deep Knowledge-Aware Network for News Recommendation","timestamp":1545220940000,"tags":["DeepLearning","WWW","Recommendation","論文読み"]},{"rawMarkdown":"---\ntitle: \"Object 間の関係を使って後処理 0 の物体検出を実現する: Relation Networks for Object Detection\"\ndate: 2018-12-10T23:35:40+09:00\ntags: [\"画像処理\", \"DeepLearning\", \"論文読み\", \"物体検出\"]\nurl: https://qiita.com/agatan/items/1c2cadeaabfc9f122f6f\n---\n\nObject Detection は、一枚の画像中の「どこに」「なにが」うつっているかを当てるタスクです。\n典型的な手法では、オブジェクトごとの bounding box を予測し、それぞれがどのクラスに分類されるかを**個別で**予測します。\nまた、ひとつのオブジェクトに対してすこしずつ座標のずれた box を複数予測してしまう可能性があるという問題があり、1 object 1 box になるように重複を削除しなければなりません。\nこれには nox maximum supression という方法を使うことが多いですが、これはヒューリスティックに基づく後処理になってしまっています。\n\nこの論文では、予測した box 間の関係に着目することで、分類精度を向上し、後処理 0 の完全 End-to-End での物体検出ネットワークを構築しています。\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/9b80d772-3b27-5b14-2eac-2d1c1275492e.png)\n\n（青い box について分類する際に、オレンジの box との関連が強く活用されている）\n\nこの記事は、Wantedly の勉強会で取り上げられた論文・技術をまとめたものです。\n[2018 年に読んだ機械学習系論文・技術まとめ at Wantedly Advent Calendar 2018 - Qiita](https://qiita.com/advent-calendar/2018/wantedly_ml)\n\n## Reference\n\n- Relation Networks for Object Detection\n  - Han Hu, Jiayuan Gu, Zheng Zhang, Jifeng Dai, Yichen Wei, et al., CVPR 2018\n- https://arxiv.org/abs/1711.11575\n\n文中の図表は論文より引用しています。\n\n## モチベーション\n\n自然言語処理の世界では、Attention module が非常に強力な武器として大活躍しており、特に Transformer[^1] 以降の SoTA モデルたちは大体 Attention の仕組みを組み込んでいるといっても良いくらいの活躍ぶりです。\n[^1]: https://arxiv.org/abs/1706.03762\n雑に言えば Attention は、ある entity と他の entity の関係性を 0~1 で出力し、その値をもとに entity を表す何らかのベクトルの加重和をとるといった操作をします。関係性を表す 0~1 を計算するためのパラメータも学習されます。\nまた画像に対応するタイトルを自動生成する Image Captioning の世界でも Attention は活躍しており、活用されるフィールドがどんどん増してきています。\n\nそこで、Attention を Object Detection の世界にもってこよう、というのがこの論文です。\nAttention module を用いて box 間の関係性を表し、object detection の精度向上を達成しています。\n\n## End-to-End\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/4c30de7e-8f64-60fb-0bd6-712c33a990df.png)\n\n概観でいうと Region Proposal network + RoI Pooling + relation module という構造を採用しています。\nrelation module は box 間の attention をとる module です。\n\n完全に後処理をなくすためには、大量の box のなかから採用すべき box だけを残す必要があります。（通常は non maximum supression で、スコアの高い box を優先的に残しつつ、重複した領域の大きい box はすてる）\nそこでこの論文では、Attention module をいくつか通したのち、box ごとに 0~1 の値を出力し、残すべき box は 1 になり捨てるべき box は 0 になるように学習します。\n\n## Object Relation Module\n\n物体間の関係には 2 つの意味があります。1 つは意味的な関連で、もう一つは座標的な関連です。\nボールっぽいものとバットっぽいものがあったとしても、ものすごく離れた場所にあるのであれば無関係かもしれないですが、近くにあればきっとボールとバットのペアと予測するのが正しそうです。\nしかし、通常の attention は、意味的な関連しか扱えていません。すべての box をなんらかの vector にしてしまっていますし、convolution や pooling は座標に依存しない操作なのでその vector に座標そのものの情報は埋め込まれません。\n\nそこでこの論文ではふつうの Attention をちょっといじった object relation module というものを提案しています。\n\n物体 m, n 間の attention を算出する式を見てみます。\n\n\u003cimg src=\"https://qiita-image-store.s3.amazonaws.com/0/39030/2e19515d-72ea-f065-e517-f8c7fe466fb8.png\" width=\"50%\"\u003e\n\n$\\omega_A^{mn}$ は通常の Attention と同じで、object m と n を表すベクトル（を線形変換したもの）の内積です。\n$\\omega_G$ の部分を無視すれば、この式は単に各 object ごとに内積をとったものを softmax にかけている = ふつうの Attention の計算式と一致します。\n\n$\\omega_G$ は、座標的な関係を考慮するためのパラメータです。\n\n\u003cimg src=\"https://qiita-image-store.s3.amazonaws.com/0/39030/9a24daf8-38cc-a765-af2f-7efa141cdad9.png\" width=\"50%\"\u003e\n\n$f_G^m$ は object m の座標情報 (x,y, w, h) を表します。\n$\\varepsilon_G$ はふたつの object の座標情報からそれらの座標的関係を計算する関数です。\n\n```math\n\\varepsilon(f_G^m, f_G^n) = (\\log(\\frac{|x_m - x_n|}{w_m}), \\log(\\frac{|y_m - y_n|}{h_m}), \\log(\\frac{w_n}{w_m}), \\log(\\frac{h_n}{h_m}))\n```\n\nさらに、 $max{0, ...}$ をとることで、ReLU 的な働きをし、まったく座標的に関係のない object からの影響を 0 にしています。\n\n## まとめと感想\n\n実験と結果はもと論文を読むのが一番くわしいのでそちらを参照ください。\n「一枚の画像から X を取得したいが、画像の主体となるような X だけを取りたい」といったケースにも応用できる手法かなと思っていて、実験してみたいと思いつつ、本当に end-to-end でやるのはちょっと大変そうすぎるという印象もあります。\n（また論文中には end-to-end が 0 から学習する際の問題にも触れられています。）\nとはいえ完全に end-to-end というのはやっぱり夢があって好きです。\n","contentMarkdown":"\nObject Detection は、一枚の画像中の「どこに」「なにが」うつっているかを当てるタスクです。\n典型的な手法では、オブジェクトごとの bounding box を予測し、それぞれがどのクラスに分類されるかを**個別で**予測します。\nまた、ひとつのオブジェクトに対してすこしずつ座標のずれた box を複数予測してしまう可能性があるという問題があり、1 object 1 box になるように重複を削除しなければなりません。\nこれには nox maximum supression という方法を使うことが多いですが、これはヒューリスティックに基づく後処理になってしまっています。\n\nこの論文では、予測した box 間の関係に着目することで、分類精度を向上し、後処理 0 の完全 End-to-End での物体検出ネットワークを構築しています。\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/9b80d772-3b27-5b14-2eac-2d1c1275492e.png)\n\n（青い box について分類する際に、オレンジの box との関連が強く活用されている）\n\nこの記事は、Wantedly の勉強会で取り上げられた論文・技術をまとめたものです。\n[2018 年に読んだ機械学習系論文・技術まとめ at Wantedly Advent Calendar 2018 - Qiita](https://qiita.com/advent-calendar/2018/wantedly_ml)\n\n## Reference\n\n- Relation Networks for Object Detection\n  - Han Hu, Jiayuan Gu, Zheng Zhang, Jifeng Dai, Yichen Wei, et al., CVPR 2018\n- https://arxiv.org/abs/1711.11575\n\n文中の図表は論文より引用しています。\n\n## モチベーション\n\n自然言語処理の世界では、Attention module が非常に強力な武器として大活躍しており、特に Transformer[^1] 以降の SoTA モデルたちは大体 Attention の仕組みを組み込んでいるといっても良いくらいの活躍ぶりです。\n[^1]: https://arxiv.org/abs/1706.03762\n雑に言えば Attention は、ある entity と他の entity の関係性を 0~1 で出力し、その値をもとに entity を表す何らかのベクトルの加重和をとるといった操作をします。関係性を表す 0~1 を計算するためのパラメータも学習されます。\nまた画像に対応するタイトルを自動生成する Image Captioning の世界でも Attention は活躍しており、活用されるフィールドがどんどん増してきています。\n\nそこで、Attention を Object Detection の世界にもってこよう、というのがこの論文です。\nAttention module を用いて box 間の関係性を表し、object detection の精度向上を達成しています。\n\n## End-to-End\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/4c30de7e-8f64-60fb-0bd6-712c33a990df.png)\n\n概観でいうと Region Proposal network + RoI Pooling + relation module という構造を採用しています。\nrelation module は box 間の attention をとる module です。\n\n完全に後処理をなくすためには、大量の box のなかから採用すべき box だけを残す必要があります。（通常は non maximum supression で、スコアの高い box を優先的に残しつつ、重複した領域の大きい box はすてる）\nそこでこの論文では、Attention module をいくつか通したのち、box ごとに 0~1 の値を出力し、残すべき box は 1 になり捨てるべき box は 0 になるように学習します。\n\n## Object Relation Module\n\n物体間の関係には 2 つの意味があります。1 つは意味的な関連で、もう一つは座標的な関連です。\nボールっぽいものとバットっぽいものがあったとしても、ものすごく離れた場所にあるのであれば無関係かもしれないですが、近くにあればきっとボールとバットのペアと予測するのが正しそうです。\nしかし、通常の attention は、意味的な関連しか扱えていません。すべての box をなんらかの vector にしてしまっていますし、convolution や pooling は座標に依存しない操作なのでその vector に座標そのものの情報は埋め込まれません。\n\nそこでこの論文ではふつうの Attention をちょっといじった object relation module というものを提案しています。\n\n物体 m, n 間の attention を算出する式を見てみます。\n\n\u003cimg src=\"https://qiita-image-store.s3.amazonaws.com/0/39030/2e19515d-72ea-f065-e517-f8c7fe466fb8.png\" width=\"50%\"\u003e\n\n$\\omega_A^{mn}$ は通常の Attention と同じで、object m と n を表すベクトル（を線形変換したもの）の内積です。\n$\\omega_G$ の部分を無視すれば、この式は単に各 object ごとに内積をとったものを softmax にかけている = ふつうの Attention の計算式と一致します。\n\n$\\omega_G$ は、座標的な関係を考慮するためのパラメータです。\n\n\u003cimg src=\"https://qiita-image-store.s3.amazonaws.com/0/39030/9a24daf8-38cc-a765-af2f-7efa141cdad9.png\" width=\"50%\"\u003e\n\n$f_G^m$ は object m の座標情報 (x,y, w, h) を表します。\n$\\varepsilon_G$ はふたつの object の座標情報からそれらの座標的関係を計算する関数です。\n\n```math\n\\varepsilon(f_G^m, f_G^n) = (\\log(\\frac{|x_m - x_n|}{w_m}), \\log(\\frac{|y_m - y_n|}{h_m}), \\log(\\frac{w_n}{w_m}), \\log(\\frac{h_n}{h_m}))\n```\n\nさらに、 $max{0, ...}$ をとることで、ReLU 的な働きをし、まったく座標的に関係のない object からの影響を 0 にしています。\n\n## まとめと感想\n\n実験と結果はもと論文を読むのが一番くわしいのでそちらを参照ください。\n「一枚の画像から X を取得したいが、画像の主体となるような X だけを取りたい」といったケースにも応用できる手法かなと思っていて、実験してみたいと思いつつ、本当に end-to-end でやるのはちょっと大変そうすぎるという印象もあります。\n（また論文中には end-to-end が 0 から学習する際の問題にも触れられています。）\nとはいえ完全に end-to-end というのはやっぱり夢があって好きです。\n","slug":"Object_間の関係を使って後処理_0_の物体検出を実現する:_Relation_Networks_for_Object_Detection","title":"Object 間の関係を使って後処理 0 の物体検出を実現する: Relation Networks for Object Detection","timestamp":1544452540000,"tags":["画像処理","DeepLearning","論文読み","物体検出"]},{"rawMarkdown":"---\ntitle: \"簡単な問題は省エネで解き、難しい問題には全力を出すネットワーク: Multi-Scale Dense Networks\"\ndate: 2018-12-07T22:34:37+09:00\ntags: [\"DeepLearning\", \"論文読み\"]\nurl: https://qiita.com/agatan/items/9ea14206bf7a3881ed6d\n---\n\nCNN による画像分類を現実のアプリケーションで使う際には、限られた計算資源で推論をする必要があります。\n推論を待って処理するような場合は latency が重要になり、バッチ処理でも throughput を最大化したいという要求があります。\n\n各タスクで SoTA を達成しているようなモデルはとても Deep であり、毎回走らせるには大きすぎます。\nとはいえ浅いネットワークでは精度に限界もあるので、速度と精度のトレードオフを常に考える必要があります。\n計算量を抑えつつ精度を向上するネットワークを設計する、という方向で MobileNet や SqueezeNet などが提案されています。\n\n今回紹介する論文は、ちょっと別のアプローチで計算資源の問題に立ち向かっています。\nひとことでまとめると **分類が十分に簡単だった場合は早期 exit し、難しいケースだけ深く計算する** という構造をとります。\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/763043e9-c89d-1521-011a-2ae6905eb2fe.png)\n\n（この図の \"easy\" と書かれている行の画像は省エネで、 \"hard\" と書かれている行の画像は全力で予測する）\n\nこの記事は、Wantedly の勉強会で取り上げられた論文・技術をまとめたものです。\n[2018 年に読んだ機械学習系論文・技術まとめ at Wantedly Advent Calendar 2018 - Qiita](https://qiita.com/advent-calendar/2018/wantedly_ml)\n\n### Reference\n\n- Multi-Scale Dense Networks for Resource Efficient Image Classification - Gao Huang, Danlu Chen, Tianhong Li, Felix Wu, Laurens van der Maaten, Kilian Q. Weinberger, et al., ICLR 2018\n- https://arxiv.org/abs/1703.09844\n\n文中の図表は論文より引用しています。\n\n### モチベーション\n\n先にも書きましたが、DNN は計算量の大きなアルゴリズムであり、実際に利用するケースを考えると、その速度や計算効率が気になってきます。\n現実の入力画像は様々な難易度のものがあるので、簡単な画像は浅いネットワークで解きたくなりますし、難しい画像は深いネットワークで解きたくなります。\n\nこう表現すると単純そうに見えますが、これを実現するためには「この画像は簡単か（浅いネットワークで解くべきか）、難しいか（深いネットワークで解くべきか）」を決定しなければなりません。\n実際に解く前に難易度を推定するのは難しく、事前に 2 つの model を定義しておく方法ではうまくいきません。\n\nMulti-Scale Dense Networks は、ひとつのモデルで逐次的に推論結果を出しつつ、十分に精度が出せそうであれば早期に Exit し、それ以降の計算を省略します。\n\n### アーキテクチャ\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/c6304ac6-1ccc-369b-0de0-2654fe5cef8d.png)\n↑ がモデルのイメージです。\n`classifier` と書かれた module が複数回出てきているように、それぞれが逐次的な推論結果を出す module になります。\n\nこのような構造を単純にとると以下の２点が問題になります。\n\n1. 粒度の大きい特徴を捉えづらい\n2. 浅い層の classifier が深い層の classifier の精度を下げてしまう\n\n#### 粒度の大きい特徴を捉えづらい\n\n典型的な画像分類のネットワークでは、浅い層で細かい粒度の特徴を獲得し、多くの Convolution や Pooling を経たあとの深い層で荒い粒度の特徴を獲得しています。\n浅い層で分類をやってしまうと解像度の大きい特徴を獲得することができていないため、エラー率が高くなってしまいます。\n次の図は、 ResNet や DenseNet の浅い層に分類器をつけて学習させた場合のエラー率をプロットしています。（損失関数は最終層の分類器の損失と浅い層の分類器の損失の和です。）\n\n\u003cimg src=\"https://qiita-image-store.s3.amazonaws.com/0/39030/f0488839-0129-0b45-74c4-159779694ade.png\" width=\"60%\"\u003e\n\n横軸は全体の深さに対して「どの深さに分類器をつけるか」を表しています。\n浅い層につけた分類器ほどエラー率が高くなっていることがわかります。\n\nreceptive field を大きく取った視野の広い特徴を効率的に獲得するためには feature map の解像度を下げる operation （2x2, stride 2 の convolution や pooling など）がよく使われますが、浅いうちにそれらの operation をやってしまうと、細かい粒度の特徴を獲得しづらくなってしまいます。\n\n##### 解決策\n\nMulti-Scale Dense Networks では、複数の解像度の feature map を各深さごとに用意するという解決方法をとっています。\nアーキテクチャの全体像の図で縦に 3 種類のスケールの feature map が並んでいます。\n各層では、以下の２つの operation の concatenate を分類器につなげます。\n\n1. 一つ前の層のもっとも解像度の低い feature map\n2. 一つ前の層の一段細かい feature map を畳み込んで解像度を荒くしたもの\n\nこれによって、細かい粒度の特徴の獲得を維持しつつ、視野の広い特徴を分類器に流すことを可能にしています。\n\n#### 浅い層の classifier が深い層の classifier の精度を下げてしまう\n\n\u003cimg src=\"https://qiita-image-store.s3.amazonaws.com/0/39030/9dd54945-ba86-d438-882b-9636fced2451.png\" width=\"60%\"\u003e\n\nこのグラフは ResNet や DenseNet の浅い層に分類器をつけたときの **最終層の分類器の精度** を表したものです。\n縦軸は最終層の分類器のみで学習したときの精度の相対精度です。\n特に ResNet で顕著ですが、浅い層に分類器をつけてしまうと **最終層の分類器の精度が悪くなる** ことがわかります。\n浅い層の分類器を最適化するために、細かい粒度の特徴が失われ、深い層にその特徴が伝わらないことが問題になっています。\n\n##### 解決策\n\n**Dense connectivity** によってこの問題を解決しています。\nDense connectiviy は DenseNet で提案されたもので、あるブロック内の中間層をすべて concatenate するブロックです。\nResNet で提案された Residual Module は、入力とそれを convolution などに通したものを足し合わせるというものでしたが、更にその考えを推し進めたのが DenseNet です。\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/d19184ee-8412-8705-e8a6-b92f1ab00108.png)\n\nこれによって、浅い層の結果がそのまま深い層に直結するため、一度細かい特徴を失っても浅い層の出力で recovery することができます。\n\n### まとめと感想\n\n僕らも Wantedly People というスマートフォンのカメラを使ったアプリケーションを提供しているので、モバイル上での推論をしたいというモチベーションがあって読んだ論文でした。\nDenseNet の特徴をきれいに活用していて面白い論文だなと思いました。\nDenseBlock の有効性をちゃんと検証していてこの論文の提案のいいところがわかりやすいのも好きなところです。\n","contentMarkdown":"\nCNN による画像分類を現実のアプリケーションで使う際には、限られた計算資源で推論をする必要があります。\n推論を待って処理するような場合は latency が重要になり、バッチ処理でも throughput を最大化したいという要求があります。\n\n各タスクで SoTA を達成しているようなモデルはとても Deep であり、毎回走らせるには大きすぎます。\nとはいえ浅いネットワークでは精度に限界もあるので、速度と精度のトレードオフを常に考える必要があります。\n計算量を抑えつつ精度を向上するネットワークを設計する、という方向で MobileNet や SqueezeNet などが提案されています。\n\n今回紹介する論文は、ちょっと別のアプローチで計算資源の問題に立ち向かっています。\nひとことでまとめると **分類が十分に簡単だった場合は早期 exit し、難しいケースだけ深く計算する** という構造をとります。\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/763043e9-c89d-1521-011a-2ae6905eb2fe.png)\n\n（この図の \"easy\" と書かれている行の画像は省エネで、 \"hard\" と書かれている行の画像は全力で予測する）\n\nこの記事は、Wantedly の勉強会で取り上げられた論文・技術をまとめたものです。\n[2018 年に読んだ機械学習系論文・技術まとめ at Wantedly Advent Calendar 2018 - Qiita](https://qiita.com/advent-calendar/2018/wantedly_ml)\n\n### Reference\n\n- Multi-Scale Dense Networks for Resource Efficient Image Classification - Gao Huang, Danlu Chen, Tianhong Li, Felix Wu, Laurens van der Maaten, Kilian Q. Weinberger, et al., ICLR 2018\n- https://arxiv.org/abs/1703.09844\n\n文中の図表は論文より引用しています。\n\n### モチベーション\n\n先にも書きましたが、DNN は計算量の大きなアルゴリズムであり、実際に利用するケースを考えると、その速度や計算効率が気になってきます。\n現実の入力画像は様々な難易度のものがあるので、簡単な画像は浅いネットワークで解きたくなりますし、難しい画像は深いネットワークで解きたくなります。\n\nこう表現すると単純そうに見えますが、これを実現するためには「この画像は簡単か（浅いネットワークで解くべきか）、難しいか（深いネットワークで解くべきか）」を決定しなければなりません。\n実際に解く前に難易度を推定するのは難しく、事前に 2 つの model を定義しておく方法ではうまくいきません。\n\nMulti-Scale Dense Networks は、ひとつのモデルで逐次的に推論結果を出しつつ、十分に精度が出せそうであれば早期に Exit し、それ以降の計算を省略します。\n\n### アーキテクチャ\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/c6304ac6-1ccc-369b-0de0-2654fe5cef8d.png)\n↑ がモデルのイメージです。\n`classifier` と書かれた module が複数回出てきているように、それぞれが逐次的な推論結果を出す module になります。\n\nこのような構造を単純にとると以下の２点が問題になります。\n\n1. 粒度の大きい特徴を捉えづらい\n2. 浅い層の classifier が深い層の classifier の精度を下げてしまう\n\n#### 粒度の大きい特徴を捉えづらい\n\n典型的な画像分類のネットワークでは、浅い層で細かい粒度の特徴を獲得し、多くの Convolution や Pooling を経たあとの深い層で荒い粒度の特徴を獲得しています。\n浅い層で分類をやってしまうと解像度の大きい特徴を獲得することができていないため、エラー率が高くなってしまいます。\n次の図は、 ResNet や DenseNet の浅い層に分類器をつけて学習させた場合のエラー率をプロットしています。（損失関数は最終層の分類器の損失と浅い層の分類器の損失の和です。）\n\n\u003cimg src=\"https://qiita-image-store.s3.amazonaws.com/0/39030/f0488839-0129-0b45-74c4-159779694ade.png\" width=\"60%\"\u003e\n\n横軸は全体の深さに対して「どの深さに分類器をつけるか」を表しています。\n浅い層につけた分類器ほどエラー率が高くなっていることがわかります。\n\nreceptive field を大きく取った視野の広い特徴を効率的に獲得するためには feature map の解像度を下げる operation （2x2, stride 2 の convolution や pooling など）がよく使われますが、浅いうちにそれらの operation をやってしまうと、細かい粒度の特徴を獲得しづらくなってしまいます。\n\n##### 解決策\n\nMulti-Scale Dense Networks では、複数の解像度の feature map を各深さごとに用意するという解決方法をとっています。\nアーキテクチャの全体像の図で縦に 3 種類のスケールの feature map が並んでいます。\n各層では、以下の２つの operation の concatenate を分類器につなげます。\n\n1. 一つ前の層のもっとも解像度の低い feature map\n2. 一つ前の層の一段細かい feature map を畳み込んで解像度を荒くしたもの\n\nこれによって、細かい粒度の特徴の獲得を維持しつつ、視野の広い特徴を分類器に流すことを可能にしています。\n\n#### 浅い層の classifier が深い層の classifier の精度を下げてしまう\n\n\u003cimg src=\"https://qiita-image-store.s3.amazonaws.com/0/39030/9dd54945-ba86-d438-882b-9636fced2451.png\" width=\"60%\"\u003e\n\nこのグラフは ResNet や DenseNet の浅い層に分類器をつけたときの **最終層の分類器の精度** を表したものです。\n縦軸は最終層の分類器のみで学習したときの精度の相対精度です。\n特に ResNet で顕著ですが、浅い層に分類器をつけてしまうと **最終層の分類器の精度が悪くなる** ことがわかります。\n浅い層の分類器を最適化するために、細かい粒度の特徴が失われ、深い層にその特徴が伝わらないことが問題になっています。\n\n##### 解決策\n\n**Dense connectivity** によってこの問題を解決しています。\nDense connectiviy は DenseNet で提案されたもので、あるブロック内の中間層をすべて concatenate するブロックです。\nResNet で提案された Residual Module は、入力とそれを convolution などに通したものを足し合わせるというものでしたが、更にその考えを推し進めたのが DenseNet です。\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/d19184ee-8412-8705-e8a6-b92f1ab00108.png)\n\nこれによって、浅い層の結果がそのまま深い層に直結するため、一度細かい特徴を失っても浅い層の出力で recovery することができます。\n\n### まとめと感想\n\n僕らも Wantedly People というスマートフォンのカメラを使ったアプリケーションを提供しているので、モバイル上での推論をしたいというモチベーションがあって読んだ論文でした。\nDenseNet の特徴をきれいに活用していて面白い論文だなと思いました。\nDenseBlock の有効性をちゃんと検証していてこの論文の提案のいいところがわかりやすいのも好きなところです。\n","slug":"簡単な問題は省エネで解き、難しい問題には全力を出すネットワーク:_Multi-Scale_Dense_Networks","title":"簡単な問題は省エネで解き、難しい問題には全力を出すネットワーク: Multi-Scale Dense Networks","timestamp":1544189677000,"tags":["DeepLearning","論文読み"]},{"rawMarkdown":"---\ntitle: \"[論文紹介] Focal Loss for Dense Object Detection\"\ndate: 2018-12-02T18:41:00+09:00\ntags: [\"DeepLearning\", \"論文読み\", \"ICCV\"]\nurl: https://qiita.com/agatan/items/53fe8d21f2147b0ac982\n---\n\n高速かつ高精度に物体検出を行う RetinaNet に使われている **Focal Loss** という損失関数を提案した論文を読んだので紹介します。\nFAIR(Facebook AI Research) が書いた論文で ICCV 2017 に採択されています。\n\n\u003cimg src=\"https://qiita-image-store.s3.amazonaws.com/0/39030/38ab1ef6-c4c5-fd78-d903-0954479143a6.png\" width=\"60%\"\u003e\n\nこの記事は、Wantedly の勉強会で取り上げられた論文・技術をまとめたものです。\n[2018 年に読んだ機械学習系論文・技術まとめ at Wantedly Advent Calendar 2018 - Qiita](https://qiita.com/advent-calendar/2018/wantedly_ml)\n\n#### Reference\n\n- Focal Loss for Dense Object Detection [Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, Piotr Dollár @ ICCV 2017]\n- https://arxiv.org/abs/1708.02002\n\n（文中の図表は論文より引用）\n\n### モチベーション\n\n精度の良い object detector の多くは R-CNN[^1] ベースの two-stage object detector の構成を取っています。\n[^1]: [Rich feature hierarchies for accurate object detection and semantic segmentation](https://arxiv.org/abs/1311.2524) [Ross Girshick, et al.]\nR-CNN は、 まず物体のある bounding box の候補集合を提案し、その後 2nd stage で提案された各 box について classification を行うという構成になっています。\ntwo-stage object detector は高い精度を記録していますが、一方で複雑さと推論速度に問題がありました。\n\nそこで、 YOLO[^2][^3] や SSD[^4][^5] のような one-stage で高速に物体検出を行うネットワークが提案されてきました。\n[^2]: [You Only Look Once: Unified, Real-Time Object Detection](https://arxiv.org/abs/1506.02640) [J. Redmon, et al.]\n[^3]: [YOLO9000: Better, Faster, Stronger](https://arxiv.org/abs/1612.08242) [J. Redmon, et al.]\n[^4]: [SSD: Single Shot MultiBox Detector](https://arxiv.org/abs/1512.02325) [W. Liu, et al.]\n[^5]: [DSSD : Deconvolutional Single Shot Detector](https://arxiv.org/abs/1701.06659) [C.-Y. Fu, et al.]\n\nしかし、これらの one-stage detector は高速な一方で（当時の） state-of-the-art methods と比べると精度は劣るという課題がありました。\n\nこの論文では one-stage detector が two-stage detector と並ぶ精度を出せないのは「クラス間の不均衡」が原因であるという仮説をたてています。\n画像のほとんどの pixel は background であり、foreground（背景以外のクラス）に属する pixel の数と比べると圧倒的な不均衡があるため、学習のほとんどが簡単な background 判定に支配されてしまいます。\n（two-stage の場合は 1st stage で注目すべき部分を限定しているため、background の多くは 1st stage でフィルタリングされ、2nd stage は不均衡が解決された状態で学習することができます。）\n\nそこで登場するのが **Focal Loss** です。\n\n### Focal Loss\n\nFocal Loss は通常の cross entropy loss (CE) を動的に scaling させる損失関数です。\n通常の CE と比較したのが次の図です。\n\n\u003cimg src=\"https://qiita-image-store.s3.amazonaws.com/0/39030/77bb8c08-0a47-d77a-fed7-17038e29a8cf.png\" width=\"60%\"\u003e\n\n通常の CE は以下のようなものです(binary cross entropy の場合）。\n\n```math\n{\\rm CE}(p_t) = -{\\rm log}(p_t).  \\\\\n\np_t = \\left\\{\n\\begin{array}{ll}\np \u0026 {\\rm if}\\: y = 1 \\\\\n1 - p \u0026 {\\rm otherwise.} \\\\\n\\end{array}\n\\right.\n```\n\nさきほどの図の `γ = 0` の青い曲線は通常の CE を `p` を x 軸にグラフにしたものです。\n図からわかるように、 0.6 といった十分に分類できている probability を出力したとしても、損失は無視できない値になっています。\nそのため、簡単に background と分類できていても大量の exmaple が積み重なって、 foreground の損失よりも強くなってしまいます。\n\nFocal Loss は、easy example （簡単に分類に成功している example）の損失を小さく scale します。\n\n```math\n{\\rm FL}(p_t) = -(1 - p_t) ^ \\gamma {\\rm log} (p_t).\n```\n\n`γ` はパラメータで、どのくらい easy example の損失を decay するかを決定します。\n簡単に分類に成功している example では\n\n```math\n(1 - p_t) ^ \\gamma\n```\n\nが小さい値になるため、損失への寄与が小さくなります。\nこれによって、より難しい focus すべき example が学習に強く寄与できるようになります。\n（論文中の実験では `γ = 2` を採用しています。）\n\nこの論文では RetinaNet というアーキテクチャを設計し Focal Loss を用いて学習させています。\nRetinaNet の設計の詳細は省きますが、精度を既存の object detector と比較したのが冒頭の図と次の表です。\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/d711797c-f160-08c3-97b8-027f34bbf373.png)\n\ntwo-stage detector と同等（以上）の精度を達成しています。\n\n### 感想\n\nアイディアがシンプルで、 object detection 以外のタスクに対しても応用可能な手法ですごく好きな論文です。\n実装も簡単なので試しやすく良い結果がでたので、何度かお世話になっています。\n\nちなみにこの論文のあとに書かれた YOLOv3[^6] では Focal Loss を採用していませんが、 RetinaNet と精度・速度を比較していて Focal Loss に関する考察も書かれています。\n\n\u003cimg src=\"https://qiita-image-store.s3.amazonaws.com/0/39030/d2caf698-0c58-c03d-4fa4-085e63ccb24c.png\" width=\"60%\"\u003e\n\n[^6]: [YOLOv3: An Incremental Improvement](https://arxiv.org/abs/1804.02767) [J. Redmon, et al.]\n\nまた、facebook が公開している [Detectron](https://github.com/facebookresearch/Detectron) にも RetineNet の実装が含まれているので簡単に利用することもできそうです。\n","contentMarkdown":"\n高速かつ高精度に物体検出を行う RetinaNet に使われている **Focal Loss** という損失関数を提案した論文を読んだので紹介します。\nFAIR(Facebook AI Research) が書いた論文で ICCV 2017 に採択されています。\n\n\u003cimg src=\"https://qiita-image-store.s3.amazonaws.com/0/39030/38ab1ef6-c4c5-fd78-d903-0954479143a6.png\" width=\"60%\"\u003e\n\nこの記事は、Wantedly の勉強会で取り上げられた論文・技術をまとめたものです。\n[2018 年に読んだ機械学習系論文・技術まとめ at Wantedly Advent Calendar 2018 - Qiita](https://qiita.com/advent-calendar/2018/wantedly_ml)\n\n#### Reference\n\n- Focal Loss for Dense Object Detection [Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, Piotr Dollár @ ICCV 2017]\n- https://arxiv.org/abs/1708.02002\n\n（文中の図表は論文より引用）\n\n### モチベーション\n\n精度の良い object detector の多くは R-CNN[^1] ベースの two-stage object detector の構成を取っています。\n[^1]: [Rich feature hierarchies for accurate object detection and semantic segmentation](https://arxiv.org/abs/1311.2524) [Ross Girshick, et al.]\nR-CNN は、 まず物体のある bounding box の候補集合を提案し、その後 2nd stage で提案された各 box について classification を行うという構成になっています。\ntwo-stage object detector は高い精度を記録していますが、一方で複雑さと推論速度に問題がありました。\n\nそこで、 YOLO[^2][^3] や SSD[^4][^5] のような one-stage で高速に物体検出を行うネットワークが提案されてきました。\n[^2]: [You Only Look Once: Unified, Real-Time Object Detection](https://arxiv.org/abs/1506.02640) [J. Redmon, et al.]\n[^3]: [YOLO9000: Better, Faster, Stronger](https://arxiv.org/abs/1612.08242) [J. Redmon, et al.]\n[^4]: [SSD: Single Shot MultiBox Detector](https://arxiv.org/abs/1512.02325) [W. Liu, et al.]\n[^5]: [DSSD : Deconvolutional Single Shot Detector](https://arxiv.org/abs/1701.06659) [C.-Y. Fu, et al.]\n\nしかし、これらの one-stage detector は高速な一方で（当時の） state-of-the-art methods と比べると精度は劣るという課題がありました。\n\nこの論文では one-stage detector が two-stage detector と並ぶ精度を出せないのは「クラス間の不均衡」が原因であるという仮説をたてています。\n画像のほとんどの pixel は background であり、foreground（背景以外のクラス）に属する pixel の数と比べると圧倒的な不均衡があるため、学習のほとんどが簡単な background 判定に支配されてしまいます。\n（two-stage の場合は 1st stage で注目すべき部分を限定しているため、background の多くは 1st stage でフィルタリングされ、2nd stage は不均衡が解決された状態で学習することができます。）\n\nそこで登場するのが **Focal Loss** です。\n\n### Focal Loss\n\nFocal Loss は通常の cross entropy loss (CE) を動的に scaling させる損失関数です。\n通常の CE と比較したのが次の図です。\n\n\u003cimg src=\"https://qiita-image-store.s3.amazonaws.com/0/39030/77bb8c08-0a47-d77a-fed7-17038e29a8cf.png\" width=\"60%\"\u003e\n\n通常の CE は以下のようなものです(binary cross entropy の場合）。\n\n```math\n{\\rm CE}(p_t) = -{\\rm log}(p_t).  \\\\\n\np_t = \\left\\{\n\\begin{array}{ll}\np \u0026 {\\rm if}\\: y = 1 \\\\\n1 - p \u0026 {\\rm otherwise.} \\\\\n\\end{array}\n\\right.\n```\n\nさきほどの図の `γ = 0` の青い曲線は通常の CE を `p` を x 軸にグラフにしたものです。\n図からわかるように、 0.6 といった十分に分類できている probability を出力したとしても、損失は無視できない値になっています。\nそのため、簡単に background と分類できていても大量の exmaple が積み重なって、 foreground の損失よりも強くなってしまいます。\n\nFocal Loss は、easy example （簡単に分類に成功している example）の損失を小さく scale します。\n\n```math\n{\\rm FL}(p_t) = -(1 - p_t) ^ \\gamma {\\rm log} (p_t).\n```\n\n`γ` はパラメータで、どのくらい easy example の損失を decay するかを決定します。\n簡単に分類に成功している example では\n\n```math\n(1 - p_t) ^ \\gamma\n```\n\nが小さい値になるため、損失への寄与が小さくなります。\nこれによって、より難しい focus すべき example が学習に強く寄与できるようになります。\n（論文中の実験では `γ = 2` を採用しています。）\n\nこの論文では RetinaNet というアーキテクチャを設計し Focal Loss を用いて学習させています。\nRetinaNet の設計の詳細は省きますが、精度を既存の object detector と比較したのが冒頭の図と次の表です。\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/d711797c-f160-08c3-97b8-027f34bbf373.png)\n\ntwo-stage detector と同等（以上）の精度を達成しています。\n\n### 感想\n\nアイディアがシンプルで、 object detection 以外のタスクに対しても応用可能な手法ですごく好きな論文です。\n実装も簡単なので試しやすく良い結果がでたので、何度かお世話になっています。\n\nちなみにこの論文のあとに書かれた YOLOv3[^6] では Focal Loss を採用していませんが、 RetinaNet と精度・速度を比較していて Focal Loss に関する考察も書かれています。\n\n\u003cimg src=\"https://qiita-image-store.s3.amazonaws.com/0/39030/d2caf698-0c58-c03d-4fa4-085e63ccb24c.png\" width=\"60%\"\u003e\n\n[^6]: [YOLOv3: An Incremental Improvement](https://arxiv.org/abs/1804.02767) [J. Redmon, et al.]\n\nまた、facebook が公開している [Detectron](https://github.com/facebookresearch/Detectron) にも RetineNet の実装が含まれているので簡単に利用することもできそうです。\n","slug":"[論文紹介]_Focal_Loss_for_Dense_Object_Detection","title":"[論文紹介] Focal Loss for Dense Object Detection","timestamp":1543743660000,"tags":["DeepLearning","論文読み","ICCV"]}]},"__N_SSG":true},"page":"/tags/[name]","query":{"name":"論文読み"},"buildId":"LTCqLRPEd5Qd2agqEQJZo","isFallback":false,"gsp":true,"scriptLoader":[]}</script></body></html>