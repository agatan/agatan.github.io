{"pageProps":{"tag":"画像処理","postMetas":[{"rawMarkdown":"---\ntitle: \"【論文読み】 Multi-Oriented Scene Text Detection via Corner Localization and Region Segmentation\"\ndate: 2019-02-13T00:12:22+09:00\ntags: [\"画像処理\", \"DeepLearning\", \"OCR\", \"論文読み\"]\nurl: https://qiita.com/agatan/items/afbdb922688984283775\n---\n\n画像中の文字領域検出における 2 つの主流な手法のいいとこ取りを目指した論文、 Multi-Oriented Scene Text Detection via Corner Localization and Region Segmentation を読んでみました。\n\nこの記事は、Wantedly の勉強会で取り上げられた論文・技術をまとめたものです。\n[2018 年に読んだ機械学習系論文・技術まとめ at Wantedly Advent Calendar 2018 - Qiita](https://qiita.com/advent-calendar/2018/wantedly_ml)\n\n## Reference\n\n- Multi-Oriented Scene Text Detection via Corner Localization and Region Segmentation [Pengyuan Lyu, Cong Yao, Wenhao Wu, Shuicheng Yan, Xiang Bai. CVPR 2018]\n- https://arxiv.org/abs/1802.08948\n\n（文中の図表は論文より引用しています）\n\n## Scene Text Detection\n\nScene Text Detection は、風景写真のなかにある文字領域（かんばん、ポスターなど）を検出するタスクです。\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/a78b72da-8550-641f-819b-ed024d9be7fa.png)\n(Figure 6.)\n\nCNN を活用した研究が進んでおり、現在では 2 つのアプローチが主流となっています。（このあたりは [【論文読み】Semi-convolutional Operators for Instance Segmentation](https://qiita.com/agatan/items/2cf1209b7370db45eba5) や [[論文紹介] Focal Loss for Dense Object Detection](https://qiita.com/agatan/items/53fe8d21f2147b0ac982) でもすこし触れています）\n\n1 つ目は、文字領域検出を、一般的な物体検知(Object Detection)の特殊系とみなして解く手法です。\n物体検知に対するアプローチとして主流なのは bounding box の座標を regression として解くというものです。\nこの場合、bounding box 形式で当てに行くので、歪んだ形状への対応が難しく、縦横比が大きく偏った文字領域に弱いといった問題があります。\n[EAST: An Efficient and Accurate Scene Text Detector](https://arxiv.org/abs/1704.03155) [X. Zhou et al., CVPR 2018] はこちらのアプローチを採用しています。\n\n2 つ目は、Instance Segmentation として解くアプローチです。\nピクセル単位で文字領域かどうかの 2 クラス分類 + なんらかの方法でインスタンスの分離を行うという方法ですが、インスタンスの分離には複雑な後処理を要するケースが多く、複雑さや実行時間に問題があります。\n[PixelLink: Detecting Scene Text via Instance Segmentation](https://arxiv.org/abs/1801.01315) [D. Deng et al., AAAI 2018] が代表例です。（論文中で refer されているのは Multi-oriented Text Detection with Fully Convolutional Networks [Z. Zhang et al., CVPR 2016])\n\nこの論文ではこれらの 2 つの手法をいいとこ取りした Scene Text Detector を提案しています。\n\n## Network Architecture\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/281d3451-0e23-c20b-bb18-88b46495da15.png)\n\n全体像は ↑ の図のようになっています。\n**Corner Detection** と **Position Sensitive Segmentation** の 2 つからなる architecture です。\n\nCorner Detection はその名の通り、文字領域の角の位置を予測します。ただし、「角である」ことだけを考慮し、「どの 4 つの組み合わせが 1 領域を表しているのか」は考えません。\nCorner Detection が予測した大量の「角」たちを sampling & grouping し、大量の「文字領域候補」をつくります。\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/d62ec4c7-2e8b-56e9-045c-af825dfac279.png)\n（Corner Detection は概念的にはわかりやすいですが、実際には default box を用意して offset 計算して...と、SSD や YOLO と同程度には複雑なことをしています。詳細は論文をご参照ください...）\n\nCorner Detection と並行して、Position Sensitive Segmentation 側では、各ピクセルを「文字領域の右上」「右下」「左上」「左下」の 4 クラス（+ 背景）に分類します。\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/f8a4b98a-a428-48be-2d97-f57246f53a5a.png)\nそれぞれ、白が「左上」、赤が「右上」、青が「左下」、緑が「右下」に分類された領域です。\n\nさいごに、Corner Detection によって生成された大量の「文字領域候補」を、Position Sensitive Segmentation の結果との整合性に応じてスコアづけします。\n「文字領域候補」の左上にあるピクセルが Segmentation によって「左上」に分類されていればいるほど高いスコアになります。\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/aad5fe04-8bad-c758-fa7a-fd037c62f877.png)\n\n## 感想\n\nInstance Segmentation 方式は後処理が複雑すぎる、という問題提起のわりには、提案手法の後処理も相当大変そうという印象があります。\nまた、Corner Detection 部分はやけに複雑で、なぜこんなに複雑なことをしているのかあんまり理解できませんでした。\n一方、Object Detection 系のやり方と Segmentation 系のやり方を組み合わせる手法としては概念的にもわかりやすい構成で面白かったです。\nPosition Sensitive Segmentation というアプローチもこの論文を読むまで知らなかったので勉強になりました。\n\nこの論文の少しあとに ECCV 2018 に通った論文で関連していそうなものとして、CornerNet と PixelLink という論文があります。\n[CornerNet: Detecting Objects as Paired Keypoints](https://arxiv.org/abs/1808.01244) は、 Corner を予測 + ピクセル単位の Embedding を計算 → Embedding の距離に応じて Corner のペアを作っていくという手法で、よりシンプルに Corner のグルーピングを実現しています。\nまた、[PixelLink: Detecting Scene Text via Instance Segmentation](https://arxiv.org/abs/1801.01315) は text/non-text の segmentation + 隣接ピクセルと連結するかしないかの 2 クラス分類を組み合わせて Instance Segmentation をし、文字領域検出を行っています。\nどちらもとてもおもしろい論文なのでおすすめです。\n","contentMarkdown":"\n画像中の文字領域検出における 2 つの主流な手法のいいとこ取りを目指した論文、 Multi-Oriented Scene Text Detection via Corner Localization and Region Segmentation を読んでみました。\n\nこの記事は、Wantedly の勉強会で取り上げられた論文・技術をまとめたものです。\n[2018 年に読んだ機械学習系論文・技術まとめ at Wantedly Advent Calendar 2018 - Qiita](https://qiita.com/advent-calendar/2018/wantedly_ml)\n\n## Reference\n\n- Multi-Oriented Scene Text Detection via Corner Localization and Region Segmentation [Pengyuan Lyu, Cong Yao, Wenhao Wu, Shuicheng Yan, Xiang Bai. CVPR 2018]\n- https://arxiv.org/abs/1802.08948\n\n（文中の図表は論文より引用しています）\n\n## Scene Text Detection\n\nScene Text Detection は、風景写真のなかにある文字領域（かんばん、ポスターなど）を検出するタスクです。\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/a78b72da-8550-641f-819b-ed024d9be7fa.png)\n(Figure 6.)\n\nCNN を活用した研究が進んでおり、現在では 2 つのアプローチが主流となっています。（このあたりは [【論文読み】Semi-convolutional Operators for Instance Segmentation](https://qiita.com/agatan/items/2cf1209b7370db45eba5) や [[論文紹介] Focal Loss for Dense Object Detection](https://qiita.com/agatan/items/53fe8d21f2147b0ac982) でもすこし触れています）\n\n1 つ目は、文字領域検出を、一般的な物体検知(Object Detection)の特殊系とみなして解く手法です。\n物体検知に対するアプローチとして主流なのは bounding box の座標を regression として解くというものです。\nこの場合、bounding box 形式で当てに行くので、歪んだ形状への対応が難しく、縦横比が大きく偏った文字領域に弱いといった問題があります。\n[EAST: An Efficient and Accurate Scene Text Detector](https://arxiv.org/abs/1704.03155) [X. Zhou et al., CVPR 2018] はこちらのアプローチを採用しています。\n\n2 つ目は、Instance Segmentation として解くアプローチです。\nピクセル単位で文字領域かどうかの 2 クラス分類 + なんらかの方法でインスタンスの分離を行うという方法ですが、インスタンスの分離には複雑な後処理を要するケースが多く、複雑さや実行時間に問題があります。\n[PixelLink: Detecting Scene Text via Instance Segmentation](https://arxiv.org/abs/1801.01315) [D. Deng et al., AAAI 2018] が代表例です。（論文中で refer されているのは Multi-oriented Text Detection with Fully Convolutional Networks [Z. Zhang et al., CVPR 2016])\n\nこの論文ではこれらの 2 つの手法をいいとこ取りした Scene Text Detector を提案しています。\n\n## Network Architecture\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/281d3451-0e23-c20b-bb18-88b46495da15.png)\n\n全体像は ↑ の図のようになっています。\n**Corner Detection** と **Position Sensitive Segmentation** の 2 つからなる architecture です。\n\nCorner Detection はその名の通り、文字領域の角の位置を予測します。ただし、「角である」ことだけを考慮し、「どの 4 つの組み合わせが 1 領域を表しているのか」は考えません。\nCorner Detection が予測した大量の「角」たちを sampling & grouping し、大量の「文字領域候補」をつくります。\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/d62ec4c7-2e8b-56e9-045c-af825dfac279.png)\n（Corner Detection は概念的にはわかりやすいですが、実際には default box を用意して offset 計算して...と、SSD や YOLO と同程度には複雑なことをしています。詳細は論文をご参照ください...）\n\nCorner Detection と並行して、Position Sensitive Segmentation 側では、各ピクセルを「文字領域の右上」「右下」「左上」「左下」の 4 クラス（+ 背景）に分類します。\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/f8a4b98a-a428-48be-2d97-f57246f53a5a.png)\nそれぞれ、白が「左上」、赤が「右上」、青が「左下」、緑が「右下」に分類された領域です。\n\nさいごに、Corner Detection によって生成された大量の「文字領域候補」を、Position Sensitive Segmentation の結果との整合性に応じてスコアづけします。\n「文字領域候補」の左上にあるピクセルが Segmentation によって「左上」に分類されていればいるほど高いスコアになります。\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/aad5fe04-8bad-c758-fa7a-fd037c62f877.png)\n\n## 感想\n\nInstance Segmentation 方式は後処理が複雑すぎる、という問題提起のわりには、提案手法の後処理も相当大変そうという印象があります。\nまた、Corner Detection 部分はやけに複雑で、なぜこんなに複雑なことをしているのかあんまり理解できませんでした。\n一方、Object Detection 系のやり方と Segmentation 系のやり方を組み合わせる手法としては概念的にもわかりやすい構成で面白かったです。\nPosition Sensitive Segmentation というアプローチもこの論文を読むまで知らなかったので勉強になりました。\n\nこの論文の少しあとに ECCV 2018 に通った論文で関連していそうなものとして、CornerNet と PixelLink という論文があります。\n[CornerNet: Detecting Objects as Paired Keypoints](https://arxiv.org/abs/1808.01244) は、 Corner を予測 + ピクセル単位の Embedding を計算 → Embedding の距離に応じて Corner のペアを作っていくという手法で、よりシンプルに Corner のグルーピングを実現しています。\nまた、[PixelLink: Detecting Scene Text via Instance Segmentation](https://arxiv.org/abs/1801.01315) は text/non-text の segmentation + 隣接ピクセルと連結するかしないかの 2 クラス分類を組み合わせて Instance Segmentation をし、文字領域検出を行っています。\nどちらもとてもおもしろい論文なのでおすすめです。\n","slug":"【論文読み】_Multi-Oriented_Scene_Text_Detection_via_Corner_Localization_and_Region_Segmentation","title":"【論文読み】 Multi-Oriented Scene Text Detection via Corner Localization and Region Segmentation","timestamp":1549984342000,"tags":["画像処理","DeepLearning","OCR","論文読み"]},{"rawMarkdown":"---\ntitle: \"ILSVRC 2017 画像分類 Top の手法 Squeeze-and-Excitation Networks\"\ndate: 2018-12-13T22:25:45+09:00\ntags: [\"画像処理\", \"ComputerVision\", \"MachineLearning\", \"DeepLearning\", \"CNN\"]\nurl: https://qiita.com/agatan/items/8cf2566908228eaa5450\n---\n\nILSVRC 2017 の画像分類タスクでは Squeeze-and-Excitation という手法が 1 位を記録しました。\nシンプルなアイディア・実装で、既存モデルの拡張にも利用できるうえ、精度も 2016 年の top 1 と比べてエラー率を約 25% 減らすという大きな成果をあげています。\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/20b939b8-c65b-ce06-4525-37ccfe19c7a2.png)\n\n### Reference\n\n- Jie Hu, et al., https://arxiv.org/pdf/1709.01507.pdf\n\n文中の図表は論文から引用しています。\n\nこの記事は、Wantedly の勉強会で取り上げられた論文・技術をまとめたものです。\n[2018 年に読んだ機械学習系論文・技術まとめ at Wantedly Advent Calendar 2018 - Qiita](https://qiita.com/advent-calendar/2018/wantedly_ml)\n\n## Squeeze and Excitation\n\nこの論文では、SE block というブロックを提案しています。\nSE block は特定のネットワーク全体の設計の提案ではなく、ネットワーク中の 1 component として振る舞うものです。\nSE block を既存のいろいろなネットワークやモジュール（ResNet、Inception、...）に組み込むことで精度が向上することを実験で確かめています。\n\nSE block は非常に実装が簡単なので先に実装を見てしまったほうがわかりやすいかもしれません。\n\n```python\ndef se_block(input, channels):\n    \"\"\"\n    Args:\n        input: (N, H, W, C)\n        channels: C\n    Returns:\n        tensor: (N, H, W, C)\n    \"\"\"\n    # Squeeze\n    x = GlobalAveragePooling2D()(input)  # (N, C)\n    # Excitation\n    x = Dense(channels // 16, activation='relu')(x)\n    x = Dense(channels, activation='sigmoid')(x) # (N, C)\n    return Multiply()([input, x])\n```\n\nSE block は、通常の convolution の出力をそのまま使うのではなく、 **各 channel の出力を重み付けして使う** ようにすることで、チャンネル間の関係性の学習を可能にするブロックです。\n\n### Squeeze\n\n既存のモデルは、convolution と activation を重ねることで、局所的な特徴を獲得していきます。\n層が深くなったり pooling したりすると、局所的といいつつも広い視野を持っていくことになりますが、視野を一歩こえた先の情報などはまったく考慮できず、画像全体におけるチャンネル間の関係性を表すことはできません。\nそこで、画像全体の特徴を活用するために、 global average pooling を利用します。（Spatial Squeeze）\n\n### Excitation\n\nそうして得た「画像全体のチャンネルの状況」をいくつかの layer に通したのち、sigmoid に通します。\n最後にブロックに入力されてきた値 `input` に、 sigmoid 関数を通して 0~1 の範囲に収めた「各チャンネルの重み」を掛けて出力しています。\nこの部分が Excitation とよばれる部分です。\n\n## 既存モデルへの組み込み\n\nSE block は既存のモデルへの組み込みが容易であることも大きな強みです。\nいくつかの組み込み方が提案・実験されています。\n\n<table>\n<tr>\n<td>\n<img src=\"https://qiita-image-store.s3.amazonaws.com/0/39030/76082379-9db4-4f02-7236-355f6804908b.png\">\n</td>\n<td>\n<img src=\"https://qiita-image-store.s3.amazonaws.com/0/39030/4c3fa967-234c-57ee-36a9-1c15fca7c603.png\">\n</td>\n</tr>\n</table>\n\n組み込み方もシンプルで簡単に試せるのですばらしいですね。\n\n## まとめと感想\n\nかなりいろんなセットアップで実験をしているので、詳細は論文を参照ください。\nWantedly People で使われているモデルにも実験的に組みこんでみたところ、確かに数%の改善が確認できました。\nこの論文の続編的なものとして、segmentation タスクなどの fully convolutional networks 用の SE block 亜種が提案されています。\nこの advent calendar のどこかでそちらの紹介もできればと思います。\n","contentMarkdown":"\nILSVRC 2017 の画像分類タスクでは Squeeze-and-Excitation という手法が 1 位を記録しました。\nシンプルなアイディア・実装で、既存モデルの拡張にも利用できるうえ、精度も 2016 年の top 1 と比べてエラー率を約 25% 減らすという大きな成果をあげています。\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/20b939b8-c65b-ce06-4525-37ccfe19c7a2.png)\n\n### Reference\n\n- Jie Hu, et al., https://arxiv.org/pdf/1709.01507.pdf\n\n文中の図表は論文から引用しています。\n\nこの記事は、Wantedly の勉強会で取り上げられた論文・技術をまとめたものです。\n[2018 年に読んだ機械学習系論文・技術まとめ at Wantedly Advent Calendar 2018 - Qiita](https://qiita.com/advent-calendar/2018/wantedly_ml)\n\n## Squeeze and Excitation\n\nこの論文では、SE block というブロックを提案しています。\nSE block は特定のネットワーク全体の設計の提案ではなく、ネットワーク中の 1 component として振る舞うものです。\nSE block を既存のいろいろなネットワークやモジュール（ResNet、Inception、...）に組み込むことで精度が向上することを実験で確かめています。\n\nSE block は非常に実装が簡単なので先に実装を見てしまったほうがわかりやすいかもしれません。\n\n```python\ndef se_block(input, channels):\n    \"\"\"\n    Args:\n        input: (N, H, W, C)\n        channels: C\n    Returns:\n        tensor: (N, H, W, C)\n    \"\"\"\n    # Squeeze\n    x = GlobalAveragePooling2D()(input)  # (N, C)\n    # Excitation\n    x = Dense(channels // 16, activation='relu')(x)\n    x = Dense(channels, activation='sigmoid')(x) # (N, C)\n    return Multiply()([input, x])\n```\n\nSE block は、通常の convolution の出力をそのまま使うのではなく、 **各 channel の出力を重み付けして使う** ようにすることで、チャンネル間の関係性の学習を可能にするブロックです。\n\n### Squeeze\n\n既存のモデルは、convolution と activation を重ねることで、局所的な特徴を獲得していきます。\n層が深くなったり pooling したりすると、局所的といいつつも広い視野を持っていくことになりますが、視野を一歩こえた先の情報などはまったく考慮できず、画像全体におけるチャンネル間の関係性を表すことはできません。\nそこで、画像全体の特徴を活用するために、 global average pooling を利用します。（Spatial Squeeze）\n\n### Excitation\n\nそうして得た「画像全体のチャンネルの状況」をいくつかの layer に通したのち、sigmoid に通します。\n最後にブロックに入力されてきた値 `input` に、 sigmoid 関数を通して 0~1 の範囲に収めた「各チャンネルの重み」を掛けて出力しています。\nこの部分が Excitation とよばれる部分です。\n\n## 既存モデルへの組み込み\n\nSE block は既存のモデルへの組み込みが容易であることも大きな強みです。\nいくつかの組み込み方が提案・実験されています。\n\n<table>\n<tr>\n<td>\n<img src=\"https://qiita-image-store.s3.amazonaws.com/0/39030/76082379-9db4-4f02-7236-355f6804908b.png\">\n</td>\n<td>\n<img src=\"https://qiita-image-store.s3.amazonaws.com/0/39030/4c3fa967-234c-57ee-36a9-1c15fca7c603.png\">\n</td>\n</tr>\n</table>\n\n組み込み方もシンプルで簡単に試せるのですばらしいですね。\n\n## まとめと感想\n\nかなりいろんなセットアップで実験をしているので、詳細は論文を参照ください。\nWantedly People で使われているモデルにも実験的に組みこんでみたところ、確かに数%の改善が確認できました。\nこの論文の続編的なものとして、segmentation タスクなどの fully convolutional networks 用の SE block 亜種が提案されています。\nこの advent calendar のどこかでそちらの紹介もできればと思います。\n","slug":"ILSVRC_2017_画像分類_Top_の手法_Squeeze-and-Excitation_Networks","title":"ILSVRC 2017 画像分類 Top の手法 Squeeze-and-Excitation Networks","timestamp":1544707545000,"tags":["画像処理","ComputerVision","MachineLearning","DeepLearning","CNN"]},{"rawMarkdown":"---\ntitle: \"Object 間の関係を使って後処理 0 の物体検出を実現する: Relation Networks for Object Detection\"\ndate: 2018-12-10T23:35:40+09:00\ntags: [\"画像処理\", \"DeepLearning\", \"論文読み\", \"物体検出\"]\nurl: https://qiita.com/agatan/items/1c2cadeaabfc9f122f6f\n---\n\nObject Detection は、一枚の画像中の「どこに」「なにが」うつっているかを当てるタスクです。\n典型的な手法では、オブジェクトごとの bounding box を予測し、それぞれがどのクラスに分類されるかを**個別で**予測します。\nまた、ひとつのオブジェクトに対してすこしずつ座標のずれた box を複数予測してしまう可能性があるという問題があり、1 object 1 box になるように重複を削除しなければなりません。\nこれには nox maximum supression という方法を使うことが多いですが、これはヒューリスティックに基づく後処理になってしまっています。\n\nこの論文では、予測した box 間の関係に着目することで、分類精度を向上し、後処理 0 の完全 End-to-End での物体検出ネットワークを構築しています。\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/9b80d772-3b27-5b14-2eac-2d1c1275492e.png)\n\n（青い box について分類する際に、オレンジの box との関連が強く活用されている）\n\nこの記事は、Wantedly の勉強会で取り上げられた論文・技術をまとめたものです。\n[2018 年に読んだ機械学習系論文・技術まとめ at Wantedly Advent Calendar 2018 - Qiita](https://qiita.com/advent-calendar/2018/wantedly_ml)\n\n## Reference\n\n- Relation Networks for Object Detection\n  - Han Hu, Jiayuan Gu, Zheng Zhang, Jifeng Dai, Yichen Wei, et al., CVPR 2018\n- https://arxiv.org/abs/1711.11575\n\n文中の図表は論文より引用しています。\n\n## モチベーション\n\n自然言語処理の世界では、Attention module が非常に強力な武器として大活躍しており、特に Transformer[^1] 以降の SoTA モデルたちは大体 Attention の仕組みを組み込んでいるといっても良いくらいの活躍ぶりです。\n[^1]: https://arxiv.org/abs/1706.03762\n雑に言えば Attention は、ある entity と他の entity の関係性を 0~1 で出力し、その値をもとに entity を表す何らかのベクトルの加重和をとるといった操作をします。関係性を表す 0~1 を計算するためのパラメータも学習されます。\nまた画像に対応するタイトルを自動生成する Image Captioning の世界でも Attention は活躍しており、活用されるフィールドがどんどん増してきています。\n\nそこで、Attention を Object Detection の世界にもってこよう、というのがこの論文です。\nAttention module を用いて box 間の関係性を表し、object detection の精度向上を達成しています。\n\n## End-to-End\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/4c30de7e-8f64-60fb-0bd6-712c33a990df.png)\n\n概観でいうと Region Proposal network + RoI Pooling + relation module という構造を採用しています。\nrelation module は box 間の attention をとる module です。\n\n完全に後処理をなくすためには、大量の box のなかから採用すべき box だけを残す必要があります。（通常は non maximum supression で、スコアの高い box を優先的に残しつつ、重複した領域の大きい box はすてる）\nそこでこの論文では、Attention module をいくつか通したのち、box ごとに 0~1 の値を出力し、残すべき box は 1 になり捨てるべき box は 0 になるように学習します。\n\n## Object Relation Module\n\n物体間の関係には 2 つの意味があります。1 つは意味的な関連で、もう一つは座標的な関連です。\nボールっぽいものとバットっぽいものがあったとしても、ものすごく離れた場所にあるのであれば無関係かもしれないですが、近くにあればきっとボールとバットのペアと予測するのが正しそうです。\nしかし、通常の attention は、意味的な関連しか扱えていません。すべての box をなんらかの vector にしてしまっていますし、convolution や pooling は座標に依存しない操作なのでその vector に座標そのものの情報は埋め込まれません。\n\nそこでこの論文ではふつうの Attention をちょっといじった object relation module というものを提案しています。\n\n物体 m, n 間の attention を算出する式を見てみます。\n\n<img src=\"https://qiita-image-store.s3.amazonaws.com/0/39030/2e19515d-72ea-f065-e517-f8c7fe466fb8.png\" width=\"50%\">\n\n$\\omega_A^{mn}$ は通常の Attention と同じで、object m と n を表すベクトル（を線形変換したもの）の内積です。\n$\\omega_G$ の部分を無視すれば、この式は単に各 object ごとに内積をとったものを softmax にかけている = ふつうの Attention の計算式と一致します。\n\n$\\omega_G$ は、座標的な関係を考慮するためのパラメータです。\n\n<img src=\"https://qiita-image-store.s3.amazonaws.com/0/39030/9a24daf8-38cc-a765-af2f-7efa141cdad9.png\" width=\"50%\">\n\n$f_G^m$ は object m の座標情報 (x,y, w, h) を表します。\n$\\varepsilon_G$ はふたつの object の座標情報からそれらの座標的関係を計算する関数です。\n\n```math\n\\varepsilon(f_G^m, f_G^n) = (\\log(\\frac{|x_m - x_n|}{w_m}), \\log(\\frac{|y_m - y_n|}{h_m}), \\log(\\frac{w_n}{w_m}), \\log(\\frac{h_n}{h_m}))\n```\n\nさらに、 $max{0, ...}$ をとることで、ReLU 的な働きをし、まったく座標的に関係のない object からの影響を 0 にしています。\n\n## まとめと感想\n\n実験と結果はもと論文を読むのが一番くわしいのでそちらを参照ください。\n「一枚の画像から X を取得したいが、画像の主体となるような X だけを取りたい」といったケースにも応用できる手法かなと思っていて、実験してみたいと思いつつ、本当に end-to-end でやるのはちょっと大変そうすぎるという印象もあります。\n（また論文中には end-to-end が 0 から学習する際の問題にも触れられています。）\nとはいえ完全に end-to-end というのはやっぱり夢があって好きです。\n","contentMarkdown":"\nObject Detection は、一枚の画像中の「どこに」「なにが」うつっているかを当てるタスクです。\n典型的な手法では、オブジェクトごとの bounding box を予測し、それぞれがどのクラスに分類されるかを**個別で**予測します。\nまた、ひとつのオブジェクトに対してすこしずつ座標のずれた box を複数予測してしまう可能性があるという問題があり、1 object 1 box になるように重複を削除しなければなりません。\nこれには nox maximum supression という方法を使うことが多いですが、これはヒューリスティックに基づく後処理になってしまっています。\n\nこの論文では、予測した box 間の関係に着目することで、分類精度を向上し、後処理 0 の完全 End-to-End での物体検出ネットワークを構築しています。\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/9b80d772-3b27-5b14-2eac-2d1c1275492e.png)\n\n（青い box について分類する際に、オレンジの box との関連が強く活用されている）\n\nこの記事は、Wantedly の勉強会で取り上げられた論文・技術をまとめたものです。\n[2018 年に読んだ機械学習系論文・技術まとめ at Wantedly Advent Calendar 2018 - Qiita](https://qiita.com/advent-calendar/2018/wantedly_ml)\n\n## Reference\n\n- Relation Networks for Object Detection\n  - Han Hu, Jiayuan Gu, Zheng Zhang, Jifeng Dai, Yichen Wei, et al., CVPR 2018\n- https://arxiv.org/abs/1711.11575\n\n文中の図表は論文より引用しています。\n\n## モチベーション\n\n自然言語処理の世界では、Attention module が非常に強力な武器として大活躍しており、特に Transformer[^1] 以降の SoTA モデルたちは大体 Attention の仕組みを組み込んでいるといっても良いくらいの活躍ぶりです。\n[^1]: https://arxiv.org/abs/1706.03762\n雑に言えば Attention は、ある entity と他の entity の関係性を 0~1 で出力し、その値をもとに entity を表す何らかのベクトルの加重和をとるといった操作をします。関係性を表す 0~1 を計算するためのパラメータも学習されます。\nまた画像に対応するタイトルを自動生成する Image Captioning の世界でも Attention は活躍しており、活用されるフィールドがどんどん増してきています。\n\nそこで、Attention を Object Detection の世界にもってこよう、というのがこの論文です。\nAttention module を用いて box 間の関係性を表し、object detection の精度向上を達成しています。\n\n## End-to-End\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/4c30de7e-8f64-60fb-0bd6-712c33a990df.png)\n\n概観でいうと Region Proposal network + RoI Pooling + relation module という構造を採用しています。\nrelation module は box 間の attention をとる module です。\n\n完全に後処理をなくすためには、大量の box のなかから採用すべき box だけを残す必要があります。（通常は non maximum supression で、スコアの高い box を優先的に残しつつ、重複した領域の大きい box はすてる）\nそこでこの論文では、Attention module をいくつか通したのち、box ごとに 0~1 の値を出力し、残すべき box は 1 になり捨てるべき box は 0 になるように学習します。\n\n## Object Relation Module\n\n物体間の関係には 2 つの意味があります。1 つは意味的な関連で、もう一つは座標的な関連です。\nボールっぽいものとバットっぽいものがあったとしても、ものすごく離れた場所にあるのであれば無関係かもしれないですが、近くにあればきっとボールとバットのペアと予測するのが正しそうです。\nしかし、通常の attention は、意味的な関連しか扱えていません。すべての box をなんらかの vector にしてしまっていますし、convolution や pooling は座標に依存しない操作なのでその vector に座標そのものの情報は埋め込まれません。\n\nそこでこの論文ではふつうの Attention をちょっといじった object relation module というものを提案しています。\n\n物体 m, n 間の attention を算出する式を見てみます。\n\n<img src=\"https://qiita-image-store.s3.amazonaws.com/0/39030/2e19515d-72ea-f065-e517-f8c7fe466fb8.png\" width=\"50%\">\n\n$\\omega_A^{mn}$ は通常の Attention と同じで、object m と n を表すベクトル（を線形変換したもの）の内積です。\n$\\omega_G$ の部分を無視すれば、この式は単に各 object ごとに内積をとったものを softmax にかけている = ふつうの Attention の計算式と一致します。\n\n$\\omega_G$ は、座標的な関係を考慮するためのパラメータです。\n\n<img src=\"https://qiita-image-store.s3.amazonaws.com/0/39030/9a24daf8-38cc-a765-af2f-7efa141cdad9.png\" width=\"50%\">\n\n$f_G^m$ は object m の座標情報 (x,y, w, h) を表します。\n$\\varepsilon_G$ はふたつの object の座標情報からそれらの座標的関係を計算する関数です。\n\n```math\n\\varepsilon(f_G^m, f_G^n) = (\\log(\\frac{|x_m - x_n|}{w_m}), \\log(\\frac{|y_m - y_n|}{h_m}), \\log(\\frac{w_n}{w_m}), \\log(\\frac{h_n}{h_m}))\n```\n\nさらに、 $max{0, ...}$ をとることで、ReLU 的な働きをし、まったく座標的に関係のない object からの影響を 0 にしています。\n\n## まとめと感想\n\n実験と結果はもと論文を読むのが一番くわしいのでそちらを参照ください。\n「一枚の画像から X を取得したいが、画像の主体となるような X だけを取りたい」といったケースにも応用できる手法かなと思っていて、実験してみたいと思いつつ、本当に end-to-end でやるのはちょっと大変そうすぎるという印象もあります。\n（また論文中には end-to-end が 0 から学習する際の問題にも触れられています。）\nとはいえ完全に end-to-end というのはやっぱり夢があって好きです。\n","slug":"Object_間の関係を使って後処理_0_の物体検出を実現する:_Relation_Networks_for_Object_Detection","title":"Object 間の関係を使って後処理 0 の物体検出を実現する: Relation Networks for Object Detection","timestamp":1544452540000,"tags":["画像処理","DeepLearning","論文読み","物体検出"]}]},"__N_SSG":true}