{"pageProps":{"post":{"content":"<p>Instance Segmentation のタスクに対する手法を整理・分解し、精度をより向上する <code>Semi-convolutional operators</code> を提案した論文です。</p>\n<p><img src=\"https://qiita-image-store.s3.amazonaws.com/0/39030/8164ed4c-3f5d-c772-e21d-7d02d5146461.png\" alt=\"image.png\"></p>\n<p>この記事は、Wantedly の勉強会で取り上げられた論文・技術をまとめたものです。\n<a href=\"https://qiita.com/advent-calendar/2018/wantedly_ml\">2018 年に読んだ機械学習系論文・技術まとめ at Wantedly Advent Calendar 2018 - Qiita</a></p>\n<h2>Reference</h2>\n<ul>\n<li>Semi-convolutional Operators for Instance Segmentation [David Novotny, Samuel Albanie, Diane Larlus, and Andrea Vedaldi. ECCV 2018]</li>\n<li><a href=\"https://arxiv.org/abs/1807.10712\">https://arxiv.org/abs/1807.10712</a></li>\n</ul>\n<p>（文中の図表は論文より引用しています）</p>\n<h2>Instance Segmentation</h2>\n<p>まずはじめに簡単に Instance Segmentation というタスクと、現在主流とされているアプローチについて述べます。</p>\n<p>Instance Segmentation とは、画像の各 Pixel について、 <strong>どのクラスに属すか、どのインスタンスに属するか</strong> を予測するタスクです。\n入力画像を「この領域は人、この領域は車、...」というように色塗りしていくタスクです。</p>\n<p><img src=\"https://qiita-image-store.s3.amazonaws.com/0/39030/146f0988-659b-4f15-df32-e065ddae5e70.png\" alt=\"image.png\">(Fig. 5 より)</p>\n<p>Instance Segmentation において重要なのが <strong>どのインスタンスに属するか</strong> も予測しなければならないという点です。\nたとえば人が 3 人で肩を組んでいるような画像の場合、どこからどこまでが 1 人目かを予測しなければなりません。\n一方、インスタンスを考慮せず色塗りをしていくようなタスクを Semantic Segmentation といいます。</p>\n<p>Semantic Segmentation の場合は、入力画像の各 Pixel について多クラス分類を行えば Segmentation の完成になります。\nInstance Segmentation ではそれに加えて個々のインスタンスを区別するような仕組みが必要になります。</p>\n<h3>propose &#x26; verify</h3>\n<p>Instance Segmentation タスクへのアプローチとして、現在主流とされているのは Mask R-CNN [^1] に代表される Region based な手法です。\n（Mask R-CNN は FAIR から出ている論文で、 OSS として公開されている Detectron に実装が含まれています。 <a href=\"https://github.com/facebookresearch/Detectron\">https://github.com/facebookresearch/Detectron</a> ）</p>\n<p>[^1]: K. He, et al., <a href=\"https://arxiv.org/abs/1703.06870\">https://arxiv.org/abs/1703.06870</a></p>\n<p>Mask R-CNN は、物体のクラスと bounding box だけを予測する Object Detection タスクへのアプローチを応用しています。\nまず Object Detection をすることで「この bounding box に人間が 1 人いる」ということを予測し、その後 bounding box 内を色塗りしていきます。\nObject Detection として bounding box を予測している時点で Instance を分離することが出来ています。色塗りのフェーズでは、すでに Instance が分離されているので単なる Pixel 単位の 2 クラス分類をやればよいことになります。</p>\n<p>はじめに Region を提案し、その中を精査するこれらの手法を、この論文では <em>propose &#x26; verify</em> (P&#x26;V) と呼んでいます。</p>\n<p>ここで、 <strong>P&#x26;V は必ず一度矩形で切り取ってから色塗りをしなければならない</strong> という点が問題になります。\n予測したい物体は必ずしも矩形で近似できるような形状をしているとは限りません。\n実際の形状と極端にかけ離れた場合、bounding box を予測すること自体が難しく、また Instance の分離も難しくなります。</p>\n<p><img src=\"https://qiita-image-store.s3.amazonaws.com/0/39030/39b808c2-cc63-a245-73c1-5b6ebc89c9be.png\" alt=\"image.png\"></p>\n<h3>instance coloring</h3>\n<p>P&#x26;V の問題点を解決する方法として、Pixel ごとに <strong>ラベル + Instance の identifier となる何か</strong> を予測する方法があります。\nこれらをこの論文では <em>instance coloring</em> (IC) と呼んでいます。</p>\n<p>「Instance の identifier となる何か」 は、連番などではうまく学習できません（どの Object が ID 1 なのか ID 2 なのかわからない）。\nそこで、 Pixel ごとに低次元の embedding を出力し、<strong>同じ Instance に所属する Pixel の embedding たちが似たものになるように学習します</strong>。\n入力画像に対して、Pixel ごとのラベルと embedding を出力し、embedding を基に Pixel たちをクラスタリングすることで Instance を分離します。</p>\n<p>IC の良いところは、典型的な image-to-image の問題と同じネットワーク構造を利用できるところです。\nSemantic Segmetation, Style Transfer など、画像を入力とし同じサイズの feature map を出力とするタスクは他にも数多くあり、それらと同じ構造をシンプルに流用できるのは大きな利点になります。\n（P&#x26;V の場合は Region Proposal + Region ごとの Coloring が必要で、ネットワーク構造としてはかなり複雑かつ独特なものになります）</p>\n<p>一方、IC であまり精度が出ない大きな理由の一つに <strong>画像的に似た領域が繰り返されると Instance の分離に失敗する</strong> という問題があります。\nimage-to-image のネットワークは通常 Convolutional operators をベースにしていますが、CNN の出力は、入力である pixel の特徴量にのみ依存し、 <strong>座標は全く結果に影響を及ぼしません</strong> 。\nそのため、画像的にそっくりな領域が複数あると、それらの pixel に対する embedding は同じような値になってしまい、クラスタリングがうまくいきません。</p>\n<h2>Semi-convolutional operators</h2>\n<p>一般的な IC では、出力された embedding が次の条件をみたすことを目標とします。</p>\n<p><img src=\"https://qiita-image-store.s3.amazonaws.com/0/39030/00e031cb-860d-d25c-af24-213ad5fd8565.png\" alt=\"image.png\"></p>\n<p>ここで、 $\\Omega$ は全 Pixel の集合、 $x$ は入力画像、 $\\Phi$ は学習したい関数（NN）、 $S_k$ はクラス k の segmentation mask、 $M$ はマージン （hyperparameter） です。\n言葉で説明すると、 <strong>同じクラスに属する Pixel $u$, $v$ の embedding の距離をより近づけ、違うクラスに属する場合はより遠ざける</strong> という感じです。\n$M$ は分離境界をよりくっきりさせるためのパラメータです。</p>\n<p>さきほど述べたように $\\Phi$ は CNN であり、座標情報を加味できません。\nSemi-convolutinal 版では、 $\\Phi$ の代わりに次のような $\\Psi$ を考えます。</p>\n<div class=\"remark-highlight\"><pre class=\"language-math\"><code class=\"language-math\">\\Psi_u(x) = f(\\Phi_u(x), u)</code></pre></div>\n<p>ここで、 $u$ は Pixel の座標を表し、 $f$ は $\\Phi$ の結果と座標情報を合成するなんらかの関数です。\n$f$ の簡単な例としては、単純な足し算が考えられます。\n$\\Psi$ は、CNN の結果に加えて座標情報も持ち合わせているため、IC の弱点を克服できています。\n$f$ を単純な加算とし、うまく学習が成功した場合、各 Instance ごとに centroid $c_k$ が決定され、</p>\n<div class=\"remark-highlight\"><pre class=\"language-math\"><code class=\"language-math\">\\forall u \\in S_k: \\Phi_u(x) + u = c_k</code></pre></div>\n<p>となるように $\\Phi$ が学習されます。\nこれを可視化すると次の画像のようになります。</p>\n<p><img src=\"https://qiita-image-store.s3.amazonaws.com/0/39030/74875b7b-be1b-4eb3-0e3d-f3c53fced5a8.png\" alt=\"image.png\">\n(Fig.2 より)</p>\n<p>各インスタンス内の Pixel から、なんとなく中心っぽい場所へベクトルが伸びているのがわかります。</p>\n<p>実際の学習の際の損失関数は次のようになります。</p>\n<img src=\"https://qiita-image-store.s3.amazonaws.com/0/39030/914e8f93-3c56-8d39-270e-1a5f3703bc79.png\" width=\"60%\">\n<p>同じインスタンスに属する Pixel の embedding たちを平均値になるべく近づける、というのが損失関数になります。\n（マージンの考えも含まれていないし、「違うインスタンスとの距離を取る」という損失も含まれていないですが、これで十分に良い学習ができたと述べられています。）</p>\n<p>実際にはもうちょっと複雑な $\\Psi$ や距離の定義を使っていますが、概要としては上記のようなものを Semi-convolutional operators として提案しています。</p>\n<h2>Experiments</h2>\n<p>Mask R-CNN との統合もこの論文の重要な topic なのですが、ぶっちゃけ論文を読んだほうがわかりやすいので飛ばして実験結果をざーっと眺めてみます。</p>\n<p><img src=\"https://qiita-image-store.s3.amazonaws.com/0/39030/e7516b68-9a66-154b-45ae-61199e0de90a.png\" alt=\"image.png\">(Fig. 3 より)</p>\n<p>まずはじめに、 画像的にそっくりな領域が繰り返されてもうまく Instance を分離できることを確認しています。\n(c) は通常の Conv. のみを使って IC を行った場合の結果です。クラスタリングに大失敗していることがわかります。\n一方 (d) の Semi-conv. 版ではきれいな分離が実現されています。</p>\n<p><img src=\"https://qiita-image-store.s3.amazonaws.com/0/39030/09bb8f76-5ba7-ad51-af97-2dbfc902cd66.png\" alt=\"image.png\"></p>\n<p>つぎに線虫の segmentation です。こちらは P&#x26;V のように矩形で認識するタイプの手法がニガテとするようなタスクです。\n現在主流である Mask RCNN よりも良い結果が示されています。</p>\n<p><img src=\"https://qiita-image-store.s3.amazonaws.com/0/39030/77a2f7fe-ee56-13ae-f49b-faf825cbf406.png\" alt=\"image.png\"></p>\n<p>より一般的なデータである PASCAL VOC2012 に対しても Mask RCNN より良い結果となっています（Mask RCNN に Semi-conv. の仕組みを組み込んだもので比較しています。）</p>\n<h2>まとめと感想</h2>\n<p>instance coloring の手法をまったく知らなかったのですが、 <a href=\"https://arxiv.org/abs/1808.01244\">CornerNet: Detecting Objects as Paired Keypoints</a> で Pixel ごとの embedding をクラスタリングしてペアを作るという手法を知り、興味を持ったのでその関連で読んでみた論文です。\nP&#x26;V 形式はかなり複雑な構造になるので、それを避けられるならすごく面白いなと思ったのですが、この論文では Mask R-CNN と組み合わせることで精度向上と言っているので、まだまだ IC 単体で勝てる感じではないのでしょうか？</p>\n<p>同じタスクに対して全く違う 2 つのアプローチが（比較対象になるくらいには）同じような成果を出しているのも面白いところです。segmentation は主流ではなかった分、まだまだ改善がありそうで楽しみです。</p>\n","meta":{"rawMarkdown":"---\ntitle: \"【論文読み】Semi-convolutional Operators for Instance Segmentation\"\ndate: 2019-02-11T16:59:46+09:00\ntags: [\"DeepLearning\", \"論文読み\"]\nurl: https://qiita.com/agatan/items/2cf1209b7370db45eba5\n---\n\nInstance Segmentation のタスクに対する手法を整理・分解し、精度をより向上する `Semi-convolutional operators` を提案した論文です。\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/8164ed4c-3f5d-c772-e21d-7d02d5146461.png)\n\nこの記事は、Wantedly の勉強会で取り上げられた論文・技術をまとめたものです。\n[2018 年に読んだ機械学習系論文・技術まとめ at Wantedly Advent Calendar 2018 - Qiita](https://qiita.com/advent-calendar/2018/wantedly_ml)\n\n## Reference\n\n- Semi-convolutional Operators for Instance Segmentation [David Novotny, Samuel Albanie, Diane Larlus, and Andrea Vedaldi. ECCV 2018]\n- https://arxiv.org/abs/1807.10712\n\n（文中の図表は論文より引用しています）\n\n## Instance Segmentation\n\nまずはじめに簡単に Instance Segmentation というタスクと、現在主流とされているアプローチについて述べます。\n\nInstance Segmentation とは、画像の各 Pixel について、 **どのクラスに属すか、どのインスタンスに属するか** を予測するタスクです。\n入力画像を「この領域は人、この領域は車、...」というように色塗りしていくタスクです。\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/146f0988-659b-4f15-df32-e065ddae5e70.png)(Fig. 5 より)\n\nInstance Segmentation において重要なのが **どのインスタンスに属するか** も予測しなければならないという点です。\nたとえば人が 3 人で肩を組んでいるような画像の場合、どこからどこまでが 1 人目かを予測しなければなりません。\n一方、インスタンスを考慮せず色塗りをしていくようなタスクを Semantic Segmentation といいます。\n\nSemantic Segmentation の場合は、入力画像の各 Pixel について多クラス分類を行えば Segmentation の完成になります。\nInstance Segmentation ではそれに加えて個々のインスタンスを区別するような仕組みが必要になります。\n\n### propose & verify\n\nInstance Segmentation タスクへのアプローチとして、現在主流とされているのは Mask R-CNN [^1] に代表される Region based な手法です。\n（Mask R-CNN は FAIR から出ている論文で、 OSS として公開されている Detectron に実装が含まれています。 https://github.com/facebookresearch/Detectron ）\n\n[^1]: K. He, et al., https://arxiv.org/abs/1703.06870\n\nMask R-CNN は、物体のクラスと bounding box だけを予測する Object Detection タスクへのアプローチを応用しています。\nまず Object Detection をすることで「この bounding box に人間が 1 人いる」ということを予測し、その後 bounding box 内を色塗りしていきます。\nObject Detection として bounding box を予測している時点で Instance を分離することが出来ています。色塗りのフェーズでは、すでに Instance が分離されているので単なる Pixel 単位の 2 クラス分類をやればよいことになります。\n\nはじめに Region を提案し、その中を精査するこれらの手法を、この論文では _propose & verify_ (P&V) と呼んでいます。\n\nここで、 **P&V は必ず一度矩形で切り取ってから色塗りをしなければならない** という点が問題になります。\n予測したい物体は必ずしも矩形で近似できるような形状をしているとは限りません。\n実際の形状と極端にかけ離れた場合、bounding box を予測すること自体が難しく、また Instance の分離も難しくなります。\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/39b808c2-cc63-a245-73c1-5b6ebc89c9be.png)\n\n### instance coloring\n\nP&V の問題点を解決する方法として、Pixel ごとに **ラベル + Instance の identifier となる何か** を予測する方法があります。\nこれらをこの論文では _instance coloring_ (IC) と呼んでいます。\n\n「Instance の identifier となる何か」 は、連番などではうまく学習できません（どの Object が ID 1 なのか ID 2 なのかわからない）。\nそこで、 Pixel ごとに低次元の embedding を出力し、**同じ Instance に所属する Pixel の embedding たちが似たものになるように学習します**。\n入力画像に対して、Pixel ごとのラベルと embedding を出力し、embedding を基に Pixel たちをクラスタリングすることで Instance を分離します。\n\nIC の良いところは、典型的な image-to-image の問題と同じネットワーク構造を利用できるところです。\nSemantic Segmetation, Style Transfer など、画像を入力とし同じサイズの feature map を出力とするタスクは他にも数多くあり、それらと同じ構造をシンプルに流用できるのは大きな利点になります。\n（P&V の場合は Region Proposal + Region ごとの Coloring が必要で、ネットワーク構造としてはかなり複雑かつ独特なものになります）\n\n一方、IC であまり精度が出ない大きな理由の一つに **画像的に似た領域が繰り返されると Instance の分離に失敗する** という問題があります。\nimage-to-image のネットワークは通常 Convolutional operators をベースにしていますが、CNN の出力は、入力である pixel の特徴量にのみ依存し、 **座標は全く結果に影響を及ぼしません** 。\nそのため、画像的にそっくりな領域が複数あると、それらの pixel に対する embedding は同じような値になってしまい、クラスタリングがうまくいきません。\n\n## Semi-convolutional operators\n\n一般的な IC では、出力された embedding が次の条件をみたすことを目標とします。\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/00e031cb-860d-d25c-af24-213ad5fd8565.png)\n\nここで、 $\\Omega$ は全 Pixel の集合、 $x$ は入力画像、 $\\Phi$ は学習したい関数（NN）、 $S_k$ はクラス k の segmentation mask、 $M$ はマージン （hyperparameter） です。\n言葉で説明すると、 **同じクラスに属する Pixel $u$, $v$ の embedding の距離をより近づけ、違うクラスに属する場合はより遠ざける** という感じです。\n$M$ は分離境界をよりくっきりさせるためのパラメータです。\n\nさきほど述べたように $\\Phi$ は CNN であり、座標情報を加味できません。\nSemi-convolutinal 版では、 $\\Phi$ の代わりに次のような $\\Psi$ を考えます。\n\n```math\n\\Psi_u(x) = f(\\Phi_u(x), u)\n```\n\nここで、 $u$ は Pixel の座標を表し、 $f$ は $\\Phi$ の結果と座標情報を合成するなんらかの関数です。\n$f$ の簡単な例としては、単純な足し算が考えられます。\n$\\Psi$ は、CNN の結果に加えて座標情報も持ち合わせているため、IC の弱点を克服できています。\n$f$ を単純な加算とし、うまく学習が成功した場合、各 Instance ごとに centroid $c_k$ が決定され、\n\n```math\n\\forall u \\in S_k: \\Phi_u(x) + u = c_k\n```\n\nとなるように $\\Phi$ が学習されます。\nこれを可視化すると次の画像のようになります。\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/74875b7b-be1b-4eb3-0e3d-f3c53fced5a8.png)\n(Fig.2 より)\n\n各インスタンス内の Pixel から、なんとなく中心っぽい場所へベクトルが伸びているのがわかります。\n\n実際の学習の際の損失関数は次のようになります。\n\n<img src=\"https://qiita-image-store.s3.amazonaws.com/0/39030/914e8f93-3c56-8d39-270e-1a5f3703bc79.png\" width=\"60%\">\n\n同じインスタンスに属する Pixel の embedding たちを平均値になるべく近づける、というのが損失関数になります。\n（マージンの考えも含まれていないし、「違うインスタンスとの距離を取る」という損失も含まれていないですが、これで十分に良い学習ができたと述べられています。）\n\n実際にはもうちょっと複雑な $\\Psi$ や距離の定義を使っていますが、概要としては上記のようなものを Semi-convolutional operators として提案しています。\n\n## Experiments\n\nMask R-CNN との統合もこの論文の重要な topic なのですが、ぶっちゃけ論文を読んだほうがわかりやすいので飛ばして実験結果をざーっと眺めてみます。\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/e7516b68-9a66-154b-45ae-61199e0de90a.png)(Fig. 3 より)\n\nまずはじめに、 画像的にそっくりな領域が繰り返されてもうまく Instance を分離できることを確認しています。\n(c) は通常の Conv. のみを使って IC を行った場合の結果です。クラスタリングに大失敗していることがわかります。\n一方 (d) の Semi-conv. 版ではきれいな分離が実現されています。\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/09bb8f76-5ba7-ad51-af97-2dbfc902cd66.png)\n\nつぎに線虫の segmentation です。こちらは P&V のように矩形で認識するタイプの手法がニガテとするようなタスクです。\n現在主流である Mask RCNN よりも良い結果が示されています。\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/77a2f7fe-ee56-13ae-f49b-faf825cbf406.png)\n\nより一般的なデータである PASCAL VOC2012 に対しても Mask RCNN より良い結果となっています（Mask RCNN に Semi-conv. の仕組みを組み込んだもので比較しています。）\n\n## まとめと感想\n\ninstance coloring の手法をまったく知らなかったのですが、 [CornerNet: Detecting Objects as Paired Keypoints](https://arxiv.org/abs/1808.01244) で Pixel ごとの embedding をクラスタリングしてペアを作るという手法を知り、興味を持ったのでその関連で読んでみた論文です。\nP&V 形式はかなり複雑な構造になるので、それを避けられるならすごく面白いなと思ったのですが、この論文では Mask R-CNN と組み合わせることで精度向上と言っているので、まだまだ IC 単体で勝てる感じではないのでしょうか？\n\n同じタスクに対して全く違う 2 つのアプローチが（比較対象になるくらいには）同じような成果を出しているのも面白いところです。segmentation は主流ではなかった分、まだまだ改善がありそうで楽しみです。\n","contentMarkdown":"\nInstance Segmentation のタスクに対する手法を整理・分解し、精度をより向上する `Semi-convolutional operators` を提案した論文です。\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/8164ed4c-3f5d-c772-e21d-7d02d5146461.png)\n\nこの記事は、Wantedly の勉強会で取り上げられた論文・技術をまとめたものです。\n[2018 年に読んだ機械学習系論文・技術まとめ at Wantedly Advent Calendar 2018 - Qiita](https://qiita.com/advent-calendar/2018/wantedly_ml)\n\n## Reference\n\n- Semi-convolutional Operators for Instance Segmentation [David Novotny, Samuel Albanie, Diane Larlus, and Andrea Vedaldi. ECCV 2018]\n- https://arxiv.org/abs/1807.10712\n\n（文中の図表は論文より引用しています）\n\n## Instance Segmentation\n\nまずはじめに簡単に Instance Segmentation というタスクと、現在主流とされているアプローチについて述べます。\n\nInstance Segmentation とは、画像の各 Pixel について、 **どのクラスに属すか、どのインスタンスに属するか** を予測するタスクです。\n入力画像を「この領域は人、この領域は車、...」というように色塗りしていくタスクです。\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/146f0988-659b-4f15-df32-e065ddae5e70.png)(Fig. 5 より)\n\nInstance Segmentation において重要なのが **どのインスタンスに属するか** も予測しなければならないという点です。\nたとえば人が 3 人で肩を組んでいるような画像の場合、どこからどこまでが 1 人目かを予測しなければなりません。\n一方、インスタンスを考慮せず色塗りをしていくようなタスクを Semantic Segmentation といいます。\n\nSemantic Segmentation の場合は、入力画像の各 Pixel について多クラス分類を行えば Segmentation の完成になります。\nInstance Segmentation ではそれに加えて個々のインスタンスを区別するような仕組みが必要になります。\n\n### propose & verify\n\nInstance Segmentation タスクへのアプローチとして、現在主流とされているのは Mask R-CNN [^1] に代表される Region based な手法です。\n（Mask R-CNN は FAIR から出ている論文で、 OSS として公開されている Detectron に実装が含まれています。 https://github.com/facebookresearch/Detectron ）\n\n[^1]: K. He, et al., https://arxiv.org/abs/1703.06870\n\nMask R-CNN は、物体のクラスと bounding box だけを予測する Object Detection タスクへのアプローチを応用しています。\nまず Object Detection をすることで「この bounding box に人間が 1 人いる」ということを予測し、その後 bounding box 内を色塗りしていきます。\nObject Detection として bounding box を予測している時点で Instance を分離することが出来ています。色塗りのフェーズでは、すでに Instance が分離されているので単なる Pixel 単位の 2 クラス分類をやればよいことになります。\n\nはじめに Region を提案し、その中を精査するこれらの手法を、この論文では _propose & verify_ (P&V) と呼んでいます。\n\nここで、 **P&V は必ず一度矩形で切り取ってから色塗りをしなければならない** という点が問題になります。\n予測したい物体は必ずしも矩形で近似できるような形状をしているとは限りません。\n実際の形状と極端にかけ離れた場合、bounding box を予測すること自体が難しく、また Instance の分離も難しくなります。\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/39b808c2-cc63-a245-73c1-5b6ebc89c9be.png)\n\n### instance coloring\n\nP&V の問題点を解決する方法として、Pixel ごとに **ラベル + Instance の identifier となる何か** を予測する方法があります。\nこれらをこの論文では _instance coloring_ (IC) と呼んでいます。\n\n「Instance の identifier となる何か」 は、連番などではうまく学習できません（どの Object が ID 1 なのか ID 2 なのかわからない）。\nそこで、 Pixel ごとに低次元の embedding を出力し、**同じ Instance に所属する Pixel の embedding たちが似たものになるように学習します**。\n入力画像に対して、Pixel ごとのラベルと embedding を出力し、embedding を基に Pixel たちをクラスタリングすることで Instance を分離します。\n\nIC の良いところは、典型的な image-to-image の問題と同じネットワーク構造を利用できるところです。\nSemantic Segmetation, Style Transfer など、画像を入力とし同じサイズの feature map を出力とするタスクは他にも数多くあり、それらと同じ構造をシンプルに流用できるのは大きな利点になります。\n（P&V の場合は Region Proposal + Region ごとの Coloring が必要で、ネットワーク構造としてはかなり複雑かつ独特なものになります）\n\n一方、IC であまり精度が出ない大きな理由の一つに **画像的に似た領域が繰り返されると Instance の分離に失敗する** という問題があります。\nimage-to-image のネットワークは通常 Convolutional operators をベースにしていますが、CNN の出力は、入力である pixel の特徴量にのみ依存し、 **座標は全く結果に影響を及ぼしません** 。\nそのため、画像的にそっくりな領域が複数あると、それらの pixel に対する embedding は同じような値になってしまい、クラスタリングがうまくいきません。\n\n## Semi-convolutional operators\n\n一般的な IC では、出力された embedding が次の条件をみたすことを目標とします。\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/00e031cb-860d-d25c-af24-213ad5fd8565.png)\n\nここで、 $\\Omega$ は全 Pixel の集合、 $x$ は入力画像、 $\\Phi$ は学習したい関数（NN）、 $S_k$ はクラス k の segmentation mask、 $M$ はマージン （hyperparameter） です。\n言葉で説明すると、 **同じクラスに属する Pixel $u$, $v$ の embedding の距離をより近づけ、違うクラスに属する場合はより遠ざける** という感じです。\n$M$ は分離境界をよりくっきりさせるためのパラメータです。\n\nさきほど述べたように $\\Phi$ は CNN であり、座標情報を加味できません。\nSemi-convolutinal 版では、 $\\Phi$ の代わりに次のような $\\Psi$ を考えます。\n\n```math\n\\Psi_u(x) = f(\\Phi_u(x), u)\n```\n\nここで、 $u$ は Pixel の座標を表し、 $f$ は $\\Phi$ の結果と座標情報を合成するなんらかの関数です。\n$f$ の簡単な例としては、単純な足し算が考えられます。\n$\\Psi$ は、CNN の結果に加えて座標情報も持ち合わせているため、IC の弱点を克服できています。\n$f$ を単純な加算とし、うまく学習が成功した場合、各 Instance ごとに centroid $c_k$ が決定され、\n\n```math\n\\forall u \\in S_k: \\Phi_u(x) + u = c_k\n```\n\nとなるように $\\Phi$ が学習されます。\nこれを可視化すると次の画像のようになります。\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/74875b7b-be1b-4eb3-0e3d-f3c53fced5a8.png)\n(Fig.2 より)\n\n各インスタンス内の Pixel から、なんとなく中心っぽい場所へベクトルが伸びているのがわかります。\n\n実際の学習の際の損失関数は次のようになります。\n\n<img src=\"https://qiita-image-store.s3.amazonaws.com/0/39030/914e8f93-3c56-8d39-270e-1a5f3703bc79.png\" width=\"60%\">\n\n同じインスタンスに属する Pixel の embedding たちを平均値になるべく近づける、というのが損失関数になります。\n（マージンの考えも含まれていないし、「違うインスタンスとの距離を取る」という損失も含まれていないですが、これで十分に良い学習ができたと述べられています。）\n\n実際にはもうちょっと複雑な $\\Psi$ や距離の定義を使っていますが、概要としては上記のようなものを Semi-convolutional operators として提案しています。\n\n## Experiments\n\nMask R-CNN との統合もこの論文の重要な topic なのですが、ぶっちゃけ論文を読んだほうがわかりやすいので飛ばして実験結果をざーっと眺めてみます。\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/e7516b68-9a66-154b-45ae-61199e0de90a.png)(Fig. 3 より)\n\nまずはじめに、 画像的にそっくりな領域が繰り返されてもうまく Instance を分離できることを確認しています。\n(c) は通常の Conv. のみを使って IC を行った場合の結果です。クラスタリングに大失敗していることがわかります。\n一方 (d) の Semi-conv. 版ではきれいな分離が実現されています。\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/09bb8f76-5ba7-ad51-af97-2dbfc902cd66.png)\n\nつぎに線虫の segmentation です。こちらは P&V のように矩形で認識するタイプの手法がニガテとするようなタスクです。\n現在主流である Mask RCNN よりも良い結果が示されています。\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/77a2f7fe-ee56-13ae-f49b-faf825cbf406.png)\n\nより一般的なデータである PASCAL VOC2012 に対しても Mask RCNN より良い結果となっています（Mask RCNN に Semi-conv. の仕組みを組み込んだもので比較しています。）\n\n## まとめと感想\n\ninstance coloring の手法をまったく知らなかったのですが、 [CornerNet: Detecting Objects as Paired Keypoints](https://arxiv.org/abs/1808.01244) で Pixel ごとの embedding をクラスタリングしてペアを作るという手法を知り、興味を持ったのでその関連で読んでみた論文です。\nP&V 形式はかなり複雑な構造になるので、それを避けられるならすごく面白いなと思ったのですが、この論文では Mask R-CNN と組み合わせることで精度向上と言っているので、まだまだ IC 単体で勝てる感じではないのでしょうか？\n\n同じタスクに対して全く違う 2 つのアプローチが（比較対象になるくらいには）同じような成果を出しているのも面白いところです。segmentation は主流ではなかった分、まだまだ改善がありそうで楽しみです。\n","slug":"【論文読み】Semi-convolutional_Operators_for_Instance_Segmentation","title":"【論文読み】Semi-convolutional Operators for Instance Segmentation","timestamp":1549871986000,"tags":["DeepLearning","論文読み"]}}},"__N_SSG":true}