{"pageProps":{"post":{"content":"<p>高速かつ高精度に物体検出を行う RetinaNet に使われている <strong>Focal Loss</strong> という損失関数を提案した論文を読んだので紹介します。\nFAIR(Facebook AI Research) が書いた論文で ICCV 2017 に採択されています。</p>\n<img src=\"https://qiita-image-store.s3.amazonaws.com/0/39030/38ab1ef6-c4c5-fd78-d903-0954479143a6.png\" width=\"60%\">\n<p>この記事は、Wantedly の勉強会で取り上げられた論文・技術をまとめたものです。\n<a href=\"https://qiita.com/advent-calendar/2018/wantedly_ml\">2018 年に読んだ機械学習系論文・技術まとめ at Wantedly Advent Calendar 2018 - Qiita</a></p>\n<h4>Reference</h4>\n<ul>\n<li>Focal Loss for Dense Object Detection [Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, Piotr Dollár @ ICCV 2017]</li>\n<li><a href=\"https://arxiv.org/abs/1708.02002\">https://arxiv.org/abs/1708.02002</a></li>\n</ul>\n<p>（文中の図表は論文より引用）</p>\n<h3>モチベーション</h3>\n<p>精度の良い object detector の多くは R-CNN[^1] ベースの two-stage object detector の構成を取っています。\n[^1]: <a href=\"https://arxiv.org/abs/1311.2524\">Rich feature hierarchies for accurate object detection and semantic segmentation</a> [Ross Girshick, et al.]\nR-CNN は、 まず物体のある bounding box の候補集合を提案し、その後 2nd stage で提案された各 box について classification を行うという構成になっています。\ntwo-stage object detector は高い精度を記録していますが、一方で複雑さと推論速度に問題がありました。</p>\n<p>そこで、 YOLO[^2][^3] や SSD[^4][^5] のような one-stage で高速に物体検出を行うネットワークが提案されてきました。\n[^2]: <a href=\"https://arxiv.org/abs/1506.02640\">You Only Look Once: Unified, Real-Time Object Detection</a> [J. Redmon, et al.]\n[^3]: <a href=\"https://arxiv.org/abs/1612.08242\">YOLO9000: Better, Faster, Stronger</a> [J. Redmon, et al.]\n[^4]: <a href=\"https://arxiv.org/abs/1512.02325\">SSD: Single Shot MultiBox Detector</a> [W. Liu, et al.]\n[^5]: <a href=\"https://arxiv.org/abs/1701.06659\">DSSD : Deconvolutional Single Shot Detector</a> [C.-Y. Fu, et al.]</p>\n<p>しかし、これらの one-stage detector は高速な一方で（当時の） state-of-the-art methods と比べると精度は劣るという課題がありました。</p>\n<p>この論文では one-stage detector が two-stage detector と並ぶ精度を出せないのは「クラス間の不均衡」が原因であるという仮説をたてています。\n画像のほとんどの pixel は background であり、foreground（背景以外のクラス）に属する pixel の数と比べると圧倒的な不均衡があるため、学習のほとんどが簡単な background 判定に支配されてしまいます。\n（two-stage の場合は 1st stage で注目すべき部分を限定しているため、background の多くは 1st stage でフィルタリングされ、2nd stage は不均衡が解決された状態で学習することができます。）</p>\n<p>そこで登場するのが <strong>Focal Loss</strong> です。</p>\n<h3>Focal Loss</h3>\n<p>Focal Loss は通常の cross entropy loss (CE) を動的に scaling させる損失関数です。\n通常の CE と比較したのが次の図です。</p>\n<img src=\"https://qiita-image-store.s3.amazonaws.com/0/39030/77bb8c08-0a47-d77a-fed7-17038e29a8cf.png\" width=\"60%\">\n<p>通常の CE は以下のようなものです(binary cross entropy の場合）。</p>\n<div class=\"remark-highlight\"><pre class=\"language-math\"><code class=\"language-math\">{\\rm CE}(p_t) = -{\\rm log}(p_t).  \\\\\n\np_t = \\left\\{\n\\begin{array}{ll}\np &#x26;amp; {\\rm if}\\: y = 1 \\\\\n1 - p &#x26;amp; {\\rm otherwise.} \\\\\n\\end{array}\n\\right.</code></pre></div>\n<p>さきほどの図の <code>γ = 0</code> の青い曲線は通常の CE を <code>p</code> を x 軸にグラフにしたものです。\n図からわかるように、 0.6 といった十分に分類できている probability を出力したとしても、損失は無視できない値になっています。\nそのため、簡単に background と分類できていても大量の exmaple が積み重なって、 foreground の損失よりも強くなってしまいます。</p>\n<p>Focal Loss は、easy example （簡単に分類に成功している example）の損失を小さく scale します。</p>\n<div class=\"remark-highlight\"><pre class=\"language-math\"><code class=\"language-math\">{\\rm FL}(p_t) = -(1 - p_t) ^ \\gamma {\\rm log} (p_t).</code></pre></div>\n<p><code>γ</code> はパラメータで、どのくらい easy example の損失を decay するかを決定します。\n簡単に分類に成功している example では</p>\n<div class=\"remark-highlight\"><pre class=\"language-math\"><code class=\"language-math\">(1 - p_t) ^ \\gamma</code></pre></div>\n<p>が小さい値になるため、損失への寄与が小さくなります。\nこれによって、より難しい focus すべき example が学習に強く寄与できるようになります。\n（論文中の実験では <code>γ = 2</code> を採用しています。）</p>\n<p>この論文では RetinaNet というアーキテクチャを設計し Focal Loss を用いて学習させています。\nRetinaNet の設計の詳細は省きますが、精度を既存の object detector と比較したのが冒頭の図と次の表です。</p>\n<p><img src=\"https://qiita-image-store.s3.amazonaws.com/0/39030/d711797c-f160-08c3-97b8-027f34bbf373.png\" alt=\"image.png\"></p>\n<p>two-stage detector と同等（以上）の精度を達成しています。</p>\n<h3>感想</h3>\n<p>アイディアがシンプルで、 object detection 以外のタスクに対しても応用可能な手法ですごく好きな論文です。\n実装も簡単なので試しやすく良い結果がでたので、何度かお世話になっています。</p>\n<p>ちなみにこの論文のあとに書かれた YOLOv3[^6] では Focal Loss を採用していませんが、 RetinaNet と精度・速度を比較していて Focal Loss に関する考察も書かれています。</p>\n<img src=\"https://qiita-image-store.s3.amazonaws.com/0/39030/d2caf698-0c58-c03d-4fa4-085e63ccb24c.png\" width=\"60%\">\n<p>[^6]: <a href=\"https://arxiv.org/abs/1804.02767\">YOLOv3: An Incremental Improvement</a> [J. Redmon, et al.]</p>\n<p>また、facebook が公開している <a href=\"https://github.com/facebookresearch/Detectron\">Detectron</a> にも RetineNet の実装が含まれているので簡単に利用することもできそうです。</p>\n","meta":{"rawMarkdown":"---\ntitle: \"[論文紹介] Focal Loss for Dense Object Detection\"\ndate: 2018-12-02T18:41:00+09:00\ntags: [\"DeepLearning\", \"論文読み\", \"ICCV\"]\nurl: https://qiita.com/agatan/items/53fe8d21f2147b0ac982\n---\n\n高速かつ高精度に物体検出を行う RetinaNet に使われている **Focal Loss** という損失関数を提案した論文を読んだので紹介します。\nFAIR(Facebook AI Research) が書いた論文で ICCV 2017 に採択されています。\n\n<img src=\"https://qiita-image-store.s3.amazonaws.com/0/39030/38ab1ef6-c4c5-fd78-d903-0954479143a6.png\" width=\"60%\">\n\nこの記事は、Wantedly の勉強会で取り上げられた論文・技術をまとめたものです。\n[2018 年に読んだ機械学習系論文・技術まとめ at Wantedly Advent Calendar 2018 - Qiita](https://qiita.com/advent-calendar/2018/wantedly_ml)\n\n#### Reference\n\n- Focal Loss for Dense Object Detection [Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, Piotr Dollár @ ICCV 2017]\n- https://arxiv.org/abs/1708.02002\n\n（文中の図表は論文より引用）\n\n### モチベーション\n\n精度の良い object detector の多くは R-CNN[^1] ベースの two-stage object detector の構成を取っています。\n[^1]: [Rich feature hierarchies for accurate object detection and semantic segmentation](https://arxiv.org/abs/1311.2524) [Ross Girshick, et al.]\nR-CNN は、 まず物体のある bounding box の候補集合を提案し、その後 2nd stage で提案された各 box について classification を行うという構成になっています。\ntwo-stage object detector は高い精度を記録していますが、一方で複雑さと推論速度に問題がありました。\n\nそこで、 YOLO[^2][^3] や SSD[^4][^5] のような one-stage で高速に物体検出を行うネットワークが提案されてきました。\n[^2]: [You Only Look Once: Unified, Real-Time Object Detection](https://arxiv.org/abs/1506.02640) [J. Redmon, et al.]\n[^3]: [YOLO9000: Better, Faster, Stronger](https://arxiv.org/abs/1612.08242) [J. Redmon, et al.]\n[^4]: [SSD: Single Shot MultiBox Detector](https://arxiv.org/abs/1512.02325) [W. Liu, et al.]\n[^5]: [DSSD : Deconvolutional Single Shot Detector](https://arxiv.org/abs/1701.06659) [C.-Y. Fu, et al.]\n\nしかし、これらの one-stage detector は高速な一方で（当時の） state-of-the-art methods と比べると精度は劣るという課題がありました。\n\nこの論文では one-stage detector が two-stage detector と並ぶ精度を出せないのは「クラス間の不均衡」が原因であるという仮説をたてています。\n画像のほとんどの pixel は background であり、foreground（背景以外のクラス）に属する pixel の数と比べると圧倒的な不均衡があるため、学習のほとんどが簡単な background 判定に支配されてしまいます。\n（two-stage の場合は 1st stage で注目すべき部分を限定しているため、background の多くは 1st stage でフィルタリングされ、2nd stage は不均衡が解決された状態で学習することができます。）\n\nそこで登場するのが **Focal Loss** です。\n\n### Focal Loss\n\nFocal Loss は通常の cross entropy loss (CE) を動的に scaling させる損失関数です。\n通常の CE と比較したのが次の図です。\n\n<img src=\"https://qiita-image-store.s3.amazonaws.com/0/39030/77bb8c08-0a47-d77a-fed7-17038e29a8cf.png\" width=\"60%\">\n\n通常の CE は以下のようなものです(binary cross entropy の場合）。\n\n```math\n{\\rm CE}(p_t) = -{\\rm log}(p_t).  \\\\\n\np_t = \\left\\{\n\\begin{array}{ll}\np & {\\rm if}\\: y = 1 \\\\\n1 - p & {\\rm otherwise.} \\\\\n\\end{array}\n\\right.\n```\n\nさきほどの図の `γ = 0` の青い曲線は通常の CE を `p` を x 軸にグラフにしたものです。\n図からわかるように、 0.6 といった十分に分類できている probability を出力したとしても、損失は無視できない値になっています。\nそのため、簡単に background と分類できていても大量の exmaple が積み重なって、 foreground の損失よりも強くなってしまいます。\n\nFocal Loss は、easy example （簡単に分類に成功している example）の損失を小さく scale します。\n\n```math\n{\\rm FL}(p_t) = -(1 - p_t) ^ \\gamma {\\rm log} (p_t).\n```\n\n`γ` はパラメータで、どのくらい easy example の損失を decay するかを決定します。\n簡単に分類に成功している example では\n\n```math\n(1 - p_t) ^ \\gamma\n```\n\nが小さい値になるため、損失への寄与が小さくなります。\nこれによって、より難しい focus すべき example が学習に強く寄与できるようになります。\n（論文中の実験では `γ = 2` を採用しています。）\n\nこの論文では RetinaNet というアーキテクチャを設計し Focal Loss を用いて学習させています。\nRetinaNet の設計の詳細は省きますが、精度を既存の object detector と比較したのが冒頭の図と次の表です。\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/d711797c-f160-08c3-97b8-027f34bbf373.png)\n\ntwo-stage detector と同等（以上）の精度を達成しています。\n\n### 感想\n\nアイディアがシンプルで、 object detection 以外のタスクに対しても応用可能な手法ですごく好きな論文です。\n実装も簡単なので試しやすく良い結果がでたので、何度かお世話になっています。\n\nちなみにこの論文のあとに書かれた YOLOv3[^6] では Focal Loss を採用していませんが、 RetinaNet と精度・速度を比較していて Focal Loss に関する考察も書かれています。\n\n<img src=\"https://qiita-image-store.s3.amazonaws.com/0/39030/d2caf698-0c58-c03d-4fa4-085e63ccb24c.png\" width=\"60%\">\n\n[^6]: [YOLOv3: An Incremental Improvement](https://arxiv.org/abs/1804.02767) [J. Redmon, et al.]\n\nまた、facebook が公開している [Detectron](https://github.com/facebookresearch/Detectron) にも RetineNet の実装が含まれているので簡単に利用することもできそうです。\n","contentMarkdown":"\n高速かつ高精度に物体検出を行う RetinaNet に使われている **Focal Loss** という損失関数を提案した論文を読んだので紹介します。\nFAIR(Facebook AI Research) が書いた論文で ICCV 2017 に採択されています。\n\n<img src=\"https://qiita-image-store.s3.amazonaws.com/0/39030/38ab1ef6-c4c5-fd78-d903-0954479143a6.png\" width=\"60%\">\n\nこの記事は、Wantedly の勉強会で取り上げられた論文・技術をまとめたものです。\n[2018 年に読んだ機械学習系論文・技術まとめ at Wantedly Advent Calendar 2018 - Qiita](https://qiita.com/advent-calendar/2018/wantedly_ml)\n\n#### Reference\n\n- Focal Loss for Dense Object Detection [Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, Piotr Dollár @ ICCV 2017]\n- https://arxiv.org/abs/1708.02002\n\n（文中の図表は論文より引用）\n\n### モチベーション\n\n精度の良い object detector の多くは R-CNN[^1] ベースの two-stage object detector の構成を取っています。\n[^1]: [Rich feature hierarchies for accurate object detection and semantic segmentation](https://arxiv.org/abs/1311.2524) [Ross Girshick, et al.]\nR-CNN は、 まず物体のある bounding box の候補集合を提案し、その後 2nd stage で提案された各 box について classification を行うという構成になっています。\ntwo-stage object detector は高い精度を記録していますが、一方で複雑さと推論速度に問題がありました。\n\nそこで、 YOLO[^2][^3] や SSD[^4][^5] のような one-stage で高速に物体検出を行うネットワークが提案されてきました。\n[^2]: [You Only Look Once: Unified, Real-Time Object Detection](https://arxiv.org/abs/1506.02640) [J. Redmon, et al.]\n[^3]: [YOLO9000: Better, Faster, Stronger](https://arxiv.org/abs/1612.08242) [J. Redmon, et al.]\n[^4]: [SSD: Single Shot MultiBox Detector](https://arxiv.org/abs/1512.02325) [W. Liu, et al.]\n[^5]: [DSSD : Deconvolutional Single Shot Detector](https://arxiv.org/abs/1701.06659) [C.-Y. Fu, et al.]\n\nしかし、これらの one-stage detector は高速な一方で（当時の） state-of-the-art methods と比べると精度は劣るという課題がありました。\n\nこの論文では one-stage detector が two-stage detector と並ぶ精度を出せないのは「クラス間の不均衡」が原因であるという仮説をたてています。\n画像のほとんどの pixel は background であり、foreground（背景以外のクラス）に属する pixel の数と比べると圧倒的な不均衡があるため、学習のほとんどが簡単な background 判定に支配されてしまいます。\n（two-stage の場合は 1st stage で注目すべき部分を限定しているため、background の多くは 1st stage でフィルタリングされ、2nd stage は不均衡が解決された状態で学習することができます。）\n\nそこで登場するのが **Focal Loss** です。\n\n### Focal Loss\n\nFocal Loss は通常の cross entropy loss (CE) を動的に scaling させる損失関数です。\n通常の CE と比較したのが次の図です。\n\n<img src=\"https://qiita-image-store.s3.amazonaws.com/0/39030/77bb8c08-0a47-d77a-fed7-17038e29a8cf.png\" width=\"60%\">\n\n通常の CE は以下のようなものです(binary cross entropy の場合）。\n\n```math\n{\\rm CE}(p_t) = -{\\rm log}(p_t).  \\\\\n\np_t = \\left\\{\n\\begin{array}{ll}\np & {\\rm if}\\: y = 1 \\\\\n1 - p & {\\rm otherwise.} \\\\\n\\end{array}\n\\right.\n```\n\nさきほどの図の `γ = 0` の青い曲線は通常の CE を `p` を x 軸にグラフにしたものです。\n図からわかるように、 0.6 といった十分に分類できている probability を出力したとしても、損失は無視できない値になっています。\nそのため、簡単に background と分類できていても大量の exmaple が積み重なって、 foreground の損失よりも強くなってしまいます。\n\nFocal Loss は、easy example （簡単に分類に成功している example）の損失を小さく scale します。\n\n```math\n{\\rm FL}(p_t) = -(1 - p_t) ^ \\gamma {\\rm log} (p_t).\n```\n\n`γ` はパラメータで、どのくらい easy example の損失を decay するかを決定します。\n簡単に分類に成功している example では\n\n```math\n(1 - p_t) ^ \\gamma\n```\n\nが小さい値になるため、損失への寄与が小さくなります。\nこれによって、より難しい focus すべき example が学習に強く寄与できるようになります。\n（論文中の実験では `γ = 2` を採用しています。）\n\nこの論文では RetinaNet というアーキテクチャを設計し Focal Loss を用いて学習させています。\nRetinaNet の設計の詳細は省きますが、精度を既存の object detector と比較したのが冒頭の図と次の表です。\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/d711797c-f160-08c3-97b8-027f34bbf373.png)\n\ntwo-stage detector と同等（以上）の精度を達成しています。\n\n### 感想\n\nアイディアがシンプルで、 object detection 以外のタスクに対しても応用可能な手法ですごく好きな論文です。\n実装も簡単なので試しやすく良い結果がでたので、何度かお世話になっています。\n\nちなみにこの論文のあとに書かれた YOLOv3[^6] では Focal Loss を採用していませんが、 RetinaNet と精度・速度を比較していて Focal Loss に関する考察も書かれています。\n\n<img src=\"https://qiita-image-store.s3.amazonaws.com/0/39030/d2caf698-0c58-c03d-4fa4-085e63ccb24c.png\" width=\"60%\">\n\n[^6]: [YOLOv3: An Incremental Improvement](https://arxiv.org/abs/1804.02767) [J. Redmon, et al.]\n\nまた、facebook が公開している [Detectron](https://github.com/facebookresearch/Detectron) にも RetineNet の実装が含まれているので簡単に利用することもできそうです。\n","slug":"[論文紹介]_Focal_Loss_for_Dense_Object_Detection","title":"[論文紹介] Focal Loss for Dense Object Detection","timestamp":1543743660000,"tags":["DeepLearning","論文読み","ICCV"]}}},"__N_SSG":true}