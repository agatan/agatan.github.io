{"pageProps":{"post":{"content":"<p>ニュースの推薦に \"Knowledge Graph\" を活用する論文です。\nMicrosoft Research Asia のチームが WWW 2018 に投稿しています。</p>\n<p>この記事は、Wantedly の勉強会で取り上げられた論文・技術をまとめたものです。\n<a href=\"https://qiita.com/advent-calendar/2018/wantedly_ml\">2018 年に読んだ機械学習系論文・技術まとめ at Wantedly Advent Calendar 2018 - Qiita</a></p>\n<h2>Reference</h2>\n<ul>\n<li>DKN: Deep Knowledge-Aware Network for News Recommendation\n<ul>\n<li>Hongwei Wang, Fuzheng Zhang, Xing Xie and Minyi Guo, et al., WWW 2018</li>\n<li><a href=\"https://www2018.thewebconf.org/proceedings/#indus-922\">https://www2018.thewebconf.org/proceedings/#indus-922</a></li>\n</ul>\n</li>\n</ul>\n<p>文中の図表は論文より引用しています。</p>\n<h3>概要</h3>\n<p>一般にニュース中の言葉は、常識的知識を仮定していて凝縮された文章になっています。\n一方で推薦系の既存手法は ニュース中に現れない知識を取り扱えておらず、潜在的なニュース間の関係を活かした探索が出来ていないという問題がありました。</p>\n<p>この論文は、knowledge graph を活用した content-based な recommendation framework である <em>deep knowledge-aware network</em> (DKN) を提案しています。</p>\n<p>knowledge graph とは、様々なエンティティを様々なエッジでつないだ heterogeneous なグラフで、たとえば \"モナリザ -[の作者」-> ダ・ヴィンチ\" のような情報を溜め込んだ巨大なグラフです。\nGoogle Search の裏でも活躍しているらしく、一般的な「知識」を構造化された形で表現する方法としてよく使われています。\nknowledge graph を使うことで、 \"Donald Trump\" という単語そのものだけからはわからない、 \"United States\" という単語との関連、\"Politician\" という単語との関連などを導くことができます。</p>\n<p><img src=\"https://qiita-image-store.s3.amazonaws.com/0/39030/6967d5d1-0708-7e59-51c8-db107866ea63.png\" alt=\"image.png\"></p>\n<h3>何が難しいか</h3>\n<p>この論文では、ニュース推薦の難しさとして以下のようなものを挙げています。</p>\n<ul>\n<li>news の推薦は movie などと違って、リアルタイム性が高い問題（いつ publish されたニュースかが重要）であり、news 間の関係性もすぐにダメになる。\n<ul>\n<li>なので ID ベースの既存手法（協調フィルタリングなど）は効果が弱い</li>\n</ul>\n</li>\n<li>news はユーザによって興味範囲が違うし、ユーザは複数の興味範囲を持っていることがほとんど。</li>\n<li>news の文言は凝縮されている。\n<ul>\n<li>常識、大量の既知の entity を仮定している。</li>\n<li>“Boris Johnson Has Warned Donald Trump To Stick To The Iran Nuclear Deal\" というニュースは “Boris Johnson”, “Donald Trump”, “Iran\", “Nuclear” を知っている前提で書かれている</li>\n<li>↑ に興味のあるユーザはきっと “North Korean EMP Attack Would Cause Mass U.S. Starvation, Says Congressional Report” にも興味があるが、単語レベルでの関係性はほぼ無い</li>\n</ul>\n</li>\n</ul>\n<p>これらを Knowledge Graph を活用しつつ解決していきます。</p>\n<h3>Knowledge Graph Embedding</h3>\n<p>Knowledge Graph Embedding 自体はこの研究の contribution ではありませんが重要なので簡単に紹介します。\nKnowledge Graph Embedding は、通常の network embedding に近い問題設定で、 knowledge graph における各エンティティとエッジの低次元な embedding を求めるという問題です。\n<code>(head, relation, trail)</code> という triplet 構造をなるべく維持したまま、<code>h, r, t</code> それぞれを低次元空間で表現することが目標です。\nDKN は translation-based knowledge graph embedding というのを使っています。\n一番簡単な手法は <code>h + r = t</code> になるように embedding を定める手法です。\n（他にもいくつか紹介、実験されているけど大体発想は同じなので省略。）\nまずランダムな値で各エンティティ、エッジの embedding を初期化し、 <code>h + r = t</code> を満たすように gradient descent で embedding を微調整していきます。\nこれによって、エンティティやエッジ（＝関係性）の低次元なベクトルを得ることができました。</p>\n<h3>Deep Knowledge-aware Network (DKN)</h3>\n<p>DKN 自体は</p>\n<p>入力: 候補ニュースと、あるユーザの過去に見たニュースたち\n出力: クリック率</p>\n<p>となるような CTR 予測モデルです。</p>\n<p><img src=\"https://qiita-image-store.s3.amazonaws.com/0/39030/f64784da-f8b9-4620-c0b5-5a3db5640417.png\" alt=\"image.png\"></p>\n<p>候補となるニュースの特徴量をベクトルとして得るために、通常であれば単語の embedding を RNN や CNN でまとめ上げて固定長のベクトルに変換します。\nDKN ではここに Knowledge Graph Embedding によって得た特徴量を加えます。\n出現する単語ごとに Knowledge Graph 上のエンティティを探し、もし見つかったならそのエンティティ自体の embedding + 周辺のエンティティ embedding の平均を context vector として単語レベルの embedding に concat します。\nもしエンティティが見つからなければ 0 埋めでサイズをあわせます。</p>\n<p><img src=\"https://qiita-image-store.s3.amazonaws.com/0/39030/8e4f7cc4-89ee-5b3a-7343-245f33fa0b51.png\" alt=\"image.png\"></p>\n<h4>Attention-based User Interest Extraction</h4>\n<p>つぎに、「ユーザが過去に見た記事」という情報をどのように活用するかを考えます。（＝ユーザの特徴をどのようにベクトル化するか）\n記事ごとのベクトルは ↑ で求められたので、単純に過去クリックした記事のベクトルの平均値を使うという方法が考えられますが、ユーザの興味は複数にまたがりうるので単純に平均を取るのは適しているとは言えません。\n（たとえば「プログラミング」「テニス」「ラーメン」の記事をクリックしたユーザに対して、「プログラミング」系の記事を推薦するのは多分良いはずだけど、平均をとってしまっているとその寄与が薄まる。）\n「今推薦の候補に考えている記事」について過去見た記事それぞれがどう関わっているかを表現できる方法でなければならないと言っています。</p>\n<p>そこで、 Attention Module をつかっています。\n候補の記事と過去に見た記事たちとの間の attention を計算し、過去に見た記事たちのベクトルの重み付き和をとることで、興味分野が複数にまたがっていても候補記事との関連をうまく見出したベクトルが作り出せます。\nAttention Module の入力は「候補記事のベクトル」と「過去に見た記事のベクトル」で、出力はその記事の寄与度になります。過去に見たすべての記事に対してそれぞれ network に入れて寄与度を計算し、softmax にかけたうで記事ベクトルの重み付き和をとっています。</p>\n<h2>感想</h2>\n<p>ニュースタイトルは単語数が少なく固有名詞も多いので、単純な単語の embedding ではなかなか扱いづらいという問題を抱えていたので、 knowledge graph を使うというのはすごく納得の行く選択だなと思いました。\nただ、結果を見てみると Gain に対して複雑さや Knowledge Graph 自体を用意するコストが見合うかというとやはり厳しいかなという印象があります。（Microsoft はすでに自前の knowledge graph を持っているので...）</p>\n<p>Knowledge Graph Embedding については全く知らなかったのですが、面白い問題設定ですね。\n色々工夫されているようですが、 <code>h + r = t</code> というわかりやすく単純な方法でもそれなりに上手く行っていて面白かったです。</p>\n<p>また、ユーザごとのベクトル表現の作り方の部分は Knowledge Graph の活用部分よりも簡単かつ一般的なので、この部分だけでも応用できそうだなと思いました。</p>\n","meta":{"rawMarkdown":"---\ntitle: \"DKN: Deep Knowledge-Aware Network for News Recommendation\"\ndate: 2018-12-19T21:02:20+09:00\ntags: [\"DeepLearning\", \"WWW\", \"Recommendation\", \"論文読み\"]\nurl: https://qiita.com/agatan/items/24c6d8e00f2fc861bb04\n---\n\nニュースの推薦に \"Knowledge Graph\" を活用する論文です。\nMicrosoft Research Asia のチームが WWW 2018 に投稿しています。\n\nこの記事は、Wantedly の勉強会で取り上げられた論文・技術をまとめたものです。\n[2018 年に読んだ機械学習系論文・技術まとめ at Wantedly Advent Calendar 2018 - Qiita](https://qiita.com/advent-calendar/2018/wantedly_ml)\n\n## Reference\n\n- DKN: Deep Knowledge-Aware Network for News Recommendation\n  - Hongwei Wang, Fuzheng Zhang, Xing Xie and Minyi Guo, et al., WWW 2018\n  - https://www2018.thewebconf.org/proceedings/#indus-922\n\n文中の図表は論文より引用しています。\n\n### 概要\n\n一般にニュース中の言葉は、常識的知識を仮定していて凝縮された文章になっています。\n一方で推薦系の既存手法は ニュース中に現れない知識を取り扱えておらず、潜在的なニュース間の関係を活かした探索が出来ていないという問題がありました。\n\nこの論文は、knowledge graph を活用した content-based な recommendation framework である _deep knowledge-aware network_ (DKN) を提案しています。\n\nknowledge graph とは、様々なエンティティを様々なエッジでつないだ heterogeneous なグラフで、たとえば \"モナリザ -[の作者」-> ダ・ヴィンチ\" のような情報を溜め込んだ巨大なグラフです。\nGoogle Search の裏でも活躍しているらしく、一般的な「知識」を構造化された形で表現する方法としてよく使われています。\nknowledge graph を使うことで、 \"Donald Trump\" という単語そのものだけからはわからない、 \"United States\" という単語との関連、\"Politician\" という単語との関連などを導くことができます。\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/6967d5d1-0708-7e59-51c8-db107866ea63.png)\n\n### 何が難しいか\n\nこの論文では、ニュース推薦の難しさとして以下のようなものを挙げています。\n\n- news の推薦は movie などと違って、リアルタイム性が高い問題（いつ publish されたニュースかが重要）であり、news 間の関係性もすぐにダメになる。\n  - なので ID ベースの既存手法（協調フィルタリングなど）は効果が弱い\n- news はユーザによって興味範囲が違うし、ユーザは複数の興味範囲を持っていることがほとんど。\n- news の文言は凝縮されている。\n  - 常識、大量の既知の entity を仮定している。\n  - “Boris Johnson Has Warned Donald Trump To Stick To The Iran Nuclear Deal\" というニュースは “Boris Johnson”, “Donald Trump”, “Iran\", “Nuclear” を知っている前提で書かれている\n  - ↑ に興味のあるユーザはきっと “North Korean EMP Attack Would Cause Mass U.S. Starvation, Says Congressional Report” にも興味があるが、単語レベルでの関係性はほぼ無い\n\nこれらを Knowledge Graph を活用しつつ解決していきます。\n\n### Knowledge Graph Embedding\n\nKnowledge Graph Embedding 自体はこの研究の contribution ではありませんが重要なので簡単に紹介します。\nKnowledge Graph Embedding は、通常の network embedding に近い問題設定で、 knowledge graph における各エンティティとエッジの低次元な embedding を求めるという問題です。\n`(head, relation, trail)` という triplet 構造をなるべく維持したまま、`h, r, t` それぞれを低次元空間で表現することが目標です。\nDKN は translation-based knowledge graph embedding というのを使っています。\n一番簡単な手法は `h + r = t` になるように embedding を定める手法です。\n（他にもいくつか紹介、実験されているけど大体発想は同じなので省略。）\nまずランダムな値で各エンティティ、エッジの embedding を初期化し、 `h + r = t` を満たすように gradient descent で embedding を微調整していきます。\nこれによって、エンティティやエッジ（＝関係性）の低次元なベクトルを得ることができました。\n\n### Deep Knowledge-aware Network (DKN)\n\nDKN 自体は\n\n入力: 候補ニュースと、あるユーザの過去に見たニュースたち\n出力: クリック率\n\nとなるような CTR 予測モデルです。\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/f64784da-f8b9-4620-c0b5-5a3db5640417.png)\n\n候補となるニュースの特徴量をベクトルとして得るために、通常であれば単語の embedding を RNN や CNN でまとめ上げて固定長のベクトルに変換します。\nDKN ではここに Knowledge Graph Embedding によって得た特徴量を加えます。\n出現する単語ごとに Knowledge Graph 上のエンティティを探し、もし見つかったならそのエンティティ自体の embedding + 周辺のエンティティ embedding の平均を context vector として単語レベルの embedding に concat します。\nもしエンティティが見つからなければ 0 埋めでサイズをあわせます。\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/8e4f7cc4-89ee-5b3a-7343-245f33fa0b51.png)\n\n#### Attention-based User Interest Extraction\n\nつぎに、「ユーザが過去に見た記事」という情報をどのように活用するかを考えます。（＝ユーザの特徴をどのようにベクトル化するか）\n記事ごとのベクトルは ↑ で求められたので、単純に過去クリックした記事のベクトルの平均値を使うという方法が考えられますが、ユーザの興味は複数にまたがりうるので単純に平均を取るのは適しているとは言えません。\n（たとえば「プログラミング」「テニス」「ラーメン」の記事をクリックしたユーザに対して、「プログラミング」系の記事を推薦するのは多分良いはずだけど、平均をとってしまっているとその寄与が薄まる。）\n「今推薦の候補に考えている記事」について過去見た記事それぞれがどう関わっているかを表現できる方法でなければならないと言っています。\n\nそこで、 Attention Module をつかっています。\n候補の記事と過去に見た記事たちとの間の attention を計算し、過去に見た記事たちのベクトルの重み付き和をとることで、興味分野が複数にまたがっていても候補記事との関連をうまく見出したベクトルが作り出せます。\nAttention Module の入力は「候補記事のベクトル」と「過去に見た記事のベクトル」で、出力はその記事の寄与度になります。過去に見たすべての記事に対してそれぞれ network に入れて寄与度を計算し、softmax にかけたうで記事ベクトルの重み付き和をとっています。\n\n## 感想\n\nニュースタイトルは単語数が少なく固有名詞も多いので、単純な単語の embedding ではなかなか扱いづらいという問題を抱えていたので、 knowledge graph を使うというのはすごく納得の行く選択だなと思いました。\nただ、結果を見てみると Gain に対して複雑さや Knowledge Graph 自体を用意するコストが見合うかというとやはり厳しいかなという印象があります。（Microsoft はすでに自前の knowledge graph を持っているので...）\n\nKnowledge Graph Embedding については全く知らなかったのですが、面白い問題設定ですね。\n色々工夫されているようですが、 `h + r = t` というわかりやすく単純な方法でもそれなりに上手く行っていて面白かったです。\n\nまた、ユーザごとのベクトル表現の作り方の部分は Knowledge Graph の活用部分よりも簡単かつ一般的なので、この部分だけでも応用できそうだなと思いました。\n","contentMarkdown":"\nニュースの推薦に \"Knowledge Graph\" を活用する論文です。\nMicrosoft Research Asia のチームが WWW 2018 に投稿しています。\n\nこの記事は、Wantedly の勉強会で取り上げられた論文・技術をまとめたものです。\n[2018 年に読んだ機械学習系論文・技術まとめ at Wantedly Advent Calendar 2018 - Qiita](https://qiita.com/advent-calendar/2018/wantedly_ml)\n\n## Reference\n\n- DKN: Deep Knowledge-Aware Network for News Recommendation\n  - Hongwei Wang, Fuzheng Zhang, Xing Xie and Minyi Guo, et al., WWW 2018\n  - https://www2018.thewebconf.org/proceedings/#indus-922\n\n文中の図表は論文より引用しています。\n\n### 概要\n\n一般にニュース中の言葉は、常識的知識を仮定していて凝縮された文章になっています。\n一方で推薦系の既存手法は ニュース中に現れない知識を取り扱えておらず、潜在的なニュース間の関係を活かした探索が出来ていないという問題がありました。\n\nこの論文は、knowledge graph を活用した content-based な recommendation framework である _deep knowledge-aware network_ (DKN) を提案しています。\n\nknowledge graph とは、様々なエンティティを様々なエッジでつないだ heterogeneous なグラフで、たとえば \"モナリザ -[の作者」-> ダ・ヴィンチ\" のような情報を溜め込んだ巨大なグラフです。\nGoogle Search の裏でも活躍しているらしく、一般的な「知識」を構造化された形で表現する方法としてよく使われています。\nknowledge graph を使うことで、 \"Donald Trump\" という単語そのものだけからはわからない、 \"United States\" という単語との関連、\"Politician\" という単語との関連などを導くことができます。\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/6967d5d1-0708-7e59-51c8-db107866ea63.png)\n\n### 何が難しいか\n\nこの論文では、ニュース推薦の難しさとして以下のようなものを挙げています。\n\n- news の推薦は movie などと違って、リアルタイム性が高い問題（いつ publish されたニュースかが重要）であり、news 間の関係性もすぐにダメになる。\n  - なので ID ベースの既存手法（協調フィルタリングなど）は効果が弱い\n- news はユーザによって興味範囲が違うし、ユーザは複数の興味範囲を持っていることがほとんど。\n- news の文言は凝縮されている。\n  - 常識、大量の既知の entity を仮定している。\n  - “Boris Johnson Has Warned Donald Trump To Stick To The Iran Nuclear Deal\" というニュースは “Boris Johnson”, “Donald Trump”, “Iran\", “Nuclear” を知っている前提で書かれている\n  - ↑ に興味のあるユーザはきっと “North Korean EMP Attack Would Cause Mass U.S. Starvation, Says Congressional Report” にも興味があるが、単語レベルでの関係性はほぼ無い\n\nこれらを Knowledge Graph を活用しつつ解決していきます。\n\n### Knowledge Graph Embedding\n\nKnowledge Graph Embedding 自体はこの研究の contribution ではありませんが重要なので簡単に紹介します。\nKnowledge Graph Embedding は、通常の network embedding に近い問題設定で、 knowledge graph における各エンティティとエッジの低次元な embedding を求めるという問題です。\n`(head, relation, trail)` という triplet 構造をなるべく維持したまま、`h, r, t` それぞれを低次元空間で表現することが目標です。\nDKN は translation-based knowledge graph embedding というのを使っています。\n一番簡単な手法は `h + r = t` になるように embedding を定める手法です。\n（他にもいくつか紹介、実験されているけど大体発想は同じなので省略。）\nまずランダムな値で各エンティティ、エッジの embedding を初期化し、 `h + r = t` を満たすように gradient descent で embedding を微調整していきます。\nこれによって、エンティティやエッジ（＝関係性）の低次元なベクトルを得ることができました。\n\n### Deep Knowledge-aware Network (DKN)\n\nDKN 自体は\n\n入力: 候補ニュースと、あるユーザの過去に見たニュースたち\n出力: クリック率\n\nとなるような CTR 予測モデルです。\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/f64784da-f8b9-4620-c0b5-5a3db5640417.png)\n\n候補となるニュースの特徴量をベクトルとして得るために、通常であれば単語の embedding を RNN や CNN でまとめ上げて固定長のベクトルに変換します。\nDKN ではここに Knowledge Graph Embedding によって得た特徴量を加えます。\n出現する単語ごとに Knowledge Graph 上のエンティティを探し、もし見つかったならそのエンティティ自体の embedding + 周辺のエンティティ embedding の平均を context vector として単語レベルの embedding に concat します。\nもしエンティティが見つからなければ 0 埋めでサイズをあわせます。\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/8e4f7cc4-89ee-5b3a-7343-245f33fa0b51.png)\n\n#### Attention-based User Interest Extraction\n\nつぎに、「ユーザが過去に見た記事」という情報をどのように活用するかを考えます。（＝ユーザの特徴をどのようにベクトル化するか）\n記事ごとのベクトルは ↑ で求められたので、単純に過去クリックした記事のベクトルの平均値を使うという方法が考えられますが、ユーザの興味は複数にまたがりうるので単純に平均を取るのは適しているとは言えません。\n（たとえば「プログラミング」「テニス」「ラーメン」の記事をクリックしたユーザに対して、「プログラミング」系の記事を推薦するのは多分良いはずだけど、平均をとってしまっているとその寄与が薄まる。）\n「今推薦の候補に考えている記事」について過去見た記事それぞれがどう関わっているかを表現できる方法でなければならないと言っています。\n\nそこで、 Attention Module をつかっています。\n候補の記事と過去に見た記事たちとの間の attention を計算し、過去に見た記事たちのベクトルの重み付き和をとることで、興味分野が複数にまたがっていても候補記事との関連をうまく見出したベクトルが作り出せます。\nAttention Module の入力は「候補記事のベクトル」と「過去に見た記事のベクトル」で、出力はその記事の寄与度になります。過去に見たすべての記事に対してそれぞれ network に入れて寄与度を計算し、softmax にかけたうで記事ベクトルの重み付き和をとっています。\n\n## 感想\n\nニュースタイトルは単語数が少なく固有名詞も多いので、単純な単語の embedding ではなかなか扱いづらいという問題を抱えていたので、 knowledge graph を使うというのはすごく納得の行く選択だなと思いました。\nただ、結果を見てみると Gain に対して複雑さや Knowledge Graph 自体を用意するコストが見合うかというとやはり厳しいかなという印象があります。（Microsoft はすでに自前の knowledge graph を持っているので...）\n\nKnowledge Graph Embedding については全く知らなかったのですが、面白い問題設定ですね。\n色々工夫されているようですが、 `h + r = t` というわかりやすく単純な方法でもそれなりに上手く行っていて面白かったです。\n\nまた、ユーザごとのベクトル表現の作り方の部分は Knowledge Graph の活用部分よりも簡単かつ一般的なので、この部分だけでも応用できそうだなと思いました。\n","slug":"DKN:_Deep_Knowledge-Aware_Network_for_News_Recommendation","title":"DKN: Deep Knowledge-Aware Network for News Recommendation","timestamp":1545220940000,"tags":["DeepLearning","WWW","Recommendation","論文読み"]}}},"__N_SSG":true}