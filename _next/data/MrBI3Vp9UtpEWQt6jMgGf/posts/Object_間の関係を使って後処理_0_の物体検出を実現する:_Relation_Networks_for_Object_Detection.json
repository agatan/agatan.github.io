{"pageProps":{"post":{"content":"<p>Object Detection は、一枚の画像中の「どこに」「なにが」うつっているかを当てるタスクです。\n典型的な手法では、オブジェクトごとの bounding box を予測し、それぞれがどのクラスに分類されるかを<strong>個別で</strong>予測します。\nまた、ひとつのオブジェクトに対してすこしずつ座標のずれた box を複数予測してしまう可能性があるという問題があり、1 object 1 box になるように重複を削除しなければなりません。\nこれには nox maximum supression という方法を使うことが多いですが、これはヒューリスティックに基づく後処理になってしまっています。</p>\n<p>この論文では、予測した box 間の関係に着目することで、分類精度を向上し、後処理 0 の完全 End-to-End での物体検出ネットワークを構築しています。</p>\n<p><img src=\"https://qiita-image-store.s3.amazonaws.com/0/39030/9b80d772-3b27-5b14-2eac-2d1c1275492e.png\" alt=\"image.png\"></p>\n<p>（青い box について分類する際に、オレンジの box との関連が強く活用されている）</p>\n<p>この記事は、Wantedly の勉強会で取り上げられた論文・技術をまとめたものです。\n<a href=\"https://qiita.com/advent-calendar/2018/wantedly_ml\">2018 年に読んだ機械学習系論文・技術まとめ at Wantedly Advent Calendar 2018 - Qiita</a></p>\n<h2>Reference</h2>\n<ul>\n<li>Relation Networks for Object Detection\n<ul>\n<li>Han Hu, Jiayuan Gu, Zheng Zhang, Jifeng Dai, Yichen Wei, et al., CVPR 2018</li>\n</ul>\n</li>\n<li><a href=\"https://arxiv.org/abs/1711.11575\">https://arxiv.org/abs/1711.11575</a></li>\n</ul>\n<p>文中の図表は論文より引用しています。</p>\n<h2>モチベーション</h2>\n<p>自然言語処理の世界では、Attention module が非常に強力な武器として大活躍しており、特に Transformer[^1] 以降の SoTA モデルたちは大体 Attention の仕組みを組み込んでいるといっても良いくらいの活躍ぶりです。\n[^1]: <a href=\"https://arxiv.org/abs/1706.03762\">https://arxiv.org/abs/1706.03762</a>\n雑に言えば Attention は、ある entity と他の entity の関係性を 0<del>1 で出力し、その値をもとに entity を表す何らかのベクトルの加重和をとるといった操作をします。関係性を表す 0</del>1 を計算するためのパラメータも学習されます。\nまた画像に対応するタイトルを自動生成する Image Captioning の世界でも Attention は活躍しており、活用されるフィールドがどんどん増してきています。</p>\n<p>そこで、Attention を Object Detection の世界にもってこよう、というのがこの論文です。\nAttention module を用いて box 間の関係性を表し、object detection の精度向上を達成しています。</p>\n<h2>End-to-End</h2>\n<p><img src=\"https://qiita-image-store.s3.amazonaws.com/0/39030/4c30de7e-8f64-60fb-0bd6-712c33a990df.png\" alt=\"image.png\"></p>\n<p>概観でいうと Region Proposal network + RoI Pooling + relation module という構造を採用しています。\nrelation module は box 間の attention をとる module です。</p>\n<p>完全に後処理をなくすためには、大量の box のなかから採用すべき box だけを残す必要があります。（通常は non maximum supression で、スコアの高い box を優先的に残しつつ、重複した領域の大きい box はすてる）\nそこでこの論文では、Attention module をいくつか通したのち、box ごとに 0~1 の値を出力し、残すべき box は 1 になり捨てるべき box は 0 になるように学習します。</p>\n<h2>Object Relation Module</h2>\n<p>物体間の関係には 2 つの意味があります。1 つは意味的な関連で、もう一つは座標的な関連です。\nボールっぽいものとバットっぽいものがあったとしても、ものすごく離れた場所にあるのであれば無関係かもしれないですが、近くにあればきっとボールとバットのペアと予測するのが正しそうです。\nしかし、通常の attention は、意味的な関連しか扱えていません。すべての box をなんらかの vector にしてしまっていますし、convolution や pooling は座標に依存しない操作なのでその vector に座標そのものの情報は埋め込まれません。</p>\n<p>そこでこの論文ではふつうの Attention をちょっといじった object relation module というものを提案しています。</p>\n<p>物体 m, n 間の attention を算出する式を見てみます。</p>\n<img src=\"https://qiita-image-store.s3.amazonaws.com/0/39030/2e19515d-72ea-f065-e517-f8c7fe466fb8.png\" width=\"50%\">\n<p>$\\omega_A^{mn}$ は通常の Attention と同じで、object m と n を表すベクトル（を線形変換したもの）の内積です。\n$\\omega_G$ の部分を無視すれば、この式は単に各 object ごとに内積をとったものを softmax にかけている = ふつうの Attention の計算式と一致します。</p>\n<p>$\\omega_G$ は、座標的な関係を考慮するためのパラメータです。</p>\n<img src=\"https://qiita-image-store.s3.amazonaws.com/0/39030/9a24daf8-38cc-a765-af2f-7efa141cdad9.png\" width=\"50%\">\n<p>$f_G^m$ は object m の座標情報 (x,y, w, h) を表します。\n$\\varepsilon_G$ はふたつの object の座標情報からそれらの座標的関係を計算する関数です。</p>\n<div class=\"remark-highlight\"><pre class=\"language-math\"><code class=\"language-math\">\\varepsilon(f_G^m, f_G^n) = (\\log(\\frac{|x_m - x_n|}{w_m}), \\log(\\frac{|y_m - y_n|}{h_m}), \\log(\\frac{w_n}{w_m}), \\log(\\frac{h_n}{h_m}))</code></pre></div>\n<p>さらに、 $max{0, ...}$ をとることで、ReLU 的な働きをし、まったく座標的に関係のない object からの影響を 0 にしています。</p>\n<h2>まとめと感想</h2>\n<p>実験と結果はもと論文を読むのが一番くわしいのでそちらを参照ください。\n「一枚の画像から X を取得したいが、画像の主体となるような X だけを取りたい」といったケースにも応用できる手法かなと思っていて、実験してみたいと思いつつ、本当に end-to-end でやるのはちょっと大変そうすぎるという印象もあります。\n（また論文中には end-to-end が 0 から学習する際の問題にも触れられています。）\nとはいえ完全に end-to-end というのはやっぱり夢があって好きです。</p>\n","meta":{"rawMarkdown":"---\ntitle: \"Object 間の関係を使って後処理 0 の物体検出を実現する: Relation Networks for Object Detection\"\ndate: 2018-12-10T23:35:40+09:00\ntags: [\"画像処理\", \"DeepLearning\", \"論文読み\", \"物体検出\"]\nurl: https://qiita.com/agatan/items/1c2cadeaabfc9f122f6f\n---\n\nObject Detection は、一枚の画像中の「どこに」「なにが」うつっているかを当てるタスクです。\n典型的な手法では、オブジェクトごとの bounding box を予測し、それぞれがどのクラスに分類されるかを**個別で**予測します。\nまた、ひとつのオブジェクトに対してすこしずつ座標のずれた box を複数予測してしまう可能性があるという問題があり、1 object 1 box になるように重複を削除しなければなりません。\nこれには nox maximum supression という方法を使うことが多いですが、これはヒューリスティックに基づく後処理になってしまっています。\n\nこの論文では、予測した box 間の関係に着目することで、分類精度を向上し、後処理 0 の完全 End-to-End での物体検出ネットワークを構築しています。\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/9b80d772-3b27-5b14-2eac-2d1c1275492e.png)\n\n（青い box について分類する際に、オレンジの box との関連が強く活用されている）\n\nこの記事は、Wantedly の勉強会で取り上げられた論文・技術をまとめたものです。\n[2018 年に読んだ機械学習系論文・技術まとめ at Wantedly Advent Calendar 2018 - Qiita](https://qiita.com/advent-calendar/2018/wantedly_ml)\n\n## Reference\n\n- Relation Networks for Object Detection\n  - Han Hu, Jiayuan Gu, Zheng Zhang, Jifeng Dai, Yichen Wei, et al., CVPR 2018\n- https://arxiv.org/abs/1711.11575\n\n文中の図表は論文より引用しています。\n\n## モチベーション\n\n自然言語処理の世界では、Attention module が非常に強力な武器として大活躍しており、特に Transformer[^1] 以降の SoTA モデルたちは大体 Attention の仕組みを組み込んでいるといっても良いくらいの活躍ぶりです。\n[^1]: https://arxiv.org/abs/1706.03762\n雑に言えば Attention は、ある entity と他の entity の関係性を 0~1 で出力し、その値をもとに entity を表す何らかのベクトルの加重和をとるといった操作をします。関係性を表す 0~1 を計算するためのパラメータも学習されます。\nまた画像に対応するタイトルを自動生成する Image Captioning の世界でも Attention は活躍しており、活用されるフィールドがどんどん増してきています。\n\nそこで、Attention を Object Detection の世界にもってこよう、というのがこの論文です。\nAttention module を用いて box 間の関係性を表し、object detection の精度向上を達成しています。\n\n## End-to-End\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/4c30de7e-8f64-60fb-0bd6-712c33a990df.png)\n\n概観でいうと Region Proposal network + RoI Pooling + relation module という構造を採用しています。\nrelation module は box 間の attention をとる module です。\n\n完全に後処理をなくすためには、大量の box のなかから採用すべき box だけを残す必要があります。（通常は non maximum supression で、スコアの高い box を優先的に残しつつ、重複した領域の大きい box はすてる）\nそこでこの論文では、Attention module をいくつか通したのち、box ごとに 0~1 の値を出力し、残すべき box は 1 になり捨てるべき box は 0 になるように学習します。\n\n## Object Relation Module\n\n物体間の関係には 2 つの意味があります。1 つは意味的な関連で、もう一つは座標的な関連です。\nボールっぽいものとバットっぽいものがあったとしても、ものすごく離れた場所にあるのであれば無関係かもしれないですが、近くにあればきっとボールとバットのペアと予測するのが正しそうです。\nしかし、通常の attention は、意味的な関連しか扱えていません。すべての box をなんらかの vector にしてしまっていますし、convolution や pooling は座標に依存しない操作なのでその vector に座標そのものの情報は埋め込まれません。\n\nそこでこの論文ではふつうの Attention をちょっといじった object relation module というものを提案しています。\n\n物体 m, n 間の attention を算出する式を見てみます。\n\n<img src=\"https://qiita-image-store.s3.amazonaws.com/0/39030/2e19515d-72ea-f065-e517-f8c7fe466fb8.png\" width=\"50%\">\n\n$\\omega_A^{mn}$ は通常の Attention と同じで、object m と n を表すベクトル（を線形変換したもの）の内積です。\n$\\omega_G$ の部分を無視すれば、この式は単に各 object ごとに内積をとったものを softmax にかけている = ふつうの Attention の計算式と一致します。\n\n$\\omega_G$ は、座標的な関係を考慮するためのパラメータです。\n\n<img src=\"https://qiita-image-store.s3.amazonaws.com/0/39030/9a24daf8-38cc-a765-af2f-7efa141cdad9.png\" width=\"50%\">\n\n$f_G^m$ は object m の座標情報 (x,y, w, h) を表します。\n$\\varepsilon_G$ はふたつの object の座標情報からそれらの座標的関係を計算する関数です。\n\n```math\n\\varepsilon(f_G^m, f_G^n) = (\\log(\\frac{|x_m - x_n|}{w_m}), \\log(\\frac{|y_m - y_n|}{h_m}), \\log(\\frac{w_n}{w_m}), \\log(\\frac{h_n}{h_m}))\n```\n\nさらに、 $max{0, ...}$ をとることで、ReLU 的な働きをし、まったく座標的に関係のない object からの影響を 0 にしています。\n\n## まとめと感想\n\n実験と結果はもと論文を読むのが一番くわしいのでそちらを参照ください。\n「一枚の画像から X を取得したいが、画像の主体となるような X だけを取りたい」といったケースにも応用できる手法かなと思っていて、実験してみたいと思いつつ、本当に end-to-end でやるのはちょっと大変そうすぎるという印象もあります。\n（また論文中には end-to-end が 0 から学習する際の問題にも触れられています。）\nとはいえ完全に end-to-end というのはやっぱり夢があって好きです。\n","contentMarkdown":"\nObject Detection は、一枚の画像中の「どこに」「なにが」うつっているかを当てるタスクです。\n典型的な手法では、オブジェクトごとの bounding box を予測し、それぞれがどのクラスに分類されるかを**個別で**予測します。\nまた、ひとつのオブジェクトに対してすこしずつ座標のずれた box を複数予測してしまう可能性があるという問題があり、1 object 1 box になるように重複を削除しなければなりません。\nこれには nox maximum supression という方法を使うことが多いですが、これはヒューリスティックに基づく後処理になってしまっています。\n\nこの論文では、予測した box 間の関係に着目することで、分類精度を向上し、後処理 0 の完全 End-to-End での物体検出ネットワークを構築しています。\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/9b80d772-3b27-5b14-2eac-2d1c1275492e.png)\n\n（青い box について分類する際に、オレンジの box との関連が強く活用されている）\n\nこの記事は、Wantedly の勉強会で取り上げられた論文・技術をまとめたものです。\n[2018 年に読んだ機械学習系論文・技術まとめ at Wantedly Advent Calendar 2018 - Qiita](https://qiita.com/advent-calendar/2018/wantedly_ml)\n\n## Reference\n\n- Relation Networks for Object Detection\n  - Han Hu, Jiayuan Gu, Zheng Zhang, Jifeng Dai, Yichen Wei, et al., CVPR 2018\n- https://arxiv.org/abs/1711.11575\n\n文中の図表は論文より引用しています。\n\n## モチベーション\n\n自然言語処理の世界では、Attention module が非常に強力な武器として大活躍しており、特に Transformer[^1] 以降の SoTA モデルたちは大体 Attention の仕組みを組み込んでいるといっても良いくらいの活躍ぶりです。\n[^1]: https://arxiv.org/abs/1706.03762\n雑に言えば Attention は、ある entity と他の entity の関係性を 0~1 で出力し、その値をもとに entity を表す何らかのベクトルの加重和をとるといった操作をします。関係性を表す 0~1 を計算するためのパラメータも学習されます。\nまた画像に対応するタイトルを自動生成する Image Captioning の世界でも Attention は活躍しており、活用されるフィールドがどんどん増してきています。\n\nそこで、Attention を Object Detection の世界にもってこよう、というのがこの論文です。\nAttention module を用いて box 間の関係性を表し、object detection の精度向上を達成しています。\n\n## End-to-End\n\n![image.png](https://qiita-image-store.s3.amazonaws.com/0/39030/4c30de7e-8f64-60fb-0bd6-712c33a990df.png)\n\n概観でいうと Region Proposal network + RoI Pooling + relation module という構造を採用しています。\nrelation module は box 間の attention をとる module です。\n\n完全に後処理をなくすためには、大量の box のなかから採用すべき box だけを残す必要があります。（通常は non maximum supression で、スコアの高い box を優先的に残しつつ、重複した領域の大きい box はすてる）\nそこでこの論文では、Attention module をいくつか通したのち、box ごとに 0~1 の値を出力し、残すべき box は 1 になり捨てるべき box は 0 になるように学習します。\n\n## Object Relation Module\n\n物体間の関係には 2 つの意味があります。1 つは意味的な関連で、もう一つは座標的な関連です。\nボールっぽいものとバットっぽいものがあったとしても、ものすごく離れた場所にあるのであれば無関係かもしれないですが、近くにあればきっとボールとバットのペアと予測するのが正しそうです。\nしかし、通常の attention は、意味的な関連しか扱えていません。すべての box をなんらかの vector にしてしまっていますし、convolution や pooling は座標に依存しない操作なのでその vector に座標そのものの情報は埋め込まれません。\n\nそこでこの論文ではふつうの Attention をちょっといじった object relation module というものを提案しています。\n\n物体 m, n 間の attention を算出する式を見てみます。\n\n<img src=\"https://qiita-image-store.s3.amazonaws.com/0/39030/2e19515d-72ea-f065-e517-f8c7fe466fb8.png\" width=\"50%\">\n\n$\\omega_A^{mn}$ は通常の Attention と同じで、object m と n を表すベクトル（を線形変換したもの）の内積です。\n$\\omega_G$ の部分を無視すれば、この式は単に各 object ごとに内積をとったものを softmax にかけている = ふつうの Attention の計算式と一致します。\n\n$\\omega_G$ は、座標的な関係を考慮するためのパラメータです。\n\n<img src=\"https://qiita-image-store.s3.amazonaws.com/0/39030/9a24daf8-38cc-a765-af2f-7efa141cdad9.png\" width=\"50%\">\n\n$f_G^m$ は object m の座標情報 (x,y, w, h) を表します。\n$\\varepsilon_G$ はふたつの object の座標情報からそれらの座標的関係を計算する関数です。\n\n```math\n\\varepsilon(f_G^m, f_G^n) = (\\log(\\frac{|x_m - x_n|}{w_m}), \\log(\\frac{|y_m - y_n|}{h_m}), \\log(\\frac{w_n}{w_m}), \\log(\\frac{h_n}{h_m}))\n```\n\nさらに、 $max{0, ...}$ をとることで、ReLU 的な働きをし、まったく座標的に関係のない object からの影響を 0 にしています。\n\n## まとめと感想\n\n実験と結果はもと論文を読むのが一番くわしいのでそちらを参照ください。\n「一枚の画像から X を取得したいが、画像の主体となるような X だけを取りたい」といったケースにも応用できる手法かなと思っていて、実験してみたいと思いつつ、本当に end-to-end でやるのはちょっと大変そうすぎるという印象もあります。\n（また論文中には end-to-end が 0 から学習する際の問題にも触れられています。）\nとはいえ完全に end-to-end というのはやっぱり夢があって好きです。\n","slug":"Object_間の関係を使って後処理_0_の物体検出を実現する:_Relation_Networks_for_Object_Detection","title":"Object 間の関係を使って後処理 0 の物体検出を実現する: Relation Networks for Object Detection","timestamp":1544452540000,"tags":["画像処理","DeepLearning","論文読み","物体検出"]}}},"__N_SSG":true}